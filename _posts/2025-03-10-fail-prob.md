---
layout: post
title: "Differentially Private Algorithms that Never Fail"
comments: true
authors:
  - thomassteinke
timestamp: 10:00:00 -0700
categories: [Algorithms]
---

Most differentially private algorithms fail with some nonzero probability. For example, when adding Gaussian or Laplace noise, there is some chance that the noise deviates significantly from its mean. But, fortunately, large deviations are unlikely.
In this post we're going to take a closer look at failure modes of DP algorithms and we'll present some generic methods for reducing -- or even eliminating -- the failure probability.

Let's be precise about what we mean by failure probability:
Let's assume we have a \\\(\(\\varepsilon,\\delta\)\\\)-differentally private algorithm \\\(M : \\mathcal{X}^n \\to \\mathcal{Y}\\\) and we have a loss function \\\(\\ell : \\mathcal{Y} \\times \\mathcal{X}^n \\to \\mathbb{R}\\\).
The (worst-case) _failure probability_ \\\(\\beta\\\) of \\\(M\\\) is \\\[\\beta := \\sup\_{x\\in\\mathcal{X}^n} \\mathbb{P}\[\ell\(M\(x\),x\)&gt;\\alpha\],\\tag{1}\\\] where \\\(\\alpha\\\) is some target value for the loss.[^1]

For example, if \\\(M\(x\)=q\(x\)+\\mathsf{Laplace}\(1/\\varepsilon\)\\\) is the Laplace mechanism and \\\(\\ell\(y,x\)=\|y-q\(x\)\|\\\) is the absolute error, then \\\(\\beta = \\exp\(-\\varepsilon\\alpha\)\\\).
If we want to eliminate the failure probability, we could use _truncated_ Laplace noise instead of regular Laplace noise.[^tlap] And -- spoiler alert -- that's the kind of method we're going to look at.

To be clear, in this post we're talking about failures of _utility_, which are different from failures of _privacy_.
In [a previous post](/flavoursofdelta/), we talked about privacy failures; roughly, the \\\(\\delta\\\) in \\\(\(\\varepsilon,\\delta\)\\\)-DP captures the probability of a privacy failure. Privacy failures are a lot harder to fix than utility failures (which is kinda the point of this post). 

Here's our problem: We're given a DP algorithm \\\(M\\\) with failure probability \\\(\\beta\\\), and we want to modify the algorithm to get a new DP algorithm \\\(\\widetilde{M}\\\) with failure probability \\\(\\widetilde{\\beta}&lt;\\beta\\\). Ideally, we want \\\(\\widetilde{\\beta}=0\\\).

## Absorbing the failure probability into \\\(\\delta\\\)

Let's start with a simple trick:
Suppose that, in addition to the \\\(\(\\varepsilon,\\delta\)\\\)-DP algorithm \\\(M\\\) with failure probability \\\(\\beta=\\sup\_x\\mathbb{P}\[\\ell\(M\(x\),x\)&gt;\\alpha\]\\\), we have a non-private algorithm \\\(\\check{M} : \\mathcal{X}^n \\to \\mathcal{Y}\\\) that _never_ fails -- i.e., \\\(\\sup\_x \\mathbb{P}\[\\ell\(\\check{M}\(x\),x\)&gt;\\alpha\]=0\\\).[^2]

Now let's define \\\(\\widetilde{M}\(x\)\\\) as follows. First, compute \\\(y=M\(x\)\\\). If \\\(\\ell\(y,x\)\\le\\alpha\\\), return \\\(y\\\). If \\\(\\ell\(y,x\)&gt;\\alpha\\\), compute \\\(\\check{y}=\\check{M}\(x\)\\\) and return \\\(\\check{y}\\\).

Clearly \\\(\\widetilde{M}\\\) now has zero failure probability. What about privacy?

Fix arbitrary neighbouring \\\(x,x'\\in\\mathcal{X}^n\\\) and a measurable \\\(S\\subset\\mathcal{Y}\\\).
Define \\\(S^\* := \\{ y \in S : \\ell\(y,x\)\\le\\alpha \\}\\\). 
Now we have
\\\[ ~~~~~~~~~~~~~~~~~~~~~\\mathbb{P}\[\\widetilde{M}\(x\)\\in S\] = \\mathbb{P}\[M\(x\)\\in S^\*\] + \\mathbb{P}\[\\ell\(M\(x\),x\)&gt;\\alpha\] \\cdot \\mathbb{P}\[\\check{M}\(x\)\in S\]\\\]
\\\[ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\le e^\\varepsilon \\mathbb{P}\[M\(x'\)\\in S^\*\] + \\delta + \\mathbb{P}\[\\ell\(M\(x\),x\)&gt;\\alpha\] \\cdot 1 \\\]
\\\[ \\le e^\\varepsilon \\mathbb{P}\[\\widetilde{M}\(x'\)\\in S\] + \\delta + \beta. \\\]
Thus \\\(\\widetilde{M}\\\) is \\\(\(\\varepsilon,\\delta+\\beta\)\\\)-DP.
In other words, we've absorbed the utility failure probability \\\(\\beta\\\) into the privacy failure probability \\\(\\delta\\\).

This trick is neat since it lets us eliminate one of the parameters (\\\(\\beta\\\)), but, in practice, you might not want to do this. We're swapping a utility failure for a privacy failure and that often isn't a great trade. 

This trick only works if you already have a small failure probability \\\(\\beta\\\).[^3] What if we start with, say, \\\(\\beta=0.1\\\)?

## Avoiding silent failures

Above, we non-privately checked the failure condition \\\(\\ell\(\\check{M}\(x\),x\)&gt;\\alpha\\\).
Intuitively, using a non-private test _must_ cost us in terms of privacy.
Thus, to do better, we have to rely on a private test of the failure condition.

We can't do much with an arbitrary loss function, so we need to make some assumptions.





---
[^1]: For simplicity, in this post we will talk about failure as a boolean event; i.e., there is a hard utility threshold at \\\(\\alpha\\\). Of course, in most cases, there is not a hard threshold and it makes sense to talk about the tail probability \\\(\\beta\\\) as a function of the threshold \\\(\\alpha\\\), rather than a single value. 
[^2]: Note that we can always just define \\\(\\check{M}\(x\) = \\mathsf{argmin}\_y \\ell\(y,x\)\\\) or we can re-run \\\(M\\\) until we achieve the desired loss.
[^3]: Recall, that the privacy failure probablity should be tiny -- e.g., \\\(\\delta \\le 10^{-6}\\\) -- for the privacy guarantee to be compelling.
[^tlap]: To achieve \\\(\(\\varepsilon,\\delta\)\\\)-DP, we can use Laplace noise truncted to magnitude \\\(O\(\\log\(1/\\delta\)/\\varepsilon\)\\\). Truncated Laplace noise is folklore [[L16](https://arxiv.org/abs/1607.08554 "Fang Liu. Statistical Properties of Sanitized Results from Differentially Private Laplace Mechanism with Univariate Bounding Constraints. 2016.")]; Holohan et al. [[HABA18](https://arxiv.org/abs/1808.10410 "Naoise Holohan, Spiros Antonatos, Stefano Braghin, PÃ³l Mac Aonghusa. The Bounded Laplace Mechanism in Differential Privacy. 2018.")] give a sharp analysis.
