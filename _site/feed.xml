<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Differential Privacy</title>
    <description>Website for the differential privacy research community</description>
    <link>http://localhost:4000</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml" />
    
      <item>
        <title>Open problem(s) - How generic can composition results be?</title>
        <description>&lt;p&gt;The composition theorem is a cornerstone of differential privacy literature. 
In its most basic formulation, it states that if two mechanisms \(\mathcal{M}_1\) and \(\mathcal{M}_2\) are respectively \(\varepsilon_1\)-DP and \(\varepsilon_2\)-DP, then the mechanism \(\mathcal{M}\) defined by \(\mathcal{M}(D)=\left(\mathcal{M}_1(D),\mathcal{M}_2(D)\right)\) is \((\varepsilon_1+\varepsilon_2)\)-DP.
A large body of work focused on proving extensions of this composition theorem.
These extensions are of two kinds.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Some composition results apply to different &lt;em&gt;settings&lt;/em&gt; than fixed mechanisms.&lt;/li&gt;
  &lt;li&gt;Other extend known results to &lt;em&gt;variants&lt;/em&gt; of differential privacy.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In this blog post, we review existing results, and outline natural open questions appearing on both fronts.
We stumbled upon these open questions while building general-purpose differential privacy infrastructure, and we believe that solving them could have a positive impact on the usability and privacy/accuracy trade-offs provided by such tools.&lt;/p&gt;

&lt;h3 id=&quot;different-settings-for-composition&quot;&gt;Different settings for composition&lt;/h3&gt;

&lt;p&gt;First, let’s discuss what it means to compose two DP mechanisms.&lt;/p&gt;

&lt;h4 id=&quot;sequential-composition&quot;&gt;Sequential composition&lt;/h4&gt;

&lt;p&gt;In the original composition result [&lt;a href=&quot;https://link.springer.com/chapter/10.1007/11681878_14&quot;&gt;DMKS06&lt;/a&gt;], all mechanisms \(\mathcal{M}_1\), \(\mathcal{M}_2\), etc., are fixed in advance, and have a predetermined privacy budget (resp. \(\varepsilon_1\), \(\varepsilon_2\), etc.).
They only take the sensitive data \(D\) as input: \(\mathcal{M}_2\) cannot see nor depend on \(\mathcal{M}_1(D)\).
This setting is typically called &lt;em&gt;sequential composition&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/sequential-composition.svg&quot; width=&quot;80%&quot; alt=&quot;A diagram representing sequential composition. A database icon is on the left. Arrows go from it to three boxes labeled M1, M2, and M3, each labeled with ε1, ε2, ε3; these ε values are labeled &apos;fixed budgets&apos;.&quot; style=&quot;margin:auto;display: block;&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;adaptive-composition&quot;&gt;Adaptive composition&lt;/h4&gt;

&lt;p&gt;Shortly afterwards, the result was extended to a setting called &lt;em&gt;adaptive composition&lt;/em&gt; [&lt;a href=&quot;https://link.springer.com/chapter/10.1007/11761679_29&quot;&gt;DKMMN06&lt;/a&gt;].
In this context, each mechanism can access the outputs of previous mechanisms: for example, \(\mathcal{M}_2\) takes as input not only the sensitive data \(D\), but also \(\mathcal{M}_1(D)\).
However, the privacy budget associated with each mechanism is still fixed.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/adaptive-composition.svg&quot; width=&quot;80%&quot; alt=&quot;A diagram representing adaptive composition. It&apos;s the same diagram as sequential composition, except there are arrows going from M1 to M2, and from M2 to M3.&quot; style=&quot;margin:auto;display: block;&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;fully-adaptive-composition&quot;&gt;Fully adaptive composition&lt;/h4&gt;

&lt;p&gt;A natural extension of adaptive composition consists in allowing the privacy budget of each mechanism to depend on previous outputs.
This setting is called &lt;em&gt;fully adaptive composition&lt;/em&gt; [&lt;a href=&quot;https://proceedings.neurips.cc/paper_files/paper/2016/hash/58c54802a9fb9526cd0923353a34a7ae-Abstract.html&quot;&gt;RRUV16&lt;/a&gt;].
It captures a setting in which a single analyst is interacting with a DP interface, and can change which queries to run and their budget based on past results.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/fully-adaptive-composition.svg&quot; width=&quot;80%&quot; alt=&quot;A diagram representing fully adaptive composition. It&apos;s the same diagram as adaptive composition, except the &apos;fixed budgets&apos; label is gone, and there are arrows going from M1 to ε2, and from M2 to ε3.&quot; style=&quot;margin:auto;display: block;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Composition theorems in the fully adaptive setting are of two types.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Privacy filters&lt;/em&gt; assume that the DP interface has a fixed, total budget, and will refuse to answer queries once that budget is exhausted.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Privacy odometers&lt;/em&gt;, by contrast, allow the analyst to run arbitrarily many queries using as much budget as they want, and quantify the privacy loss over time.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Somewhat surprisingly, there are separation results between both types: one can obtain tighter composition theorems with privacy filters than privacy odometers.&lt;/p&gt;

&lt;h4 id=&quot;concurrent-composition&quot;&gt;Concurrent composition&lt;/h4&gt;

&lt;p&gt;This is, however, not the end of the story.
Fully adaptive composition captures a setting in which a &lt;em&gt;single&lt;/em&gt; analyst interacts with a DP interface.
What if &lt;em&gt;multiple&lt;/em&gt; analysts have access to this interface, each with their own budget?
&lt;em&gt;Concurrent composition&lt;/em&gt; [&lt;a href=&quot;https://arxiv.org/abs/2105.14427&quot;&gt;VW21&lt;/a&gt;] captures this idea.
In this setting, the mechanisms that are being composed are &lt;em&gt;interactive&lt;/em&gt; (we denote them by IM in the diagram below), and the analysts interacting with each mechanism can share results with each other, and adaptively decide which queries to run.
The goal is to quantify the total privacy budget cost, across analysts: do existing results extend to the composition of interactive mechanisms?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/concurrent-composition.svg&quot; width=&quot;80%&quot; alt=&quot;A diagram representing concurrent composition. A database icon on the left has two-sided arrows going from two boxes labeled IM1 and IM2, respectively labeled ε1 and ε2. The first box has two pairs of arrows going back and forth between it and a smiley face. The second one has the same, with a different smiley face.&quot; style=&quot;margin:auto;display: block;&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;fully-concurrent-composition&quot;&gt;Fully concurrent composition?&lt;/h4&gt;

&lt;p&gt;In concurrent composition as defined in [&lt;a href=&quot;https://arxiv.org/abs/2105.14427&quot;&gt;VW21&lt;/a&gt;], the number of analysts and their respective privacy budget is fixed upfront.
This means that concurrent composition and fully adaptive composition results are incomparable.
This suggests an even more generic setting, which (to the best of our knowledge) has not been studied in the literature: a kind of concurrent composition, where the number of analysts and their budget is &lt;em&gt;not&lt;/em&gt; predefined.
Let’s call this &lt;em&gt;fully concurrent composition&lt;/em&gt;.
In this setting, an analyst with a certain privacy budget would be able to spin off a new interactive mechanism, with an adaptively-chosen privacy budget, that can also be interacted with concurrently.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/fully-concurrent-composition.svg&quot; width=&quot;80%&quot; alt=&quot;A diagram representing fully concurrent composition. It&apos;s the same as the diagram for concurrent composition, except one of the pairs of arrows going to and from IM1 goes to a smaller box labeled IM3, labeled ε3, and there is also an arrow from IM1 to ε3. IM3 also has a pair of arrows going back and forth towards a third smiley face.&quot; style=&quot;margin:auto;display: block;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This setting might seem pointless — why would analysts want to do this? — but proving composition results in this context would help building DP interfaces that combine expressivity and conceptual simplicity.
To understand why, let’s take a look at how &lt;a href=&quot;https://tmlt.dev&quot;&gt;Tumult Analytics&lt;/a&gt;&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; allows users to use its parallel composition feature.&lt;/p&gt;

&lt;p&gt;Tumult Analytics has a concept of a &lt;em&gt;Session&lt;/em&gt;, which is initialized on some sensitive data with a given privacy budget.
Users can submit queries to this Session using a query language implemented in Python.
Each query executed by the Session will consume part of the overall privacy budget, and return DP results.
The use can then examine these results to decide which queries to submit to the Session next, and with which privacy budget.
So far, this matches the fully adaptive setting, in its privacy filter formulation.&lt;/p&gt;

&lt;p&gt;But Tumult Analytics also allows users to split their sensitive data depending on the value of an attribute, and perform different operations in each partition of the data.
With this feature, users can write algorithms that use &lt;em&gt;parallel composition&lt;/em&gt;, which is very useful.
This partitioning operation takes a fraction of the privacy budget, and spins off &lt;em&gt;sub-Sessions&lt;/em&gt; that each have access to a subset of the original data.
The following diagram visualizes an example of this process.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/parallel-composition-analytics.svg&quot; width=&quot;80%&quot; alt=&quot;A diagram visualizing an example of parallel composition in Tumult Analytics. At the top is a database icon labeled &apos;Data&apos;. A double-sided arrow goes from it to a box labeled &apos;Session 1, ε1 = 3&apos;. Under this box is a differently-colored box labeled &apos;Parallel partitioning using ε2 = 1&apos;, three dotted-line arrows go through this box towards boxes labeled &apos;Session 2a, ε2 = 1&apos;, &apos;Session 2b, ε2 = 1&apos;, and &apos;Session 1, ε1 = 2&apos;. Session 2a and 2b have arrows going to and from the database icon, cut in two parts (one for each Session).&quot; style=&quot;margin:auto;display: block;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;At the beginning, there is one Session with a privacy budget of \(\varepsilon_1=3\).
After the partitioning operation, there are now &lt;em&gt;three&lt;/em&gt; Sessions: the original Session that has access to all the data and has a leftover privacy budget of \(\varepsilon_1=2\), and two sub-Sessions that each have access to a partition of the data and have a privacy budget of \(\varepsilon_2=1\).
The analyst using this interface can interact with any of these three Sessions, and interleave queries between each, in a fully interactive manner.
This means that even though there is a single user interacting with the data, the setting is similar to concurrent composition: each Session is an interactive object with a maximum privacy budget.
However, note that the privacy budget associated with each of the sub-Sessions could, in principle, depend on the result of past queries.
This suggests that we need composition results that take this into account, and capture the fully concurrent setting suggested above.&lt;/p&gt;

&lt;h3 id=&quot;composition-for-variants-of-differential-privacy&quot;&gt;Composition for variants of differential privacy&lt;/h3&gt;

&lt;h4 id=&quot;existing-results-and-natural-questions&quot;&gt;Existing results and natural questions&lt;/h4&gt;

&lt;p&gt;A large number of variants and extensions of differential privacy have been proposed in the literature.
In many cases, a benefit of these alternative definitions is to improve the privacy analysis of mechanisms that compose a large number of simpler primitives.
For example, the \(n\)-fold composition of \(\varepsilon\)-DP mechanisms is \(n\varepsilon\)-DP, but the \(n\)-fold composition of \((\varepsilon,\delta)\)-DP mechanisms is also \((\varepsilon’,\delta’)\)-DP, with \(\varepsilon’\approx\sqrt{n}\varepsilon\) and \(\delta’\approx n\delta\).
Machine learning applications often use the moments accountant to perform privacy accounting, relying on the composition property of Rényi DP [&lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/8049725&quot;&gt;Mir17&lt;/a&gt;, &lt;a href=&quot;https://research.google/pubs/pub45428/&quot;&gt;ACGMMTZ16&lt;/a&gt;].
Gaussian DP and its generalization \(f\)-DP [&lt;a href=&quot;https://academic.oup.com/jrsssb/article/84/1/3/7056089&quot;&gt;DRS22&lt;/a&gt;] are also used in this context [&lt;a href=&quot;https://arxiv.org/abs/1911.11607&quot;&gt;BDLS20&lt;/a&gt;].
Meanwhile, statistical use cases using the Gaussian mechanism often use zero-concentrated DP [&lt;a href=&quot;https://link.springer.com/chapter/10.1007/978-3-662-53641-4_24&quot;&gt;BS16&lt;/a&gt;] (zCDP) for their privacy analysis [&lt;a href=&quot;https://desfontain.es/privacy/real-world-differential-privacy.html&quot;&gt;Des21&lt;/a&gt;]; the approximate version of this definition is also useful when queries are grouped by an unknown domain [&lt;a href=&quot;https://arxiv.org/abs/2301.01998&quot;&gt;SDH23&lt;/a&gt;].&lt;/p&gt;

&lt;p&gt;It is thus natural to study the composition of these variants under the settings described in the previous section.
For many variants and composition settings, &lt;em&gt;optimal&lt;/em&gt; composition results have been proven.
We give an overview in the following table.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt; &lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;Sequential&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;Adaptive&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;Fully adaptive&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;Concurrent&lt;/strong&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;\(\varepsilon\)-DP&lt;/td&gt;
      &lt;td&gt;[&lt;a href=&quot;https://link.springer.com/chapter/10.1007/11681878_14&quot;&gt;DMKS06&lt;/a&gt;]&lt;/td&gt;
      &lt;td&gt;[&lt;a href=&quot;https://link.springer.com/chapter/10.1007/11761679_29&quot;&gt;DKMMN06&lt;/a&gt;]&lt;/td&gt;
      &lt;td&gt;[&lt;a href=&quot;https://proceedings.neurips.cc/paper_files/paper/2016/hash/58c54802a9fb9526cd0923353a34a7ae-Abstract.html&quot;&gt;RRUV16&lt;/a&gt;]&lt;/td&gt;
      &lt;td&gt;[&lt;a href=&quot;https://arxiv.org/abs/2105.14427&quot;&gt;VW21&lt;/a&gt;]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;\((\varepsilon,\delta)\)-DP&lt;/td&gt;
      &lt;td&gt;[&lt;a href=&quot;https://proceedings.mlr.press/v37/kairouz15.html&quot;&gt;KOV15&lt;/a&gt;]&lt;/td&gt;
      &lt;td&gt;[&lt;a href=&quot;https://proceedings.mlr.press/v37/kairouz15.html&quot;&gt;KOV15&lt;/a&gt;]&lt;/td&gt;
      &lt;td&gt;[&lt;a href=&quot;https://proceedings.mlr.press/v202/whitehouse23a.html&quot;&gt;WRRW22&lt;/a&gt;]*&lt;/td&gt;
      &lt;td&gt;[&lt;a href=&quot;https://proceedings.mlr.press/v202/whitehouse23a.html&quot;&gt;WRRW22&lt;/a&gt;, &lt;a href=&quot;https://proceedings.neurips.cc/paper_files/paper/2022/hash/3f52b555967a95ee850fcecbd29ee52d-Abstract-Conference.html&quot;&gt;Lyu22&lt;/a&gt;]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Gaussian DP&lt;/td&gt;
      &lt;td&gt;[&lt;a href=&quot;https://academic.oup.com/jrsssb/article/84/1/3/7056089&quot;&gt;DRS22&lt;/a&gt;]&lt;/td&gt;
      &lt;td&gt;[&lt;a href=&quot;https://academic.oup.com/jrsssb/article/84/1/3/7056089&quot;&gt;DRS22&lt;/a&gt;]&lt;/td&gt;
      &lt;td&gt;[&lt;a href=&quot;https://arxiv.org/abs/2210.17520&quot;&gt;ST22&lt;/a&gt;]&lt;/td&gt;
      &lt;td&gt;[&lt;a href=&quot;https://arxiv.org/abs/2207.08335&quot;&gt;VZ22&lt;/a&gt;]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;\(f\)-DP&lt;/td&gt;
      &lt;td&gt;[&lt;a href=&quot;https://academic.oup.com/jrsssb/article/84/1/3/7056089&quot;&gt;DRS22&lt;/a&gt;]&lt;/td&gt;
      &lt;td&gt;[&lt;a href=&quot;https://academic.oup.com/jrsssb/article/84/1/3/7056089&quot;&gt;DRS22&lt;/a&gt;]&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;[&lt;a href=&quot;https://arxiv.org/abs/2207.08335&quot;&gt;VZ22&lt;/a&gt;]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;\((\alpha,\varepsilon)\)-Rényi DP&lt;/td&gt;
      &lt;td&gt;[&lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/8049725&quot;&gt;Mir17&lt;/a&gt;]&lt;/td&gt;
      &lt;td&gt;[&lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/8049725&quot;&gt;Mir17&lt;/a&gt;]&lt;/td&gt;
      &lt;td&gt;[&lt;a href=&quot;https://proceedings.neurips.cc/paper/2021/hash/ec7f346604f518906d35ef0492709f78-Abstract.html&quot;&gt;FZ21&lt;/a&gt;]&lt;/td&gt;
      &lt;td&gt;[&lt;a href=&quot;https://proceedings.neurips.cc/paper_files/paper/2022/hash/3f52b555967a95ee850fcecbd29ee52d-Abstract-Conference.html&quot;&gt;Lyu22&lt;/a&gt;]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;\(\rho\)-zero-concentrated DP&lt;/td&gt;
      &lt;td&gt;[&lt;a href=&quot;https://link.springer.com/chapter/10.1007/978-3-662-53641-4_24&quot;&gt;BS16&lt;/a&gt;]&lt;/td&gt;
      &lt;td&gt;[&lt;a href=&quot;https://link.springer.com/chapter/10.1007/978-3-662-53641-4_24&quot;&gt;BS16&lt;/a&gt;]&lt;/td&gt;
      &lt;td&gt;[&lt;a href=&quot;https://proceedings.neurips.cc/paper/2021/hash/ec7f346604f518906d35ef0492709f78-Abstract.html&quot;&gt;FZ21&lt;/a&gt;]&lt;/td&gt;
      &lt;td&gt;[&lt;a href=&quot;https://proceedings.neurips.cc/paper_files/paper/2022/hash/3f52b555967a95ee850fcecbd29ee52d-Abstract-Conference.html&quot;&gt;Lyu22&lt;/a&gt;]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;\(\delta\)-approx. \(\rho\)-zCDP&lt;/td&gt;
      &lt;td&gt;[&lt;a href=&quot;https://link.springer.com/chapter/10.1007/978-3-662-53641-4_24&quot;&gt;BS16&lt;/a&gt;]&lt;/td&gt;
      &lt;td&gt;[&lt;a href=&quot;https://link.springer.com/chapter/10.1007/978-3-662-53641-4_24&quot;&gt;BS16&lt;/a&gt;]&lt;/td&gt;
      &lt;td&gt;[&lt;a href=&quot;https://proceedings.mlr.press/v202/whitehouse23a.html&quot;&gt;WRRW22&lt;/a&gt;]&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;center&gt;&amp;lt;small

* Only asymptotically optimal for small ε.

&amp;lt;/small&amp;gt;&lt;/center&gt;

&lt;p&gt;This summary already suggests a few natural open questions: it is not known whether the fully adaptive composition results for \((\varepsilon,\delta)\)-DP can be improved, there is no fully adaptive composition theorem for \(f\)-DP, or concurrent for \((\rho,\delta)\)-approximate zCDP.&lt;/p&gt;

&lt;h4 id=&quot;reordering-mechanisms-during-the-privacy-analysis&quot;&gt;Reordering mechanisms during the privacy analysis&lt;/h4&gt;

&lt;p&gt;Let’s assume for a moment that the table above is completed, and that we have optimal composition theorems for all the variants of interest and all settings.
Consider an analyst using a differential privacy framework, and performing multiple operations in a fully adaptive way.
Some of these operations are using \(\rho\)-zCDP, others are \((\varepsilon,\delta)\)-DP, alternatively, with varying parameters.
How should the privacy accounting be done in such a scenario?&lt;/p&gt;

&lt;p&gt;In the context of sequential composition, it would be natural to &lt;em&gt;reorder&lt;/em&gt; those mechanisms: consider the equivalent situation where all \(\rho\)-zCDP mechanisms occur first, and all \((\varepsilon,\delta)\)-DP mechanisms occur afterwards.
In this setting, the zCDP mechanisms can be first be composed using the zCDP composition rule.
The overall zCDP guarantee can then be converted to \((\varepsilon,\delta)\)-DP, and composed with the other \((\varepsilon,\delta)\)-DP guarantees.
This will lead to a tighter privacy analysis than converting every individual \(\rho\)-zCDP mechanism to \((\varepsilon,\delta)\)-DP, and composing those guarantees.&lt;/p&gt;

&lt;p&gt;However, we would need an additional theoretical result to perform this kind of reordering operation in a fully adaptive context: the fact that composition results exist for \((\varepsilon,\delta)\)-DP and \(\rho\)-zCDP does not mean they can be combined.
How to resolve this problem, and make it possible to use the same privacy accounting techniques in the sequential setting and in the fully adaptive or fully concurrent setting?
This leads to a natural open question: when performing the privacy analysis of a privacy filter, can one “reorder” the mechanisms when composing them?
Answering this positively would allow DP frameworks to implement tighter privacy accounting at a relatively low cost in complexity.
It might very well be that the answer to this open question is negative.
In that case, proving such a separation result would be of significant theoretical interest in the study of DP composition.&lt;/p&gt;

&lt;h4 id=&quot;composing-privacy-loss-distributions&quot;&gt;Composing privacy loss distributions&lt;/h4&gt;

&lt;p&gt;When we say that a mechanism is \((\varepsilon,\delta)\)-DP, or \(\rho\)-zCDP, we are giving a “global” bound on the privacy loss random variable, defined by:
\[
    \mathcal{L}_{D,D’}(o) =
       \ln\left(\frac{\mathbb{P}\left[\mathcal{M}(D)=o\right]}{\mathbb{P}\left[\mathcal{M}(D’)=o\right]}\right)
\]
for all neighboring inputs \(D\) and \(D’\).&lt;/p&gt;

&lt;p&gt;An alternative approach to privacy accounting consists in &lt;em&gt;fully&lt;/em&gt; describing this random variable.
One approach to do this uses the formalism of &lt;em&gt;privacy loss distributions&lt;/em&gt; (PLDs) [&lt;a href=&quot;https://petsymposium.org/popets/2019/popets-2019-0029.php&quot;&gt;SMM18&lt;/a&gt;].
The PLD of a mechanism is defined as:
\[
    \omega(y) = \mathbb{P}_{o\sim\mathcal{M}(D)}\left[\mathcal{L}_{D,D’}(o)=y\right].
\]&lt;/p&gt;

&lt;p&gt;In the sequential composition setting, PLDs can be used for tight privacy analysis. 
This relies on a conceptually simple result: if \(\omega\) is the PLD of \(\mathcal{M}\) and \(\omega’\) is the PLD of \(\mathcal{M}’\) on neighboring databases \(D\), \(D’\), then the PLD of the composition of \(\mathcal{M}\) and \(\mathcal{M}’\) is \(\omega\ast\omega’\), where \(\ast\) is the convolution operator.
Of course, when doing privacy accounting, we don’t want \(\omega\) and \(\omega’\) to depend on the pair of databases, so we replace them by &lt;em&gt;worst-case&lt;/em&gt; PLDs, that are “larger” than all possible PLDs for neighboring databases.&lt;/p&gt;

&lt;p&gt;Using PLDs for privacy accounting can be done numerically [&lt;a href=&quot;https://eprint.iacr.org/2017/1034&quot;&gt;MM18&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/2102.12412&quot;&gt;KJH20&lt;/a&gt;, &lt;a href=&quot;http://proceedings.mlr.press/v130/koskela21a.html&quot;&gt;KJPH21&lt;/a&gt;, &lt;a href=&quot;https://proceedings.neurips.cc/paper_files/paper/2021/hash/6097d8f3714205740f30debe1166744e-Abstract.html&quot;&gt;GLW21&lt;/a&gt;, &lt;a href=&quot;https://proceedings.mlr.press/v162/ghazi22a.html&quot;&gt;GKKM22&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/2207.04380&quot;&gt;DGKKM22&lt;/a&gt;] or analytically [&lt;a href=&quot;https://proceedings.mlr.press/v151/zhu22c.html&quot;&gt;ZDW22&lt;/a&gt;].
This family of approaches is convenient because it is very generic: DP frameworks can use a tight upper bound PLD when known, and fall back to a worst-case PLD corresponding to \(\varepsilon\)-DP or \((\varepsilon,\delta)\)-DP when the mechanism is too complex.
Unfortunately, the composition result mentioned above has only been proven in the sequential composition setting [&lt;a href=&quot;https://eprint.iacr.org/2017/1034&quot;&gt;MM18&lt;/a&gt;].
Extending it to adaptive composition is straightforward, but extending it to the fully adaptive setting (with privacy filters) or the concurrent setting does not seem trivial.&lt;/p&gt;

&lt;p&gt;This leads us to our last open question: can these privacy accounting techniques be used in the fully adaptive or concurrent settings?&lt;/p&gt;

&lt;h3 id=&quot;summary&quot;&gt;Summary&lt;/h3&gt;

&lt;p&gt;In this blog post, we gave a high-level overview of different settings and variants of composition theorems.
Along the way, we listed a number of natural open questions.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Can we define a setting that generalizes both fully adaptive composition and concurrent composition? What composition results hold in that setting?&lt;/li&gt;
  &lt;li&gt;Can we “fill in the blanks” among existing composition results? Namely, can we prove optimal composition results for \((\varepsilon,\delta)\)-DP and \(f\)-DP in the fully adaptive setting, and for \((\varepsilon,\delta)\)-approximate zCDP in the concurrent setting?&lt;/li&gt;
  &lt;li&gt;In the fully adaptive setting with privacy filters, can one reorder mechanisms when computing their cumulative privacy loss, to optimize the privacy accounting?&lt;/li&gt;
  &lt;li&gt;Can we prove fully adaptive and concurrent composition results for privacy accounting based on privacy loss distributions?&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Progress on these open questions would either uncover surprising additional separation results, or enable usability and utility improvements to general-purpose DP infrastructure.
We’re excited about both prospects!&lt;/p&gt;

&lt;hr /&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://tmlt.dev&quot;&gt;Tumult Analytics&lt;/a&gt; is a differential privacy framework used by institutions such as the U.S. Census Bureau, the IRS, or the Wikimedia Foundation. It is developed by &lt;a href=&quot;https://tmlt.io&quot;&gt;Tumult Labs&lt;/a&gt;, the employer of the author of this blog post. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <author>
        
            <name>Damien Desfontaines</name>
        
        </author>
        <pubDate>Mon, 18 Sep 2023 21:00:00 -0400</pubDate>
        <link>http://localhost:4000/open-problems-how-generic-can-composition-be/</link>
        <guid isPermaLink="true">http://localhost:4000/open-problems-how-generic-can-composition-be/</guid>
      </item>
    
      <item>
        <title>Beyond Global Sensitivity via Inverse Sensitivity</title>
        <description>&lt;p&gt;The most well-known and widely-used method for achieving differential privacy is to compute the true function value \(f(x)\) and then add Laplace or Gaussian noise scaled to the &lt;em&gt;global sensitivity&lt;/em&gt; of \(f\). 
This may be overly conservative. In this post we’ll show how we can do better.&lt;/p&gt;

&lt;p&gt;The global sensitivity of a function \(f : \mathcal{X}^* \to \mathbb{R}\) is defined by \[ \mathsf{GS}_f := \sup_{x,x’\in\mathcal{X}^* : \mathrm{dist}(x,x’) \le 1} |f(x)-f(x’)|, \tag{1}\] where \(\mathrm{dist}(x,x’)\le 1\) denotes that \(x\) and \(x’\) are neighbouring datasets (i.e. they differ only by the addition, removal, or replacement of one person’s data); more generally, \(\mathrm{dist}(\cdot,\cdot)\) is the corresponding metric on datasets (i.e., Hamming distance).&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;The global sensitivity considers datasests that have nothing to do with the dataset at hand and which could be completely unrealistic.
Many functions have infinite global sensitivity, but, on reasonably nice datasets, their &lt;em&gt;local sensitivity&lt;/em&gt; is much lower.&lt;/p&gt;

&lt;h2 id=&quot;local-sensitivity&quot;&gt;Local Sensitivity&lt;/h2&gt;

&lt;p&gt;The local sensitivity of a function \(f : \mathcal{X}^* \to \mathbb{R}\) at \(x \in \mathcal{X}^*\) at distance \(k\) is defined by \[\mathsf{LS}^k_f(x) := \sup_{x’\in\mathcal{X}^* : \mathrm{dist}(x,x’) \le k} |f(x)-f(x’)|. \tag{2}\]
Often, we fix \(k=1\) and we may drop the superscript: \(\mathsf{LS}_f(x) := \mathsf{LS}_t^1(x)\).
Note that the local sensitivity is always at most the global sensitivity: \(\mathsf{LS}_f^k(x) \le k \cdot \mathsf{GS}_f\).&lt;/p&gt;

&lt;p&gt;As a concrete example, the median has infinite global sensitivity, but for realistic data the local sensitivity is quite reasonable. 
Specifically, \[\mathsf{LS}^k_{\mathrm{median}}(x_1, \cdots, x_n) = \max\left\{ \left|x_{(\tfrac{n+1}{2})}-x_{(\tfrac{n+1}{2}+k)}\right|, \left|x_{(\tfrac{n+1}{2})}-x_{(\tfrac{n+1}{2}-k)}\right| \right\},\tag{3}\] where \( x_{(1)} \le x_{(2)} \le \cdots \le x_{(n)}\) denotes the input in &lt;a href=&quot;https://en.wikipedia.org/wiki/Order_statistic&quot;&gt;sorted order&lt;/a&gt; and \(n\) is assumed to be odd, so, in particular, \(\mathrm{median}(x_1, \cdots, x_n) = x_{(\tfrac{n+1}{2})}\).
For example, if \(X_1, \cdots X_n\) are i.i.d. samples from a standard Gaussian and \(k \ll n\), then \(\mathsf{LS}^k_{\mathrm{median}}(X_1, \cdots, X_n) \le O(k/n)\) with high probability.&lt;/p&gt;

&lt;h2 id=&quot;using-local-sensitivity&quot;&gt;Using Local Sensitivity&lt;/h2&gt;

&lt;p&gt;Intuitively, the local sensitivity is the “real” sensitivity of the function and the global sensitivity is only a worst-case upper bound.
Thus it seems natural to add noise scaled to the local sensitivity instead of the global sensitivity.&lt;/p&gt;

&lt;p&gt;Unfortunately, naïvely adding noise scaled to local sensitivity doesn’t satisfy differential privacy. 
The problem is that the local sensitivity itself can reveal information.
For example, consider the median on the inputs \(x=(1,2,2),x’=(2,2,2)\). The output distributions of the algorithm on these two inputs must be similar.
In both cases the median is \(2\), so that is a good start for ensuring that the distributions are similar. 
But the local sensitivity is different: \(\mathsf{LS}^1_{\mathrm{median}}(x)=1\) versus \(\mathsf{LS}^1_{\mathrm{median}}(x’)=0\). 
So, if we add noise scaled to local sensitivity, then, on input \(x’\), we deterministically output \(2\), while, on input \(x\), we output a random number. If we use continuous Laplace or Gaussian noise, then the random number will be a non-integer almost surely. Thus the output perfectly distinguishes the two inputs, which is a catastrophic violation of differential privacy.&lt;/p&gt;

&lt;p&gt;The good news is that we can exploit local sensitivity; we just need to do a bit more work.
In fact, there are many methods in the differential privacy literature to exploit local sensitivity.&lt;/p&gt;

&lt;p&gt;The best-known methods for exploiting local sensitivity are &lt;em&gt;smooth sensitivity&lt;/em&gt; [&lt;a href=&quot;https://cs-people.bu.edu/ads22/pubs/NRS07/NRS07-full-draft-v1.pdf&quot; title=&quot;Kobbi Nissim, Sofya Raskhodnikova, Adam Smith. Smooth Sensitivity and Sampling in Private Data Analysis. STOC 2007.&quot;&gt;NRS07&lt;/a&gt;]&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; and &lt;em&gt;propose-test-release&lt;/em&gt; [&lt;a href=&quot;https://www.stat.cmu.edu/~jinglei/dl09.pdf&quot; title=&quot;Cynthia Dwork, Jing Lei. Differential Privacy and Robust Statistics. STOC 2009.&quot;&gt;DL09&lt;/a&gt;]&lt;sup id=&quot;fnref:3&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;In this post we will cover a different general-purpose technique. This technique is folklore.&lt;sup id=&quot;fnref:4&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt; It was first systematically studied by Asi and Duchi [&lt;a href=&quot;https://arxiv.org/abs/2005.10630&quot; title=&quot;Hilal Asi, John Duchi. Near Instance-Optimality in Differential Privacy. 2020.&quot;&gt;AD20&lt;/a&gt;,&lt;a href=&quot;https://papers.nips.cc/paper/2020/hash/a267f936e54d7c10a2bb70dbe6ad7a89-Abstract.html&quot; title=&quot;Hilal Asi, John Duchi. Instance-optimality in differential privacy via approximate inverse sensitivity mechanisms. NeurIPS 2020.&quot;&gt;AD20&lt;/a&gt;], who also named the method the &lt;em&gt;inverse sensitivity mechanism&lt;/em&gt;.&lt;/p&gt;

&lt;h2 id=&quot;the-inverse-sensitivity-mechanism&quot;&gt;The Inverse Sensitivity Mechanism&lt;/h2&gt;

&lt;p&gt;Consider a function \(f : \mathcal{X}^* \to \mathcal{Y}\).
Our goal is to estimate \(f(x)\) in a differentially private manner.
But we do not make any assumptions about the global sensitivity of the function.&lt;/p&gt;

&lt;p&gt;For simplicity we will assume that \(\mathcal{Y}\) is finite and that \(f\) is &lt;a href=&quot;https://en.wikipedia.org/wiki/Surjective_function&quot;&gt;surjective&lt;/a&gt;.&lt;sup id=&quot;fnref:5&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:5&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;Now we define a loss function \(\ell : \mathcal{X}^* \times \mathcal{Y} \to \mathbb{Z}_{\ge0}\) by \[\ell(x,y) := \min\left\{ \mathrm{dist}(x,\tilde{x}) : \tilde{x}\in\mathcal{X}^*, f(\tilde{x})=y \right\}.\tag{4}\]
In other words, \(\ell(x,y)\) measures how many entries of \(x\) we need to add or remove until \(f(x)=y\). 
Yet another way to think of it is that \(\ell(x,y)\) is the distance from the point \(x\) to the set \(f^{-1}(y)\). (Hence the name inverse sensitivity.)&lt;/p&gt;

&lt;p&gt;The loss is minimized by the desired answer: \(\ell(x,f(x))=0\). Intuitively, the loss \(\ell(x,y)\) increases as \(y\) moves further from \(f(x)\). So approximately minimizing this loss should produce a good approximation to \(f(x)\), as desired.&lt;/p&gt;

&lt;p&gt;The trick is that this loss always has bounded global sensitivity – i.e., \(\mathsf{GS}_\ell \le 1\) – no matter what the sensitivity of \(f\) is!&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Lemma 1.&lt;/strong&gt; Let \(f : \mathcal{X}^* \to \mathcal{Y}\) be arbitrary and define \(\ell : \mathcal{X}^* \times \mathcal{Y} \to \mathbb{Z}_{\ge0}\) as in Equation 4. Then, for all \(x,x’\in\mathcal{X}^*\) with \(\mathrm{dist}(x,x’)\le 1\) and all \(y \in \mathcal{Y}\), we have \(|\ell(x,y)-\ell(x’,y)|\le 1\).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;em&gt;Proof.&lt;/em&gt; 
Fix \(x,x’\in\mathcal{X}^*\) with \(\mathrm{dist}(x,x’)\le 1\) and \(y \in \mathcal{Y}\).
Let \(\widehat{x} \in\mathcal{X}^*\) satisfy \(\ell(x,y)=\mathrm{dist}(x,\widehat{x})\) and \(f(\widehat{x})=y\).
By definition, \[\ell(x’,y) = \min\left\{ \mathrm{dist}(x’,\tilde{x}) : f(\tilde{x})=y \right\} \le \mathrm{dist}(x’,\widehat{x}).\]
By the triangle inequality, \[\mathrm{dist}(x’,\widehat{x}) \le \mathrm{dist}(x’,x)+\mathrm{dist}(x,\widehat{x}) \le 1 + \ell(x,y).\]
Thus \(\ell(x’,y) \le \ell(x,y)+1\) and, by symmetry, \(\ell(x,y) \le \ell(x’,y)+1\), as required. ∎&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This means that we can run the exponential mechanism [&lt;a href=&quot;https://ieeexplore.ieee.org/document/4389483&quot; title=&quot;Frank McSherry, Kunal Talwar. Mechanism Design via Differential Privacy. FOCS 2007.&quot;&gt;MT07&lt;/a&gt;] to select from \(\mathcal{Y}\) using the loss \(\ell\).&lt;sup id=&quot;fnref:6&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:6&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt; That is, the inverse sensitivity mechanism is defined by 
\[\forall y \in \mathcal{Y} ~~~~~ \mathbb{P}[M(x)=y] ;= \frac{\exp\left(-\frac{\varepsilon}{2}\ell(x,y)\right)}{\sum_{y’\in\mathcal{Y}}\exp\left(-\frac{\varepsilon}{2}\ell(x,y’)\right)}.\tag{5}\] 
By the properties of the exponential mechanism and Lemma 1, \(M\) satisfies differential privacy:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Theorem 2. (Privacy of the Inverse Sensitivity Mechanism)&lt;/strong&gt; Let \(M : \mathcal{X}^* \to \mathcal{Y}\) be as defined in Equation 5 with the loss from Equation 4. Then \(M\) satisfies \(\varepsilon\)-differential privacy (&lt;a href=&quot;/exponential-mechanism-bounded-range/&quot;&gt;and \(\frac18\varepsilon^2\)-zCDP&lt;/a&gt;).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;utility-guarantee&quot;&gt;Utility Guarantee&lt;/h2&gt;

&lt;p&gt;The privacy guarantee of the inverse sensitivity mechanism is easy and, in particular, it doesn’t depend on the properties of \(f\).
This means that the utility will need to depend on the properties of \(f\).&lt;/p&gt;

&lt;p&gt;By the standard properties of the exponential mechanism, we can guaranatee that the output has low loss:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Lemma 3.&lt;/strong&gt; Let \(M : \mathcal{X}^* \to \mathcal{Y}\) be as defined in Equation 5 with the loss from Equation 4. For all inputs \(x \in \mathcal{X}^*\) and all \(\beta\in(0,1)\), we have \[\mathbb{P}\left[\ell(x,M(x)) &amp;lt; \frac2\varepsilon\log\left(\frac{|\mathcal{Y}|}{\beta}\right) \right] \ge 1-\beta.\tag{6}\]&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;em&gt;Proof.&lt;/em&gt;
Let \(B_x = \left\{ y \in \mathcal{Y} : \ell(x,y) \ge \frac2\varepsilon\log\left(\frac{|\mathcal{Y}|}{\beta}\right) \right\}\) be the subset of \(\mathcal{Y}\) with high loss.
Then \[ \mathbb{P}[M(x)\in B_x] = \frac{\sum_{y \in B_x} \exp\left(-\frac{\varepsilon}{2}\ell(x,y)\right)}{\sum_{y’\in\mathcal{Y}}\exp\left(-\frac{\varepsilon}{2}\ell(x,y’)\right)} \]\[ \le \frac{|B_x| \cdot \exp\left(-\frac{\varepsilon}{2}\frac2\varepsilon\log\left(\frac{|\mathcal{Y}|}{\beta}\right) \right)}{\exp\left(-\frac{\varepsilon}{2}\ell(x,f(x))\right)}\]\[= \frac{|B_x| \cdot \frac{\beta}{|\mathcal{Y}|}}{1} \le \beta, \] as required. ∎&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Now we need to translate this loss bound into something easier to interpret – local sensitivity.&lt;/p&gt;

&lt;p&gt;Suppose \(y \gets M(x)\). Then we have some loss \(k=\ell(x,y)\). What this means is that there exists \(\tilde{x}\in\mathcal{X}^*\) with \(f(\tilde{x})=y\) and \(\mathrm{dist}(x,\tilde{x})\le k\). By the definition of local sensitivity, \(|f(x)-y| = |f(x)-f(\tilde{x})| \le \mathsf{LS}_f^k(x)\). This means we can translate the loss guarantee of Lemma 3 into an accuracy guarantee in terms of local sensitivity:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Theorem 4. (Utility of the Inverse Sensitivity Mechanism)&lt;/strong&gt; Let \(M : \mathcal{X}^* \to \mathcal{Y}\) be as defined in Equation 5 with the loss from Equation 4. For all inputs \(x \in \mathcal{X}^*\) and all \(\beta\in(0,1)\), we have \[\mathbb{P}\left[\left|M(x)-f(x)\right| \le \mathsf{LS}_f^k(x) \right] \ge 1-\beta,\tag{7}\] where \(k=\left\lfloor\frac2\varepsilon\log\left(\frac{|\mathcal{Y}|}{\beta}\right)\right\rfloor\).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;We can tie this back to our concrete example of the median. Per Equation 3, \[\mathsf{LS}^k_{\mathrm{median}}(x_1, \cdots, x_n) \le \left|x_{(\tfrac{n+1}{2}+k)}-x_{(\tfrac{n+1}{2}-k)}\right| .\]
Thus the error guarantee of Theorem 4 for the median would scale with the spread of the data. E.g., if \(k=\tfrac{n+1}{4}\), then  \(\mathsf{LS}^k_{\mathrm{median}}(x_1, \cdots, x_n)\) is at most the interquartile range of the data.&lt;/p&gt;

&lt;p&gt;How does this compare with the usual global sensitivity approach?
The \(\varepsilon\)-differentially private Laplace mechanism is given by \(\widehat{M}(x):=f(x)+\mathsf{Laplace}(\mathsf{GS}_f/\varepsilon)\). For all \(x \in \mathcal{X}^*\) and all \(\beta\in(0,1/2)\), we have the utility guarantee \[\mathbb{P}\left[\left|\widehat{M}(x)-f(x)\right| \le \mathsf{GS}_f \cdot \frac1\varepsilon \log\left(\frac{1}{2\beta}\right) \right] \ge 1-\beta.\tag{8}\]
Comparing Equations 7 and 8, we see that neither guarantee dominates the other. On one hand, the local sensitivity can be much smaller than the global sensitivity. On the other hand, we pick up a dependence on \(\log|\mathcal{Y}|\). In particular, in the worst case where the local sensitivity matches the global sensitivity \(\mathsf{LS}_f^k(x)=k\cdot\mathsf{GS}_f\), the inverse sensitivity mechanism is worse by a factor of \[\frac{\mathsf{LS}_f^k(x)}{\mathsf{GS}_f \cdot \frac1\varepsilon \log\left(\frac{1}{2\beta}\right)} = 2 \frac{\log(2|\mathcal{Y}|)}{\log(1/2\beta)}+2.\tag{9}\]
Hence the inverse sensitivity mechanism is most useful in situations where the local sensitivity is significantly smaller than the global sensitivity.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;In this post we’ve covered the inverse sensitivity mechanism and showed that it is private regardless of the sensitivity of the function \(f\) and we showed that it gives error guarantees that scale with the local sensitivity of \(f\), rather than its global sensitivity.&lt;/p&gt;

&lt;p&gt;The inverse sensitivity mechanism is a simple demonstration that there is more to differential privacy than simply adding noise scaled to global sensitivity; there are many more techniques in the literature.&lt;/p&gt;

&lt;p&gt;The inverse sensitivity mechanism has two main limitations. First, it is, in general, not computationally efficient. Computing the loss function is intractable for an arbitrary \(f\) (but can be done efficiently for simple examples like the median). Second, the \(\log|\mathcal{Y}|\) term in the accuracy guarantee is problematic when the output space is large, such as when we have high-dimensional outputs. 
While there are other techniques that can be used instead of inverse sensitivity, they suffer from some of the same limitations. Thus finding ways around these limitations is an &lt;a href=&quot;/colt23-bsp/&quot;&gt;active research topic&lt;/a&gt; [&lt;a href=&quot;https://arxiv.org/abs/1905.13229&quot; title=&quot;Mark Bun, Gautam Kamath, Thomas Steinke, Zhiwei Steven Wu. Private Hypothesis Selection. NeurIPS 2019.&quot;&gt;BKSW19&lt;/a&gt;,&lt;a href=&quot;https://cse.hkust.edu.hk/~yike/ShiftedInverse.pdf&quot; title=&quot;Juanru Fang, Wei Dong, Ke Yi. Shifted Inverse: A General Mechanism for Monotonic Functions under User Differential Privacy. CCS 2022.&quot;&gt;FDY22&lt;/a&gt;,&lt;a href=&quot;https://arxiv.org/abs/2212.05015&quot; title=&quot;Samuel B. Hopkins, Gautam Kamath, Mahbod Majid, Shyam Narayanan. Robustness Implies Privacy in Statistical Estimation. STOC 2023.&quot;&gt;HKMN23&lt;/a&gt;,&lt;a href=&quot;https://arxiv.org/abs/2301.07078&quot; title=&quot;John Duchi, Saminul Haque, Rohith Kuditipudi. A Fast Algorithm for Adaptive Private Mean Estimation. COLT 2023.&quot;&gt;DHK23&lt;/a&gt;,&lt;a href=&quot;https://arxiv.org/abs/2301.12250&quot; title=&quot;Gavin Brown, Samuel B. Hopkins, Adam Smith. Fast, Sample-Efficient, Affine-Invariant Private Mean and Covariance Estimation for Subgaussian Distributions. COLT 2023.&quot;&gt;BHS23&lt;/a&gt;,&lt;a href=&quot;https://arxiv.org/abs/2302.01855&quot; title=&quot;Hilal Asi, Jonathan Ullman, Lydia Zakynthinou. From Robustness to Privacy and Back. 2023.&quot;&gt;AUZ23&lt;/a&gt;].&lt;/p&gt;

&lt;p&gt;We leave you with a riddle: What can we do if even the local sensitivity of our function is unbounded? For example, suppose we want to approximate \(f(x) = \max_i x_i\). Surprisingly, there are still things we can do and we will write a follow-up post on this.&lt;/p&gt;

&lt;hr /&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;We define \(\mathcal{X}^* = \bigcup_{n = 0}^\infty \mathcal{X}^n\) to be the set of all input tuples of arbitrary size. The metric \(\mathrm{dist} : \mathcal{X}^* \times \mathcal{X}^* \to \mathbb{R}\) can be arbitrary. E.g. we can allow addition, removal, and/or replacement of an individual’s data. For simplicity, we consider univariate functions here. But the definitions of global and local sensitivity easily extend to to vector-valued functions by taking a norm: \[ \mathsf{GS}_f := \sup_{x,x’\in\mathcal{X}^* : \mathrm{dist}(x,x’) \le 1} \|f(x)-f(x’)\|.\] If we use the 2-norm, then this cleanly corresponds to adding spherical Gaussian noise. The 1-norm corresponds to adding independent Laplace noise to the coordinates. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Briefly, smooth sensitivity is an upper bound on the local sensitivity which itself has low sensitivity in a multiplicative sense. That is, \(\mathsf{LS}_f^1(x) \le \mathsf{SS}_f^t(x)\) and \(\mathsf{SS}_f^t(x) \le e^t \cdot \mathsf{SS}_f^t(x’) \) for neighbouring \(x,x’\). This suffices to ensure that we can add noise scaled to \(\mathsf{SS}_f^t(x)\). However, that noise usually needs to be more heavy-tailed than for global sensitivity [&lt;a href=&quot;https://proceedings.neurips.cc/paper/2019/hash/3ef815416f775098fe977004015c6193-Abstract.html&quot; title=&quot;Mark Bun, Thomas Steinke. Average-Case Averages: Private Algorithms for Smooth Sensitivity and Mean Estimation. NeurIPS 2019.&quot;&gt;BS19&lt;/a&gt;]. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Roughly, the propose-test-release framework computes an upper bound on the local sensitivity in a differentially private manner and then uses this upper bound as the noise scale. (We hope to give more detail about both propose-test-release and smooth sensitivity in future posts.) &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Properly attributing the inverse sensitivity mechanism is difficult. The  earliest published instances of the inverse sensitivity mechanism of which we are aware of are from 2011 and 2013 [&lt;a href=&quot;https://www.cs.columbia.edu/~rwright/Publications/pods11.pdf&quot; title=&quot;Darakhshan Mir, S. Muthukrishnan, Aleksandar Nikolov, Rebecca N. Wright. Pan-private algorithms via statistics on sketches. PODS 2011.&quot;&gt;MMNW11&lt;/a&gt;§3.1,&lt;a href=&quot;hhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC4681528/&quot; title=&quot;Aaron Johnson, Vitaly Shmatikov. Privacy-preserving data exploration in genome-wide association studies. KDD 2013.&quot;&gt;JS13&lt;/a&gt;§5]; but this was not novel even then. Asi and Duchi [&lt;a href=&quot;https://arxiv.org/abs/2005.10630&quot; title=&quot;Hilal Asi, John Duchi. Near Instance-Optimality in Differential Privacy. 2020.&quot;&gt;AD20&lt;/a&gt;§1.2] state that McSherry and Talwar [&lt;a href=&quot;https://ieeexplore.ieee.org/document/4389483&quot; title=&quot;Frank McSherry, Kunal Talwar. Mechanism Design via Differential Privacy. FOCS 2007.&quot;&gt;MT07&lt;/a&gt;] considered it in 2007. In any case, the name we use was coined in 2020 [&lt;a href=&quot;https://arxiv.org/abs/2005.10630&quot; title=&quot;Hilal Asi, John Duchi. Near Instance-Optimality in Differential Privacy. 2020.&quot;&gt;AD20&lt;/a&gt;]. &lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:5&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Assuming that the output space \(\mathcal{Y}\) is finite is a significant assumption. While it can be relaxed a bit, it is to some extent an unavoidable limitation. For example, to apply the inverse sensitivity mechanism to the median, we must discretize and bound the inputs; bounding the inputs does impose a finite global sensitivity, but the dependence on the bound is logarithmic, so the bound can be fairly large. Assuming that the function is surjective is a minor assumption that ensures that the loss in Equation 4 is always well-defined; otherwise we can define the loss to be infinite for points that are not in the range of the function. &lt;a href=&quot;#fnref:5&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:6&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Note that we can use other selection algorithms, such as permute-and-flip [&lt;a href=&quot;https://arxiv.org/abs/2010.12603&quot; title=&quot;Ryan McKenna, Daniel Sheldon. Permute-and-Flip: A new mechanism for differentially private selection. NeurIPS 2020.&quot;&gt;MS20&lt;/a&gt;] or report-noisy-max [&lt;a href=&quot;https://arxiv.org/abs/2105.07260&quot; title=&quot;Zeyu Ding, Daniel Kifer, Sayed M. Saghaian N. E., Thomas Steinke, Yuxin Wang, Yingtai Xiao, Danfeng Zhang. The Permute-and-Flip Mechanism is Identical to Report-Noisy-Max with Exponential Noise. 2021.&quot;&gt;DKSSWXZ21&lt;/a&gt;] or gap-max [&lt;a href=&quot;https://arxiv.org/abs/1409.2177&quot; title=&quot;Kamalika Chaudhuri, Daniel Hsu, Shuang Song. The Large Margin Mechanism for Differentially Private Maximization. NIPS 2014.&quot;&gt;CHS14&lt;/a&gt;,&lt;a href=&quot;https://dl.acm.org/doi/10.1145/3188745.3188946&quot; title=&quot; Mark Bun, Cynthia Dwork, Guy N. Rothblum, Thomas Steinke. Composable and versatile privacy via truncated CDP. STOC 2018.&quot;&gt;BDRS18&lt;/a&gt;,&lt;a href=&quot;https://arxiv.org/abs/1905.13229&quot; title=&quot;Mark Bun, Gautam Kamath, Thomas Steinke, Zhiwei Steven Wu. Private Hypothesis Selection. NeurIPS 2019.&quot;&gt;BKSW19&lt;/a&gt;]. &lt;a href=&quot;#fnref:6&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <author>
        
            <name>Thomas Steinke</name>
        
        </author>
        <pubDate>Tue, 05 Sep 2023 09:00:00 -0700</pubDate>
        <link>http://localhost:4000/inverse-sensitivity/</link>
        <guid isPermaLink="true">http://localhost:4000/inverse-sensitivity/</guid>
      </item>
    
      <item>
        <title>Covariance-Aware Private Mean Estimation, Efficiently</title>
        <description>&lt;p&gt;Last week, the Mark Fulk award for best student paper at &lt;a href=&quot;https://learningtheory.org/colt2023/&quot;&gt;COLT 2023&lt;/a&gt; was awarded to the following two papers on private mean estimation:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2301.07078&quot;&gt;A Fast Algorithm for Adaptive Private Mean Estimation&lt;/a&gt;, by &lt;a href=&quot;https://web.stanford.edu/~jduchi/&quot;&gt;John Duchi&lt;/a&gt;, &lt;a href=&quot;https://dblp.org/pid/252/5821.html&quot;&gt;Saminul Haque&lt;/a&gt;, and &lt;a href=&quot;https://web.stanford.edu/~rohithk/&quot;&gt;Rohith Kuditipudi&lt;/a&gt; &lt;strong&gt;[&lt;a href=&quot;https://arxiv.org/abs/2301.07078&quot;&gt;DHK23&lt;/a&gt;]&lt;/strong&gt;;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2301.12250&quot;&gt;Fast, Sample-Efficient, Affine-Invariant Private Mean and Covariance Estimation for Subgaussian Distributions&lt;/a&gt; by &lt;a href=&quot;https://cs-people.bu.edu/grbrown/&quot;&gt;Gavin Brown&lt;/a&gt;, &lt;a href=&quot;https://www.samuelbhopkins.com/&quot;&gt;Samuel B. Hopkins&lt;/a&gt;, and &lt;a href=&quot;https://cs-people.bu.edu/ads22/&quot;&gt;Adam Smith&lt;/a&gt; &lt;strong&gt;[&lt;a href=&quot;https://arxiv.org/abs/2301.12250&quot;&gt;BHS23&lt;/a&gt;]&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The main result of both papers is the same: the first computationally-efficient \(O(d)\)-sample algorithm for differentially-private Gaussian mean estimation in Mahalanobis distance. 
In this post, we’re going to unpack the result and explain what this means.&lt;/p&gt;

&lt;p&gt;Gaussian mean estimation is a classic statistical task: given \(X_1, \dots, X_n \in \mathbb{R}^d\) sampled i.i.d. from a \(d\)-dimensional Gaussian \(N(\mu, \Sigma)\), output an vector \(\hat \mu \in \mathbb{R}^d\) that approximates the true mean \(\mu \in \mathbb{R}^d\). 
But what do we mean by &lt;em&gt;approximates&lt;/em&gt;? 
What distance measure should we use?
A reasonable first guess is the \(\ell_2\)-norm: output an estimate \(\hat \mu\) that minimizes \(\|\hat \mu - \mu\|_2\).&lt;/p&gt;

&lt;p&gt;However, we would ideally measure the quality of an estimate in an &lt;em&gt;affine-invariant&lt;/em&gt; manner: if the problem instance (i.e., the estimate, the dataset, and the underlying distribution) is shifted and rescaled, then the error should remain unchanged. 
Affine invariance allows us to perform such transformations of our data and not artificially make the problem easier or harder.
This property clearly isn’t satisfied by the \(\ell_2\)-norm: simply scaling the problem down would allow us to report an estimate with arbitrarily low error.
In other words, the distance metric needs to be calibrated to the covariance \(\Sigma \in \mathbb{R}^{d \times d}\).&lt;/p&gt;

&lt;p&gt;Instead, we consider error measured according to the &lt;em&gt;Mahalanobis distance&lt;/em&gt;: output an estimate \(\hat \mu\) that minimizes \(\|\Sigma^{-1/2}(\hat \mu - \mu)\|_2\), where \(\Sigma\) is the (unknown) covariance of the underlying distribution.
Note that, if the covariance matrix \(\Sigma = I\), then this reduces to the \(\ell_2\)-distance.
Indeed, a valid interpretation of the Mahalanobis distance is to imagine rescaling the problem so that the covariance \(\Sigma\) is mapped to the identity matrix, and measuring \(\ell_2\)-distance after this transformation.
A common way to think about Mahalanobis distance operationally is that it necessitates a more accurate estimate in directions with small variance, while permitting more error in directions with large variance.&lt;/p&gt;

&lt;p&gt;OK, so how do we learn the mean of a Gaussian in Mahalanobis distance?
In the non-private setting, the answer is simple: just take the empirical mean \(\hat \mu = \frac{1}{n} \sum_{i=1}^n X_i\)!
It turns out that with \(O(d)\) samples, the empirical mean provides an accurate estimate (in Mahalanobis distance) of the true mean \(\mu\).
Note that these guarantees hold regardless of the true covariance matrix \(\Sigma\).&lt;/p&gt;

&lt;p&gt;It isn’t quite so easy when we want to do things privately.
The most natural way would be add noise to the empirical mean.
However, we first have to “clip” the datapoints (i.e., rescale any points that are “too large”) in order to limit the sensitivity of this statistic.
This is where the challenges arise: we would ideally like to clip the data based on the shape of the (unknown) covariance matrix \(\Sigma\) &lt;strong&gt;[&lt;a href=&quot;https://arxiv.org/abs/1805.00216&quot;&gt;KLSU19&lt;/a&gt;]&lt;/strong&gt;.
Deviating significantly from \(\Sigma\) would either introduce bias due to clipping too many points, or add excessive amounts of noise.
Unfortunately, the covariance matrix \(\Sigma\) is unknown, and privately estimating it (in an appropriate metric) requires \(\Omega(d^{3/2})\) samples &lt;strong&gt;[&lt;a href=&quot;https://arxiv.org/abs/2205.08532&quot;&gt;KMS22&lt;/a&gt;]&lt;/strong&gt;.
This is substantially larger than the \(O(d)\) sample complexity of non-private Gaussian mean estimation.
Furthermore, this covariance estimation step really is the bottleneck. 
Given a coarse estimate of \(\Sigma\), only \(O(d)\) additional samples are required to estimate the mean privately in Mahalanobis distance.
This leads to the intriguing question: is it possible to privately estimate the mean of a Gaussian &lt;em&gt;without&lt;/em&gt; explicitly estimating the covariance matrix?&lt;/p&gt;

&lt;p&gt;The answer is yes!
A couple years back, Brown, Gaboardi, Smith, Ullman, and Zakynthinou &lt;strong&gt;[&lt;a href=&quot;https://arxiv.org/abs/2106.13329&quot;&gt;BGSUZ21&lt;/a&gt;]&lt;/strong&gt; gave two different algorithms for private Gaussian mean estimation in Mahalanobis distance, which both require only \(O(d)\) samples.
Interestingly, the two algorithms are quite different from each other.
One simply adds noise to the empirical mean based on the empirical covariance matrix.
The other one turns to a technique from robust statistics, sampling a point with large &lt;em&gt;Tukey depth&lt;/em&gt; using the exponential mechanism.
As described here, neither of these methods is differentially private yet – they additionally require a pre-processing step which checks if the dataset is sufficiently well-behaved, which happens with high probability when the data is generated according to a Gaussian distribution.
The major drawback of both algorithms: they require exponential time to compute.&lt;/p&gt;

&lt;p&gt;The two awarded papers &lt;strong&gt;[&lt;a href=&quot;https://arxiv.org/abs/2301.07078&quot;&gt;DHK23&lt;/a&gt;]&lt;/strong&gt; and &lt;strong&gt;[&lt;a href=&quot;https://arxiv.org/abs/2301.12250&quot;&gt;BHS23&lt;/a&gt;]&lt;/strong&gt; resolve this issue, giving the first &lt;em&gt;computationally efficient&lt;/em&gt; \(O(d)\) sample algorithms for private mean estimation in Mahalanobis distance.
Interestingly, the algorithms in both papers follow the same recipe as the first algorithm mentioned above: add noise to the empirical mean based on the empirical covariance matrix.
The catch is that the empirical mean and covariance are replaced with &lt;em&gt;stable&lt;/em&gt; estimates of the empirical mean and covariance, where stability bounds how much the estimators can change due to modification of individual datapoints. 
Importantly, these stable estimators are efficient to compute.
Further details of these subroutines are beyond the scope of this post, but the final algorithm simply adds noise to the stably-estimated mean based on the stably-estimated covariance.
Different extensions of these results are explored in the two papers, including estimation of covariance, and mean estimation in settings where the distribution may be heavy-tailed or rank-deficient.&lt;/p&gt;

&lt;p&gt;Most of the algorithms described above are based on some notion of &lt;em&gt;robustness&lt;/em&gt;, thus suggesting connections to the mature literature on robust statistics.
These connections have been explored as far back as 2009, in foundational work by Dwork and Lei &lt;strong&gt;[&lt;a href=&quot;https://dl.acm.org/doi/10.1145/1536414.1536466&quot;&gt;DL09&lt;/a&gt;]&lt;/strong&gt;.
Over the last couple of years, there has been a flurry of renewed interest in links between robustness and privacy, including, e.g., &lt;strong&gt;[&lt;a href=&quot;https://arxiv.org/abs/1905.13229&quot;&gt;BKSW19&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/2002.09464&quot;&gt;KSU20&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/2112.03548&quot;&gt;KMV22&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/2111.06578&quot;&gt;LKO22&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/2111.12981&quot;&gt;HKM22&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/2211.00724&quot;&gt;GH22&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/2212.05015&quot;&gt;HKMN23&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/2212.08018&quot;&gt;AKTVZ23&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/2302.01855&quot;&gt;AUZ23&lt;/a&gt;]&lt;/strong&gt;, beyond those mentioned above.
For example, some works &lt;strong&gt;[&lt;a href=&quot;https://arxiv.org/abs/2211.00724&quot;&gt;GH22&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/2212.05015&quot;&gt;HKMN23&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/2302.01855&quot;&gt;AUZ23&lt;/a&gt;]&lt;/strong&gt; show that, under certain conditions, a robust estimator implies a private one, and vice versa.
The two awarded papers expand this literature in a somewhat different direction – the type of stability property considered leads to algorithms which qualitatively differ from those considered prior.
It will be interesting to see how private and robust estimation evolve together over the next several years.&lt;/p&gt;

&lt;p&gt;Congratulations once more to the authors of both awarded papers on their excellent results!&lt;/p&gt;
</description>
        <author>
        
            <name>Gautam Kamath</name>
        
        </author>
        <pubDate>Mon, 17 Jul 2023 12:00:00 -0400</pubDate>
        <link>http://localhost:4000/colt23-bsp/</link>
        <guid isPermaLink="true">http://localhost:4000/colt23-bsp/</guid>
      </item>
    
      <item>
        <title>Call for Papers - TPDP 2023 - Submission deadline July 7</title>
        <description>&lt;p&gt;The &lt;a href=&quot;https://tpdp.journalprivacyconfidentiality.org/2023/&quot;&gt;9th Workshop on the Theory and Practice of Differential Privacy (TPDP 2023)&lt;/a&gt; will take place in Boston September 27-28, 2023.
This is the first year the workshop is a standalone event. However, the &lt;a href=&quot;https://opendp.org/event/opendp-community-meeting-2023&quot;&gt;OpenDP community meeting&lt;/a&gt; is the following day (also in Boston). It is also moving from a one-day event to two days.&lt;/p&gt;

&lt;p&gt;The workshop is intended to bring together the DP research community to discuss new developments over the past year. The workshop is non-archival, so does not preclude publishing the work elsewhere.&lt;/p&gt;

&lt;p&gt;The submission deadline is July 7. Submissions should be 4 pages (plus references and appendices.)&lt;/p&gt;

&lt;p&gt;Submission website: &lt;a href=&quot;https://hcrp.cs.uchicago.edu&quot;&gt;https://hcrp.cs.uchicago.edu&lt;/a&gt;&lt;/p&gt;
</description>
        <author>
        
            <name>Thomas Steinke</name>
        
        </author>
        <pubDate>Wed, 28 Jun 2023 00:01:00 +0000</pubDate>
        <link>http://localhost:4000/tpdp2023/</link>
        <guid isPermaLink="true">http://localhost:4000/tpdp2023/</guid>
      </item>
    
      <item>
        <title>Open problem - Better privacy guarantees for larger groups</title>
        <description>&lt;p&gt;Consider a simple query counting the number of people in various mutually exclusive groups.
In the differential privacy literature, it is typical to assume that each of these groups should be subject to the same privacy loss: the noise added to each count has the same magnitude, and everyone gets the same privacy guarantees.
However, in settings where these groups have vastly different population sizes, larger populations may be willing to accept more error in exchange for stronger privacy protections.
In particular, in many use cases, &lt;em&gt;relative&lt;/em&gt; error (the noisy count is within 5% of the true value) matters more than absolute error (the noisy count is at a distance of at most 100 of the true value).
This leads to a natural question: can we use this fact to develop a mechanism that improves the privacy guarantees of individuals in larger groups, subject to a constraint on relative error?&lt;/p&gt;

&lt;h3 id=&quot;problem-definition&quot;&gt;Problem definition&lt;/h3&gt;

&lt;p&gt;Our goal is to obtain a mechanism which minimizes the overall privacy loss for each group without exceeding a relative error threshold for each group.
To formalize this goal, we first define a notion of per-group privacy we call group-wise zero-concentrated differential privacy as follows.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; &lt;em&gt;Group-wise zero-concentrated differential privacy.&lt;/em&gt;
Assume possible datasets consist of records from domain \(U\), and \(U\) can be partitioned into \(k\) fixed, disjoint groups \(U_1\), …, \(U_k\). Let \(v : \mathcal{D} \rightarrow \mathbb{R}^k\) be a function associating a dataset to a vector of privacy budgets (one per group). We say a mechanism \(\mathcal{M}\) satisfies \(v\)-group-wise zero-concentrated differential privacy (zCDP) if for any two datasets \(D\), \(D’\) differing in the addition or removal of a record in \(U_i\), and for all \(\alpha&amp;gt;1\), we have:
\[
D_\alpha\left(\mathcal{M}(D||\mathcal{M}(D’)\right) \le \alpha \cdot {v(D)}_i
\]
\[
D_\alpha\left(\mathcal{M}(D’)||\mathcal{M}(D)\right) \le \alpha \cdot {v(D)}_i
\]
where \(D_\alpha\) is the Rényi divergence of order \(\alpha\).&lt;/p&gt;

&lt;p&gt;This definition is similar to &lt;em&gt;tailored DP&lt;/em&gt;, defined in [&lt;a href=&quot;https://eprint.iacr.org/2014/982.pdf&quot;&gt;LP15&lt;/a&gt;]: each individual gets a different privacy guarantee, depending on which group they belong to;
this guarantee also depends on how many people are in this group.
We use zCDP as our definition of privacy due to its compatibility with the Gaussian mechanism; the same idea could easily be applied to other definitions like with Rényi DP or pure DP.&lt;/p&gt;

&lt;p&gt;From there we can give a more formal definition of the problem as follows. The goal is to minimize the privacy loss for each individual group, while keeping the error under a given threshold.
For larger groups that can accept more noise, this means adding more noise to achieve the smallest possible privacy loss.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Problem.&lt;/strong&gt;
Let \(r \in (0,1]\) be an acceptable level of relative error, and \(k\) be the number of distinct, mutually-exclusive partitions of domain \(X\).
Given a dataset \(D\), let \(x(D)\) be a vector containing the count of records in each partition.
The objective is to find a mechanism \(\mathcal{M}\) which takes in \(r\), \(k\), and \(D\) and outputs \(\hat{x}(D)\) such that \(E\left[\left|{x(D)}_i-{\hat{x}(D)}_i\right|\right]&amp;lt;r\cdot {x(D)}_i\) for all \(i\), and satisfies \(v\)-group-wise zCDP where \(v(D)_i\) is as small as possible for all \(i\).
&lt;br /&gt;
To prevent pathological mechanisms that optimize for specific datasets, we add two constraints to the problem: the privacy guarantee \(v(D)_i\) should only depend on \(x(D)_i\), and should be nonincreasing with \(x(D)_i\).&lt;/p&gt;

&lt;p&gt;Since the relative error thresholds are proportional to the population size, each population can tolerate a different amount of noise.
This means that to minimize the privacy loss for each group, the mechanism must add noise of different scales to each group.
Of course, directly using \(x(D)_i\) to determine the scale of the noise for group \(i\) leads to a privacy loss which is data dependent, similarly to e.g. PATE [&lt;a href=&quot;https://openreview.net/forum?id=HkwoSDPg&quot;&gt;PAEGT17&lt;/a&gt;], and as such should be treated as a protected value.&lt;/p&gt;

&lt;h3 id=&quot;an-example-mechanism&quot;&gt;An example mechanism&lt;/h3&gt;

&lt;p&gt;An example mechanism that seems like it could address this problem is as follows.
First, perform the original counting query and add Gaussian noise to satisfy \(\rho\)-zCDP.
Then, add additional Gaussian noise to each count, with a variance that depends on the noisy count itself — adding more noise to larger groups.
This mechanism is outlined in Algorithm 1.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Algorithm 1.&lt;/strong&gt;
&lt;em&gt;Adding data-dependent noise as a post-processing step.&lt;/em&gt;
&lt;br /&gt;
Require: A dataset \(D\) where each data point belongs to one of \(k\) groups, a privacy parameter \(\rho\), and a relative error rate \(r\).&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Let \(\sigma^2 = 1/(2\rho)\)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;For&lt;/strong&gt; \(i=1\) to \(k\) &lt;strong&gt;do&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;\(\qquad\) Let \(x_i\) be the number of people in \(D\) in group \(i\)&lt;/li&gt;
  &lt;li&gt;\(\qquad\) Sample \(X_i \sim \mathcal{N}(x_i, \sigma^2)\)&lt;/li&gt;
  &lt;li&gt;\(\qquad\) Sample \(Y_i \sim \mathcal{N}_{k}(X_i, (rX_i)^2)\)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;end for&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;return&lt;/strong&gt; \(Y_1,\dots,Y_k\)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Algorithm 1 achieves this goal of having approximately \(r\) error in each group: the total variance error of the mechanism is \(\sigma^2 + (rX)^2\), and \(X\) is a zCDP measure of \(f(D)\).
This mechanism satisfies at least \(\rho\)-zCDP: line 4 is an invocation of the Gaussian mechanism with privacy parameter \(\rho\), and line 5 is a post processing step and as such preserves the zCDP guarantee.
We would like to show that this algorithm also satisfies a stronger group-wise zCDP guarantee.&lt;/p&gt;

&lt;p&gt;This makes intuitive sense: line 5 adds additional Gaussian noise without using the private data directly.
Since the noise scale in line 5 is proportional to the total count in line 4, we expect the privacy guarantee to be significantly stronger for large groups with more noise.
Further, we can verify experimentally that when the data magnitude is large compared to the noise, the output distribution for each group is close to a Gaussian distribution.&lt;/p&gt;

&lt;p&gt;The below figure illustrates this finding.
We plot 1,000,000 sample outputs of Algorithm 1 (red) with parameters \(\sigma^2 = 100\) and \(r= 0.3\), and compare it to the best fit Gaussian distribution (black outline) with mean \(10,002.6\) and standard deviation of \(2995.1\).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/two-stage-noise-gaussian.png&quot; width=&quot;70%&quot; alt=&quot;A comparison between sample outputs of Algorithm 1 and the best-fit Gaussian distribution, showing that both match very closely.&quot; style=&quot;margin:auto;display: block;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;With parameters such as these, the output of the mechanism looks and behaves like a Gaussian distribution, which should be ideal to characterize the zCDP guarantee.
However, it is difficult to directly quantify this guarantee, due to the changing variance which is also a random variable.
Likewise, if the true count is close to zero or if the first instance of noise is large compared to the true count than the resulting distribution takes on a heavy skew and is no longer similar to a single Gaussian distribution.
Such distributions with randomized variances have not, to the best of our knowledge, been considered much in the literature, and we do not know whether the mechanism’s output distribution follows some well-studied distribution.&lt;/p&gt;

&lt;p&gt;The randomized variance also makes it difficult to bound the Rényi divergence of the distribution and characterize the zCDP guarantees directly.
Current privacy amplification techniques are insufficient, as those techniques consider adding additional noise where the noise parameters are independent of the data itself.&lt;/p&gt;

&lt;p&gt;Perhaps the most promising direction to understand more about such processes is the area of stochastic differential equations, where it is common to study noise with data-dependent variance.
The Bessel process [&lt;a href=&quot;http://www.stat.ucla.edu/~ywu/research/documents/StochasticDifferentialEquations.pdf&quot;&gt;Øks03&lt;/a&gt;] is an example of such a process, where the noise is dependent on the current value.
This process captures the noise added as post-processing (Line 5), but not the initial noise-addition step (Line 4).
Furthermore, to the best of our knowledge, the Bessel process and other value-dependent stochastic differential equations do not have closed-form solutions.&lt;/p&gt;

&lt;h3 id=&quot;goal&quot;&gt;Goal&lt;/h3&gt;

&lt;p&gt;We see two possible paths forward to address the original question. One path would be to obtain an analysis of Algorithm 1 which shows non-trivial improved privacy guarantees for larger groups.
We tried multiple approaches, but could not prove such a result.&lt;/p&gt;

&lt;p&gt;An alternative path would be to develop a different algorithm, which achieves better privacy guarantees for larger groups while maintaining the error below the relative error threshold for all groups.&lt;/p&gt;
</description>
        <author>
        
            <name>David Pujol</name>
        
            <name>Damien Desfontaines</name>
        
        </author>
        <pubDate>Mon, 26 Jun 2023 21:00:00 -0400</pubDate>
        <link>http://localhost:4000/open-problem-better-privacy-guarantees-for-larger-groups/</link>
        <guid isPermaLink="true">http://localhost:4000/open-problem-better-privacy-guarantees-for-larger-groups/</guid>
      </item>
    
      <item>
        <title>Composition Basics</title>
        <description>&lt;p&gt;Our data is subject to many different uses. Many entities will have access to our data and those entities will perform many different analyses that involve our data. The greatest risk to privacy is that an attacker will combine multiple pieces of information from the same or different sources and that the combination of these will reveal sensitive details about us.
Thus we cannot study privacy leakage in a vacuum; it is important that we can reason about the accumulated privacy leakage over multiple independent analyses, which is known as &lt;em&gt;composition&lt;/em&gt;. We have &lt;a href=&quot;/privacy-composition/&quot;&gt;previously discussed&lt;/a&gt; why composition is so important for differential privacy.&lt;/p&gt;

&lt;p&gt;This is the first in a series of posts on &lt;em&gt;composition&lt;/em&gt; in which we will explain in more detail how compositoin analyses work.&lt;/p&gt;

&lt;p&gt;Composition is quantitative. The differential privacy guarantee of the overall system will depend on the number of analyses and the privacy parameters that they each satisfy. The exact relationship between these quantities can be complex. There are various composition theorems that give bounds on the overall parameters in terms of the parameters of the parts of the system.&lt;/p&gt;

&lt;p&gt;The simplest composition theorem is what is known as basic composition, which applies to pure \(\varepsilon\)-DP (although it can be extended to approximate \((\varepsilon,\delta)\)-DP):&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt; (Basic Composition)
Let \(M_1, M_2, \cdots, M_k : \mathcal{X}^n \to \mathcal{Y}\) be randomized algorithms. Suppose \(M_j\) is \(\varepsilon_j\)-DP for each \(j \in [k]\).
Define \(M : \mathcal{X}^n \to \mathcal{Y}^k\) by \(M(x)=(M_1(x),M_2(x),\cdots,M_k(x))\), where each algorithm is run independently. Then \(M\) is \(\varepsilon\)-DP for \(\varepsilon = \sum_{j=1}^k \varepsilon_j\).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;em&gt;Proof.&lt;/em&gt;
Fix an arbitrary pair of neighbouring datasets \(x,x’ \in \mathcal{X}^n\) and output \(y \in \mathcal{Y}^k\).
To establish that \(M\) is \(\varepsilon\)-DP, we must show that \(e^{-\varepsilon} \le \frac{\mathbb{P}[M(x)=y]}{\mathbb{P}[M(x’)=y]} \le e^\varepsilon\). By independence, we have \[\frac{\mathbb{P}[M(x)=y]}{\mathbb{P}[M(x’)=y]} = \frac{\prod_{j=1}^k\mathbb{P}[M_j(x)=y_j]}{\prod_{j=1}^k\mathbb{P}[M_j(x’)=y_j]} =  \prod_{j=1}^k \frac{\mathbb{P}[M_j(x)=y_j]}{\mathbb{P}[M_j(x’)=y_j]} \le \prod_{j=1}^k e^{\varepsilon_j} = e^{\sum_{j=1}^k \varepsilon_j} = e^\varepsilon,\] where the inequality follows from the fact that each \(M_j\) is \(\varepsilon_j\)-DP and, hence, \(e^{-\varepsilon_j} \le \frac{\mathbb{P}[M_j(x)=y_j]}{\mathbb{P}[M_j(x’)=y_j]} \le e^{\varepsilon_j}\). Similarly, \(\prod_{j=1}^k \frac{\mathbb{P}[M_j(x)=y_j]}{\mathbb{P}[M_j(x’)=y_j]} \ge \prod_{j=1}^k e^{-\varepsilon_j}\), which completes the proof. ∎&lt;/p&gt;

&lt;p&gt;Basic composition is already a powerful result, despite its simple proof; it establishes the versatility of differential privacy and allows us to begin reasoning about complex systems in terms of their building blocks. For example, suppose we have \(k\) functions \(f_1, \cdots, f_k : \mathcal{X}^n \to \mathbb{R}\) each of sensitivity \(1\). For each \(j \in [k]\), we know that adding \(\mathsf{Laplace}(1/\varepsilon)\) noise to the value of \(f_j(x)\) satisfies \(\varepsilon\)-DP. Thus, if we add independent \(\mathsf{Laplace}(1/\varepsilon)\) noise to each value \(f_j(x)\) for all \(j \in [k]\), then basic composition tells us that releasing this vector of \(k\) noisy values satisfies \(k\varepsilon\)-DP. If we want the overall system to be \(\varepsilon\)-DP, then we should add independent \(\mathsf{Laplace}(k/\varepsilon)\) noise to each value \(f_j(x)\).&lt;/p&gt;

&lt;h2 id=&quot;is-basic-composition-optimal&quot;&gt;Is Basic Composition Optimal?&lt;/h2&gt;

&lt;p&gt;If we want to release \(k\) values each of sensitivity \(1\) (as above) and have the overall release be \(\varepsilon\)-DP, then, using basic composition, we can add \(\mathsf{Laplace}(k/\varepsilon)\) noise to each value. The variance of the noise for each value is \(2k^2/\varepsilon^2\), so the standard deviation is \(\sqrt{2} k /\varepsilon\). In other words, the scale of the noise must grow linearly with the number of values \(k\) if the overall privacy and each value’s sensitivity is fixed. It is natural to wonder whether the scale of the Laplace noise can be reduced by improving the basic composition result. We now show that this is not possible.&lt;/p&gt;

&lt;p&gt;For each \(j \in [k]\), let \(M_j : \mathcal{X}^n \to \mathbb{R}\) be the algorithm that releases \(f_j(x)\) with \(\mathsf{Laplace}(k/\varepsilon)\) noise added. Let \(M : \mathcal{X}^n \to \mathbb{R}^k\) be the composition of these \(k\) algorithms. Then \(M_j\) is \(\varepsilon/k\)-DP for each \(j \in [k]\) and basic composition tells us that \(M\) is \(\varepsilon\)-DP. The question is whether \(M\) satisfies a better DP guarantee than this – i.e., does \(M\) satisfy \(\varepsilon_*\)-DP for some \(\varepsilon_*&amp;lt;\varepsilon\)?
Suppose we have neighbouring datasets \(x,x’\in\mathcal{X}^n\) such that \(f_j(x) = f_j(x’)+1\) for each \(j \in [k]\). Let \(y=(a,a,\cdots,a) \in \mathbb{R}^k\) for some \(a \ge \max_{j=1}^k f_j(x)\).
Then 
\[
        \frac{\mathbb{P}[M(x)=y]}{\mathbb{P}[M(x’)=y]} = \frac{\prod_{j=1}^k \mathbb{P}[f_j(x)+\mathsf{Laplace}(k/\varepsilon)=y_j]}{\prod_{j=1}^k \mathbb{P}[f_j(x’)+\mathsf{Laplace}(k/\varepsilon)=y_j]} 
\]
\[
         = \prod_{j=1}^k \frac{\frac{\varepsilon}{2k}\exp\left(-\frac{\varepsilon}{k} |y_j-f_j(x)| \right)}{\frac{\varepsilon}{2k}\exp\left(-\frac{\varepsilon}{k} |y_j-f_j(x’)| \right)} 
         = \prod_{j=1}^k \frac{\exp\left(-\frac{\varepsilon}{k} (y_j-f_j(x)) \right)}{\exp\left(-\frac{\varepsilon}{k} (y_j-f_j(x’)) \right)} 
\]
\[
         = \prod_{j=1}^k \exp\left(\frac{\varepsilon}{k}\left(f_j(x)-f_j(x’)\right)\right)
         = \exp\left( \frac{\varepsilon}{k} \sum_{j=1}^k \left(f_j(x)-f_j(x’)\right)\right)= e^\varepsilon,
\]
where the third equality removes the absolute values because \(y_j \ge f_j(x)\) and \(y_j \ge f_j(x’)\).
This shows that basic composition is optimal. For this example, we cannot prove a better guarantee than what is given by basic composition.&lt;/p&gt;

&lt;p&gt;Is there some other way to improve upon basic composition that circumvents this example? Note that we assumed that there are neighbouring datasets \(x,x’\in\mathcal{X}^n\) such that \(f_j(x) = f_j(x’)+1\) for each \(j \in [k]\). In some settings, no such worst case datasets exist. In that case, instead of scaling the noise linearly with \(k\), we can scale the Laplace noise according to the \(\ell_1\) sensitivity \(\Delta_1 := \sup_{x,x’ \in \mathcal{X}^n \atop \text{neighbouring}} \sum_{j=1}^k |f_j(x)-f_j(x’)|\).&lt;/p&gt;

&lt;p&gt;Instead of adding assumptions to the problem, we will look more closely at the example above.
We showed that there exists some output \(y \in \mathbb{R}^d\) such that \(\frac{\mathbb{P}[M(x)=y]}{\mathbb{P}[M(x’)=y]} = e^\varepsilon\).
However, such outputs \(y\) are very rare, as we require \(y_j \ge \max\{f_j(x),f_j(x’)\}\) for each \(j \in [k]\) where \(y_j = f_j(x) + \mathsf{Laplace}(k/\varepsilon)\). Thus, in order to observe an output \(y\) such that the likelihood ratio is maximal, all of the \(k\) Laplace noise samples must be positive, which happens with probability \(2^{-k}\). 
The fact that outputs \(y\) with maximal likelihood ratio are exceedingly rare turns out to be a general phenomenon and not specific to the example above.&lt;/p&gt;

&lt;p&gt;Can we improve on basic composition if we only ask for a high probability bound? That is, instead of demanding \(\frac{\mathbb{P}[M(x)=y]}{\mathbb{P}[M(x’)=y]} \le e^{\varepsilon_*}\) for all \(y \in \mathcal{Y}\), we demand \(\mathbb{P}_{Y \gets M(x)}\left[\frac{\mathbb{P}[M(x)=Y]}{\mathbb{P}[M(x’)=Y]} \le e^{\varepsilon_*}\right] \ge 1-\delta\) for some \(0 &amp;lt; \delta \ll 1\). Can we prove a better bound \(\varepsilon_* &amp;lt; \varepsilon\) in this relaxed setting? The answer turns out to be yes.&lt;/p&gt;

&lt;p&gt;The limitation of pure \(\varepsilon\)-DP is that events with tiny probability – which are negligible in real-world applications – can dominate the privacy analysis. This motivates us to move to relaxed notions of differential privacy, such as approximate \((\varepsilon,\delta)\)-DP and concentrated DP, which are less sensitive to low probability events.&lt;/p&gt;

&lt;h2 id=&quot;preview-advanced-composition&quot;&gt;Preview: Advanced Composition&lt;/h2&gt;

&lt;p&gt;By moving to approximate \((\varepsilon,\delta)\)-DP with \(\delta&amp;gt;0\), we can prove an asymptotically better composition theorem, which is known as &lt;em&gt;the advanced composition theorem&lt;/em&gt; &lt;strong&gt;&lt;a href=&quot;https://ieeexplore.ieee.org/document/5670947&quot; title=&quot;Cynthia Dwork, Guy Rothblum, Salil Vadhan. Boosting and Differential Privacy. FOCS 2010.&quot;&gt;[DRV10]&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt; (Advanced Composition Starting from Pure DP&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;)
Let \(M_1, M_2, \cdots, M_k : \mathcal{X}^n \to \mathcal{Y}\) be randomized algorithms. Suppose \(M_j\) is \(\varepsilon_j\)-DP for each \(j \in [k]\).
Define \(M : \mathcal{X}^n \to \mathcal{Y}^k\) by \(M(x)=(M_1(x),M_2(x),\cdots,M_k(x))\), where each algorithm is run independently. Then \(M\) is \((\varepsilon,\delta)\)-DP for any \(\delta&amp;gt;0\) with \[\varepsilon = \frac12 \sum_{j=1}^k \varepsilon_j^2 + \sqrt{2\log(1/\delta) \sum_{j=1}^k \varepsilon_j^2}.\]&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Recall that basic composition gives \(\delta=0\) and \(\varepsilon = \sum_{j=1}^k \varepsilon_j\). That is, basic composition scales with the 1-norm of the vector \((\varepsilon_1, \varepsilon_2, \cdots, \varepsilon_k)\), whereas advanced composition scales with the 2-norm of this vector (and the squared 2-norm).
Neither bound strictly dominates the other. However, asymptotically (in a sense we will make precise in the next paragraph) advanced composition dominates basic composition.&lt;/p&gt;

&lt;p&gt;Suppose we have a fixed \((\varepsilon,\delta)\)-DP guarantee for the entire system and we must answer \(k\) queries of sensitivity \(1\).
Using basic composition, we can answer each query by adding \(\mathsf{Laplace}(k/\varepsilon)\) noise to each answer.
However, using advanced composition, we can answer each query by adding \(\mathsf{Laplace}(\sqrt{k/2\rho})\) noise to each answer, where&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;
\[\rho = \frac{\varepsilon^2}{4\log(1/\delta)+4\varepsilon}.\]
If the privacy parameters \(\varepsilon,\delta&amp;gt;0\) are fixed (which implies \(\rho\) is fixed) and \(k \to \infty\), we can see that asymptotically advanced composition gives noise per query scaling as \(\Theta(\sqrt{k})\), while basic composition results in noise scaling as \(\Theta(k)\).&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;In the next few posts we will explain how advanced composition works. We hope this conveys an intuitive understanding of composition and, in particular, how this \(\sqrt{k}\) asymptotic behaviour arises. If you want to read ahead, these posts are extracts from &lt;a href=&quot;https://arxiv.org/abs/2210.00597&quot;&gt;this book chapter&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;This result generalizes to approximate DP. If instead we assume \(M_j\) is \((\varepsilon_j,\delta_j)\)-DP for each \(j \in [k]\), then the final composition is \((\varepsilon,\delta+\sum_{j=1}^k \delta_j)\)-DP with \(\varepsilon\) as before. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Adding \(\mathsf{Laplace}(\sqrt{k/2\rho})\) noise to a sensitivity-1 query ensures \(\varepsilon_j\)-DP for \(\varepsilon_j = \sqrt{2\rho/k}\). Hence \(\sum_{j=1}^k \varepsilon_j^2 = 2\rho\). Setting \(\rho = \frac{\varepsilon^2}{4\log(1/\delta)+4\varepsilon}\) ensures that \(\frac12 \sum_{j=1}^k \varepsilon_j^2 + \sqrt{2\log(1/\delta) \sum_{j=1}^k \varepsilon_j^2} = \rho + \sqrt{4\rho\log(1/\delta)} \le \varepsilon\). &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <author>
        
            <name>Thomas Steinke</name>
        
        </author>
        <pubDate>Tue, 01 Nov 2022 11:45:00 -0400</pubDate>
        <link>http://localhost:4000/composition-basics/</link>
        <guid isPermaLink="true">http://localhost:4000/composition-basics/</guid>
      </item>
    
      <item>
        <title>Privacy Doona&amp;#58; Why We Should Hide Among The Clones</title>
        <description>&lt;p&gt;In this blog post, we will discuss a recent(ish) result of Feldman, McMillan, and Talwar &lt;a href=&quot;https://arxiv.org/abs/2012.12803&quot; title=&quot;Vitaly Feldman, Audra McMillan, Kunal Talwar. Hiding Among the Clones: A Simple and Nearly Optimal Analysis of Privacy Amplification by Shuffling. FOCS 2021&quot;&gt;&lt;strong&gt;[FMT21]&lt;/strong&gt;&lt;/a&gt;, which provides an improved and simple analysis of the so-called “amplification by shuffling” formally connecting local privacy (LDP) and shuffle privacy.&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; Now, I’ll assume the reader is familiar with both LDP and Shuffle DP: if not, a quick-and-dirty refresher (with less quick, and less dirty references) can be found &lt;a href=&quot;\trustmodels&quot;&gt;here&lt;/a&gt;, and of course there is also Albert Cheu’s excellent survey on Shuffle DP &lt;a href=&quot;https://arxiv.org/abs/2107.11839&quot; title=&quot;Albert Cheu. Differential Privacy in the Shuffle Model: A Survey of Separations. arXiv 2021&quot;&gt;&lt;strong&gt;[Cheu21]&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I will also ignore most of the historical details, but it is worth mentioning that &lt;a href=&quot;https://arxiv.org/abs/2012.12803&quot; title=&quot;Vitaly Feldman, Audra McMillan, Kunal Talwar. Hiding Among the Clones: A Simple and Nearly Optimal Analysis of Privacy Amplification by Shuffling. FOCS 2021&quot;&gt;&lt;strong&gt;[FMT21]&lt;/strong&gt;&lt;/a&gt; is not the first paper on this “amplification by shuffling,” (which, for local reasons, I’ll just call a &lt;em&gt;privacy &lt;a href=&quot;https://www.collinsdictionary.com/dictionary/english/doona&quot;&gt;doona&lt;/a&gt;&lt;/em&gt;) but rather is the culmination of a rather long line of work involving many cool ideas and papers, starting with &lt;a href=&quot;https://arxiv.org/abs/1808.01394&quot; title=&quot;Albert Cheu, Adam D. Smith, Jonathan Ullman, David Zeber, Maxim Zhilyaev. Distributed Differential Privacy via Shuffling. EUROCRYPT 2019&quot;&gt;&lt;strong&gt;[CSUZZ19&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1811.12469&quot; title=&quot;Úlfar Erlingsson, Vitaly Feldman, Ilya Mironov, Ananth Raghunathan, Kunal Talwar, Abhradeep Thakurta. Amplification by Shuffling: From Local to Central Differential Privacy via Anonymity. SODA 2019&quot;&gt;&lt;strong&gt;EFMRTT19]&lt;/strong&gt;&lt;/a&gt;: I’d refer the reader to &lt;strong&gt;Table 1&lt;/strong&gt; in &lt;a href=&quot;https://arxiv.org/abs/2012.12803&quot; title=&quot;Vitaly Feldman, Audra McMillan, Kunal Talwar. Hiding Among the Clones: A Simple and Nearly Optimal Analysis of Privacy Amplification by Shuffling. FOCS 2021&quot;&gt;&lt;strong&gt;[FMT21]&lt;/strong&gt;&lt;/a&gt; for an overview.&lt;/p&gt;

&lt;p&gt;Alright, now that the caveats are behind us, what &lt;em&gt;is&lt;/em&gt; “amplification by shuffling”? In a nutshell, it is capturing the (false!) intuition that “anonymization provides privacy” (which, again, is false! Don’t do this!) and making it… less false. The idea is that while &lt;em&gt;anonymization does not provide in itself any meaningful privacy guarantee&lt;/em&gt;, it can &lt;em&gt;amplify existing, rigorous privacy guarantee&lt;/em&gt;. So if I start with a somewhat lousy LDP guarantee, but then all the messages sent by all users are completely anonymized, then my lousy LDP guarantee suddenly gets &lt;em&gt;much&lt;/em&gt; stronger (roughly speaking, the \(\varepsilon\) parameter goes down with the square root of of the number of users involved). Which is wonderful! Let’s see what this means, quantitatively.&lt;/p&gt;

&lt;h3 id=&quot;the-result-of-feldman-mcmillan-and-talwar&quot;&gt;The result of Feldman, McMillan, and Talwar.&lt;/h3&gt;
&lt;p&gt;Here, we will focus on the simpler case of &lt;em&gt;noninteractive&lt;/em&gt; protocols (one-shot messages from the users to the central server, no funny business with messages going back and forth); which is conceptually simpler to state and parse, still very rich and interesting, and, well, very relevant in practice (being the easiest and cheapest to deploy). If you want the results in their full glorious generality, though, they are in the paper.&lt;/p&gt;

&lt;p&gt;What the main theorem of &lt;a href=&quot;https://arxiv.org/abs/2012.12803&quot; title=&quot;Vitaly Feldman, Audra McMillan, Kunal Talwar. Hiding Among the Clones: A Simple and Nearly Optimal Analysis of Privacy Amplification by Shuffling. FOCS 2021&quot;&gt;&lt;strong&gt;[FMT21]&lt;/strong&gt;&lt;/a&gt; is saying for this noninteractive setting can then be stated as follows: if I have an &lt;em&gt;\(\varepsilon_L\)-locally private&lt;/em&gt; (LDP) protocol for a task, where all \(n\) users pass their data through the same randomizer (algorithm) \(R\) and send the resulting message \(y_i \gets R(x_i)\), then just permuting the messages \(y_1\dots,y_n\) immediately gives an \((\varepsilon,\delta)\)-&lt;em&gt;shuffle&lt;/em&gt; private protocol for the same task, for any pair \((\varepsilon,\delta)\) which satisfies
&lt;a name=&quot;eq:eps:epsL&quot;&gt;&lt;/a&gt;
\begin{equation}
		\varepsilon \leq \log\left( 1+ 16\frac{e^{\varepsilon_{L}}-1}{e^{\varepsilon_{L}}+1}\sqrt{\frac{e^{\varepsilon_{L}}\log\frac{4}{\delta}}{n}}\right) \tag{1}
\end{equation}
as long as \(n \gg e^{\varepsilon_{L}}\log(1/\delta)\). That is quite a lot to parse, though: what does this actually &lt;em&gt;mean&lt;/em&gt;?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;First&lt;/strong&gt;, the assumption that all users have the same randomizer (or at least cannot be distinguished by their randomizer) is quite natural: if they didn’t, then we wouldn’t be able to say anything in general, since the randomizer they use could just give away their identity completely. For instance, as an extreme case, the randomizer of user \(i\) could just append \(i\) to the message (it’s OK, still LDP!), and then shuffling achieves exactly nothing: we know who sent what. So OK, asking for all randomizers to be the same is not really a restriction.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Second&lt;/strong&gt;, each user only sends one message, and this preserves its length (we just shuffled the messages, didn’t modify them!). So if you start with an LDP protocol with amazing features XYZ (e.g., the messages are \(1\)-bit long, or users don’t share a random seed, or the randomizers run in time \(O(1)\)), then the shuffle protocol enjoys exactly the same properties. (It only enjoys naturally some &lt;em&gt;robustness&lt;/em&gt;, in the sense that if \(10\%\) if the \(n\) users maliciously deviate from the protocol, they can’t really jeopardize the privacy of the remaining \(90\%\) of users.&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; Which is… good.)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Third&lt;/strong&gt;, this is inherently approximate DP. Here we started with pure LDP (you can also extend that to approximate LDP) and ended up with approximate Shuffle DP: this is not a mistake, that’s how it is. I am not a purist (erm) myself, and that looks more than good enough to me; but if you seek pure Shuffle DP, then this result is not the droid you’re looking for.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;../images/droids-looking.png&quot; width=&quot;50%&quot; alt=&quot;This is not the pure DP guarantee you are looking for.&quot; /&gt;
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Alright, &lt;em&gt;what&lt;/em&gt; is this guarantee stated in &lt;a href=&quot;#eq:eps:epsL&quot;&gt;(1)&lt;/a&gt; giving us? Let’s interpret the expression in &lt;a href=&quot;#eq:eps:epsL&quot;&gt;(1)&lt;/a&gt; in two parameter regimes, focusing on \(\varepsilon\) (fixing some small \(\delta&amp;gt;0\)). If we start with \(\varepsilon_{L} \ll 1\) for our LDP randomizers \(R\), then a first-order Taylor expansion shows that we get
\[
		\varepsilon \approx \varepsilon_{L}\cdot 8\sqrt{\frac{\log\frac{4}{\delta}}{n}}
\]
so that &lt;em&gt;shuffling improved our privacy parameter by a factor \(\sqrt{n}\)&lt;/em&gt;.&lt;sup id=&quot;fnref:3&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt; 😲 This is great! With more users, comes more privacy!&lt;/p&gt;

&lt;p&gt;But that was starting with small \(\varepsilon_{L}\), that is, already pretty good privacy guarantees for our LDP “building block” \(R\). What happens if we start with “somewhat lousy” privacy guarantees, that is, \(\varepsilon_{L} \gg 1\)? Do we get anything interesting then?
Another Taylor expansion (everything is a Taylor expansion) shows us that, then,
&lt;a name=&quot;eq:epsL:ll:one&quot;&gt;&lt;/a&gt;
\[
\begin{equation}
		\varepsilon \approx \log\left( 1+ 8\sqrt{\frac{e^{\varepsilon_{L}}\log\frac{4}{\delta}}{n}}\right) \tag{2}
\end{equation}
\]
or, put differently,
&lt;a name=&quot;eq:epsL:gg:one&quot;&gt;&lt;/a&gt;
\[
\begin{equation}
		\varepsilon \approx 8e^{\varepsilon_{L}/2}\sqrt{\frac{\log\frac{4}{\delta}}{n}} \tag{3}
\end{equation}
\]
That’s a bit harder to interpret, but that seems… useful? It is: let us see how much, with a couple examples.&lt;/p&gt;

&lt;h4 id=&quot;learning&quot;&gt;Learning.&lt;/h4&gt;
&lt;p&gt;The first one is distribution learning, a.k.a. density estimation: you have \(n\) i.i.d. samples (one per user) from an unknown probability distribution \(\mathbf{p}\) over a discrete domain of size \(k\), and your goal is to output an estimate \(\widehat{\mathbf{p}}\) such that, with high (say, constant) probability, \(\mathbf{p}\) and \(\widehat{\mathbf{p}}\) are close in &lt;em&gt;total variation distance&lt;/em&gt;:
\[
		\operatorname{TV}(\mathbf{p},\widehat{\mathbf{p}}) = \sup_{S\subseteq [k]} (\mathbf{p}(S) - \widehat{\mathbf{p}}(S) ) \leq \alpha
\]
(if total variation distance seems a bit mysterious, it’s exactly half the \(\ell_1\) distance between the probability mass functions). We know how to solve this problem in the non-private setting: \(n=\Theta\left( \frac{k}{\alpha^2} \right)\) samples are necessary and sufficient. We know how to solve this problem in the (central) DP setting: \(n=\Theta\left( \frac{k}{\alpha^2} + \frac{k}{\alpha\varepsilon} \right)\) samples are necessary and sufficient &lt;a href=&quot;https://proceedings.neurips.cc/paper/2015/hash/2b3bf3eee2475e03885a110e9acaab61-Abstract.html&quot; title=&quot;Ilias Diakonikolas, Moritz Hardt, Ludwig Schmidt. Differentially Private Learning of Structured Discrete Distributions. NeurIPS 2015&quot;&gt;&lt;strong&gt;[DHS15]&lt;/strong&gt;&lt;/a&gt;. We know how to solve this problem in the LDP setting: 
&lt;a name=&quot;eq:learning:ldp&quot;&gt;&lt;/a&gt;
\begin{equation}
n=\Theta\left(\frac{k^2}{\alpha^2(e^\varepsilon-1)^2}+\frac{k^2}{\alpha^2e^\varepsilon}+\frac{k}{\alpha^2}\right) \tag{4}
\end{equation}
samples are necessary and sufficient &lt;a href=&quot;http://proceedings.mlr.press/v89/acharya19a.html&quot; title=&quot;Jayadev Acharya, Ziteng Sun, Huanyu Zhang. Hadamard Response: Estimating Distributions Privately, Efficiently, and with Little Communication. AISTATS 2019&quot;&gt;&lt;strong&gt;[ASZ19]&lt;/strong&gt;&lt;/a&gt; (note that the first term is just \(k/(\alpha^2\varepsilon^2)\) for small \(\varepsilon\)). Now, as they say in Mulan: &lt;em&gt;let’s make a shuffle DP algo out of you.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;If we want to achieve \((\varepsilon,\delta)\)-shuffle DP, we need to select \(\varepsilon_L\). Based on &lt;a href=&quot;#eq:epsL:ll:one&quot;&gt;(2)&lt;/a&gt; and &lt;a href=&quot;#eq:epsL:gg:one&quot;&gt;(3)&lt;/a&gt;, and ignoring pesky constants we will choose it so that
&lt;a name=&quot;eq:choice:epsL&quot;&gt;&lt;/a&gt;
\begin{equation}
	 \varepsilon_{L} \approx \varepsilon \sqrt{\frac{n}{\log(1/\delta)}}  \quad\text{ or }\quad e^{\varepsilon_{L}} \approx \varepsilon^2 \cdot \frac{n}{\log(1/\delta)}\,. \tag{5}
\end{equation}
depending on whether \(\frac{\varepsilon^2 n}{\log(1/\delta)}\geq 1\). Plugging that back in &lt;a href=&quot;#eq:learning:ldp&quot;&gt;(4)&lt;/a&gt;, we see that the first case corresponds to the first term (small \(\varepsilon_{L}\)) and the second to the second term (\(\varepsilon_{L} \geq 1\)), and overall the condition on \(n\) for the original LDP algorithm to 
successful learn the distribution becomes
\[
	n \gtrsim 
	\frac{k^2}{\alpha^2(e^{\varepsilon_{L}}-1)^2}+\frac{k^2}{\alpha^2e^{\varepsilon_{L}}}+\frac{k}{\alpha^2}
	\approx \frac{k^2\log(1/\delta)}{\alpha^2\varepsilon^2 n}+\frac{k^2\log(1/\delta)}{\alpha^2\varepsilon^2 n}+\frac{k}{\alpha^2} 
	 \approx \frac{k^2\log(1/\delta)}{\alpha^2\varepsilon^2 n}+\frac{k}{\alpha^2}
\]
(where \(\gtrsim\) means “let’s ignore constants”). There is an \(n\) in the RHS as well, so reorganizing and handling the two terms separately the condition on \(n\) becomes
\[
	n \gtrsim \frac{k \sqrt{\log(1/\delta)}}{\alpha\varepsilon}+\frac{k}{\alpha^2}
\]
which… is great? We immediately get a sample complexity \(O\left(\frac{k}{\alpha^2}+\frac{k \sqrt{\log(1/\delta)}}{\alpha\varepsilon}\right)\) in the shuffle DP model, which (ignoring the \(\sqrt{\log(1/\delta)}\)) matches the one in the &lt;em&gt;central&lt;/em&gt; DP setting!&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;tl;dr:&lt;/strong&gt; Taking an optimal LDP algorithm and just shuffling the messages &lt;em&gt;immediately&lt;/em&gt; gives an optimal shuffle DP algorithm, no extra work needed.&lt;/p&gt;

&lt;h4 id=&quot;uniformity-testing&quot;&gt;(Uniformity) Testing.&lt;/h4&gt;
&lt;p&gt;Alright, maybe it was a fluke? Let’s look at another “basic” problem close to my heart: we don’t want to learn the probability distribution \(\mathbf{p}\), just test whether it is actually &lt;em&gt;the&lt;/em&gt; uniform distribution&lt;sup id=&quot;fnref:4&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt; \(\mathbf{u}\) on the domain \([k]={1,2,\dots,k}\). So if \(\mathbf{p} =\mathbf{u}\), you’ve got to say “yes” with probability at least \(2/3\), and if \(\operatorname{TV}(\mathbf{p},\mathbf{u})&amp;gt;\alpha\), then you need to say “no” with probability at least \(2/3\).&lt;/p&gt;

&lt;p&gt;This is also well understood in the non-private setting (\(n=\Theta(\sqrt{k}/\alpha^2)\)) &lt;a href=&quot;https://ieeexplore.ieee.org/document/4626074&quot; title=&quot;Liam Paninski. A Coincidence-Based Test for Uniformity Given Very Sparsely Sampled Discrete Data. IEEE Trans. Inf. Theory 2008&quot;&gt;&lt;strong&gt;[Paninski08]&lt;/strong&gt;&lt;/a&gt; [see also &lt;a href=&quot;https://ccanonne.github.io/survey-topics-dt.html}{my upcoming survey&quot;&gt;my upcoming survey&lt;/a&gt;], in the central DP setting (\(n=\Theta\left( \frac{\sqrt{k}}{\alpha^2} + \frac{\sqrt{k}}{\alpha\sqrt{\varepsilon}}+\frac{k^{1/3}}{\alpha^{4/3}\varepsilon^{2/3}} + \frac{1}{\alpha\varepsilon} \right)\)) &lt;a href=&quot;https://arxiv.org/abs/1707.05128&quot; title=&quot;Jayadev Acharya, Ziteng Sun, Huanyu Zhang. Differentially Private Testing of Identity and Closeness of Discrete Distributions. NeurIPS 2018&quot;&gt;&lt;strong&gt;[ASZ18&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1707.05497&quot; title=&quot;Maryam Aliakbarpour, Ilias Diakonikolas, Ronitt Rubinfeld. Differentially Private Identity and Equivalence Testing of Discrete Distributions. ICML 2018&quot;&gt;&lt;strong&gt;ADR18]&lt;/strong&gt;&lt;/a&gt;, and in the LDP setting, where the result differs on whether the users can communicate or share a common random seed 
&lt;a name=&quot;eq:testing:ldp:publiccoin&quot;&gt;&lt;/a&gt;
\begin{equation}
	n=\Theta\left( \frac{k}{\alpha^2(e^\varepsilon-1)^2} + \frac{k}{\alpha^2e^{\varepsilon/2}} + \frac{\sqrt{k}}{\alpha^2}\right) \tag{6}
\end{equation} or not
&lt;a name=&quot;eq:testing:ldp:privatecoin&quot;&gt;&lt;/a&gt; 
\begin{equation}
	n=\Theta\left( \frac{k^{3/2}}{\alpha^2(e^\varepsilon-1)^2} + \frac{k^{3/2}}{\alpha^2e^{\varepsilon}} + \frac{\sqrt{k}}{\alpha^2}\right) \tag{7}
\end{equation}
as established in a sequence of papers &lt;a href=&quot;https://arxiv.org/abs/1812.11476&quot; title=&quot;Inference under Information Constraints: Lower Bounds from Chi-Square Contraction. COLT 2019&quot;&gt;&lt;strong&gt;[ACT19&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1911.01452&quot; title=&quot;Kareem Amin, Matthew Joseph, Jieming Mao. Pan-Private Uniformity Testing. COLT 2020&quot;&gt;&lt;strong&gt;AJM20&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/2101.07981&quot; title=&quot;Jayadev Acharya, Clément L. Canonne, Cody Freitag, Ziteng Sun, Himanshu Tyagi. 
Inference Under Information Constraints III: Local Privacy Constraints. IEEE J. Sel. Areas Inf. Theory 2021&quot;&gt;&lt;strong&gt;ACFST21&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/2007.10976&quot; title=&quot;Jayadev Acharya, Clément L. Canonne, Yuhan Liu, Ziteng Sun, Himanshu Tyagi. Interactive Inference Under Information Constraints. IEEE Trans. Inf. Theory 2022&quot;&gt;&lt;strong&gt;ACLST22&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/2108.08987&quot; title=&quot;Clément L. Canonne, Hongyi Lyu. Uniformity Testing in the Shuffle Model: Simpler, Better, Faster. SOSA 2022&quot;&gt;&lt;strong&gt;CL22]&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Now, say you want an (\(\varepsilon,\delta)\)-shuffle DP algorithm for uniformity testing, but don’t want to design one from scratch (though it &lt;em&gt;is&lt;/em&gt; possible to do so, and some did &lt;a href=&quot;https://arxiv.org/abs/2004.09481&quot; title=&quot;Victor Balcer, Albert Cheu, Matthew Joseph, Jieming Mao. Connecting Robust Shuffle Privacy and Pan-Privacy. SODA 2021&quot;&gt;&lt;strong&gt;[BCJM21&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/2108.08987&quot; title=&quot;Clément L. Canonne, Hongyi Lyu. Uniformity Testing in the Shuffle Model: Simpler, Better, Faster. SOSA 2022&quot;&gt;&lt;strong&gt;CL22&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/2112.10032&quot; title=&quot;Albert Cheu, Chao Yan. Pure Differential Privacy from Secure Intermediaries. arXiv 2021&quot;&gt;&lt;strong&gt;CY21]&lt;/strong&gt;&lt;/a&gt;). Let’s say you want to look at the “no-common-random-seed-shared-by-users” model (a.k.a. &lt;em&gt;private-coin&lt;/em&gt; setting): so you stare at the corresponding LDP communication complexity, &lt;a href=&quot;#eq:testing:ldp:privatecoin&quot;&gt;(7)&lt;/a&gt;, and try to choose \(\varepsilon_L\) to start with before shuffling. This will be the same as in the learning example (i.e., &lt;a href=&quot;#eq:choice:epsL&quot;&gt;(5)&lt;/a&gt;): based on &lt;a href=&quot;#eq:epsL:ll:one&quot;&gt;(2)&lt;/a&gt; and &lt;a href=&quot;#eq:epsL:gg:one&quot;&gt;(3)&lt;/a&gt;, we will set
\begin{equation}
	 \varepsilon_{L} \approx \varepsilon \sqrt{\frac{n}{\log(1/\delta)}}  \quad\text{ or }\quad e^{\varepsilon_{L}} \approx \varepsilon^2 \cdot \frac{n}{\log(1/\delta)}\,.
\end{equation}
depending on whether \(\frac{\varepsilon^2 n}{\log(1/\delta)}\geq 1\). Plugging this back in &lt;a href=&quot;#eq:testing:ldp:privatecoin&quot;&gt;(7)&lt;/a&gt; and quickly checking which case corresponds to each term, we then easily get that for our algorithm to correctly solve the uniformity testing problem, it suffices that the sample complexity (number of users) \(n\) satisfies
\[
	n \gtrsim \frac{k^{3/2}}{\alpha^2(e^{\varepsilon_L}-1)^2} + \frac{k^{3/2}}{\alpha^2e^{\varepsilon_L}} + \frac{\sqrt{k}}{\alpha^2}
	 \approx \frac{k^{3/2}\log(1/\delta)}{\alpha^2\varepsilon^2 n } + \frac{\sqrt{k}}{\alpha^2}
\]
which, reorganizing and solving for \(n\), means that it suffices to have 
\[
	n \gtrsim \frac{k^{3/4}\sqrt{\log(1/\delta)}}{\alpha\varepsilon} + \frac{\sqrt{k}}{\alpha^2}\,.
\]
And, &lt;em&gt;voilà&lt;/em&gt;! Even better, we also have strong evidence to suspect that this sample complexity \(O\Big(\frac{k^{3/4}\sqrt{\log(1/\delta)}}{\alpha\varepsilon}+ \frac{\sqrt{k}}{\alpha^2}\Big)\) is tight among all private-coin algorithms.&lt;sup id=&quot;fnref:5&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:5&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;Now, if you wanted to look at &lt;em&gt;public-coin&lt;/em&gt; shuffle DP protocols (with a common random seed available), then you would start with an optimal public-coin LDP algorithm (and look at &lt;a href=&quot;#eq:testing:ldp:publiccoin&quot;&gt;(6)&lt;/a&gt;), and setting \(\varepsilon_L\) the same way you’d get a shuffle DP algorithm with sample complexity
\[
n=O\Big(\frac{k^{2/3}\log^{1/3}(1/\delta)}{\alpha^{4/3}\varepsilon^{2/3}} + \frac{\sqrt{k\log(1/\delta)}}{\alpha\varepsilon}+ \frac{\sqrt{k}}{\alpha^2}\Big)
\]
which, well, is &lt;em&gt;also&lt;/em&gt; strongly believed to be optimal!&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;tl;dr:&lt;/strong&gt; Here again, taking an optimal off-the-shelf LDP algorithm and just shuffling the messages &lt;em&gt;immediately&lt;/em&gt; gives an optimal shuffle DP algorithm, no extra work needed.&lt;/p&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion.&lt;/h3&gt;
&lt;p&gt;I hope the above convinced you of how useful this privacy amplification can be: from an optimal LDP algorithm, featuring any extra appealing characteristics you like, &lt;em&gt;just adding an extra shuffling step as postprocessing&lt;/em&gt; yields an (often optimal? At least good) shuffle DP algorithm, &lt;em&gt;with the same characteristics&lt;/em&gt; and built-in robustness against malicious users.&lt;/p&gt;

&lt;p&gt;All you need is to make sure that your starting point, the LDP algorithm satisfies a couple things: (1) all users have the same randomizer,&lt;sup id=&quot;fnref:6&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:6&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt; and (2) it works in all regimes of \(\varepsilon\) (both high-privacy, \(\varepsilon \leq 1\), &lt;em&gt;and&lt;/em&gt; low-privacy, \(\varepsilon \gg 1\)). Once you’ve got this, Bob’s your uncle! You get shuffle DP algorithms for free.&lt;/p&gt;

&lt;p&gt;It is not only appealing from a theoretical point of view, by the way! The authors of the paper worked hard to make their empirical analysis compelling as well, and their code is available &lt;a href=&quot;https://github.com/apple/ml-shuffling-amplification&quot;&gt;on GitHub&lt;/a&gt; 📝. But more importantly, from a practitioner’s point of view, this means it is enough to design, implement, and test &lt;em&gt;one&lt;/em&gt; algorithm (the LDP one we start with) to automatically get a trusted one in the shuffle DP model as well: this reduces the risks of bugs, security failures, the amount of work spending tuning, testing…&lt;/p&gt;

&lt;p&gt;So yes, whenever possible, we &lt;em&gt;should&lt;/em&gt; hide among the clones!&lt;/p&gt;

&lt;hr /&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;The title of this post is a reference to the title of &lt;a href=&quot;https://arxiv.org/abs/2012.12803&quot; title=&quot;Vitaly Feldman, Audra McMillan, Kunal Talwar. Hiding Among the Clones: A Simple and Nearly Optimal Analysis of Privacy Amplification by Shuffling. FOCS 2021&quot;&gt;&lt;strong&gt;[FMT21]&lt;/strong&gt;&lt;/a&gt;, “Hiding Among The Clones,” and to the notion of &lt;em&gt;privacy blanket&lt;/em&gt; introduced by Balle, Bell, Gascón, and Nissim &lt;a href=&quot;https://link.springer.com/chapter/10.1007/978-3-030-26951-7_22&quot; title=&quot;Borja Balle, James Bell, Adrià Gascón, Kobbi Nissim. The Privacy Blanket of the Shuffle Model. CRYPTO 2019&quot;&gt;&lt;strong&gt;[BBGN19]&lt;/strong&gt;&lt;/a&gt;. Intuitively, the “amplification by shuffling” paradigm can be seen as anonymizing the messages from local randomizers, whose message distribution can be mathematically decomposed as a mixture of “noise distribution not depending on the user’s input” and “distribution actually depending on their input.” As a result, each user randomly sends a message from the first or second distribution of the mixture.  But the shuffling then hides the informative messages (drawn from the second part of the mixture) among the non-informative (noise) ones: so the noise messages end up providing a “privacy blanket” in which sensitive information is safely and soundly wrapped. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;More specifically, they can completely jeopardize the &lt;em&gt;utility&lt;/em&gt; (accuracy) of the result, but in terms of privacy, all they can do is slightly reduce it: if \(10\%\) of users are malicious, the remaining \(90\%\) still get the privacy amplification of guarantee of &lt;a href=&quot;#eq:eps:epsL&quot;&gt;(1)&lt;/a&gt;, but with \(0.9n\) instead of \(n\). &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Of course, we started with a local privacy guarantee, and ended up with a shuffle privacy guarantee: so the two are incomparable, and one has to interpret this “amplification” in that context. &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;You can here replace uniform by any known distribution \(\mathbf{q}\) of your choosing, that doesn’t change the question (and result), but uniform is nice. &lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:5&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;As long as one is happy with approximate DP. One can achieve that in pure DP as well, but it’s a bit more complicated &lt;a href=&quot;https://arxiv.org/abs/2112.10032&quot; title=&quot;Albert Cheu, Chao Yan. Pure Differential Privacy from Secure Intermediaries. arXiv 2021&quot;&gt;&lt;strong&gt;[CY21]&lt;/strong&gt;&lt;/a&gt;. &lt;a href=&quot;#fnref:5&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:6&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;This is not such a big assumption usually, and there are somewhat-general ways to get to that using a logarithmic factor in the number of users. &lt;a href=&quot;#fnref:6&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <author>
        
            <name>Clément Canonne</name>
        
        </author>
        <pubDate>Tue, 24 May 2022 11:45:00 -0400</pubDate>
        <link>http://localhost:4000/privacy-doona/</link>
        <guid isPermaLink="true">http://localhost:4000/privacy-doona/</guid>
      </item>
    
      <item>
        <title>Differentially private deep learning can be effective with self-supervised models</title>
        <description>&lt;p&gt;Differential Privacy (DP) is a formal definition of privacy which guarantees that the outcome of a statistical procedure does not vary much regardless of whether an individual input is included or removed from the training dataset. 
This guarantee is desirable when we are tasked to train machine learning models on private datasets that should not memorize individual inputs. 
Past works have shown that differentially private models can be resilient to strong membership inference [&lt;a href=&quot;https://proceedings.mlr.press/v37/kairouz15.html&quot;&gt;1&lt;/a&gt;, &lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/9519424&quot;&gt;34&lt;/a&gt;, &lt;a href=&quot;https://proceedings.neurips.cc/paper/2020/hash/fc4ddc15f9f4b4b06ef7844d6bb53abf-Abstract.html&quot;&gt;35&lt;/a&gt;] and data reconstruction attacks [&lt;a href=&quot;https://www.usenix.org/conference/usenixsecurity19/presentation/carlini&quot;&gt;2&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/2201.12383&quot;&gt;3&lt;/a&gt;] when the privacy parameter is set to be sufficiently small. 
See a &lt;a href=&quot;https://differentialprivacy.org/how-to-deploy-ml-with-dp/&quot;&gt;prior post&lt;/a&gt; for more background on differentially private machine learning.&lt;/p&gt;

&lt;p&gt;Yet, in practice, most attempts at training differentially private deep learning models on moderately-sized datasets have resulted in large performance drops compared to when training without privacy-protection baked in. 
These performance drops are oftentimes large enough to discourage the adoption of differential privacy protection into machine learning pipelines altogether.&lt;/p&gt;

&lt;p&gt;To provide a reference of the potential performance hit, the authors of [&lt;a href=&quot;https://arxiv.org/abs/2102.12677&quot;&gt;5&lt;/a&gt;] trained a ResNet-20 from scratch on CIFAR-10 with a privacy budget of \(\epsilon=8\) that has test accuracy barely over 62% (see their Table 1). 
Contrast this with the 8.75% error rate (91.25% accuracy) reported for training the same architecture without enforcing differential privacy [&lt;a href=&quot;https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html&quot;&gt;6&lt;/a&gt;]. 
While some works report private learning results better than the above, absent additional data, pre-training, or external knowledge, most improvements have been incremental, and the test accuracy for CIFAR-10 models trained under modest privacy leakage (\(\epsilon=3\)) has roughly settled to ~70% in the literature [&lt;a href=&quot;https://arxiv.org/abs/2011.11660&quot;&gt;4&lt;/a&gt;].&lt;/p&gt;

&lt;p&gt;One reason behind the performance drop lies in sample efficiency — differentially private learning generally requires much more data than non-private learning to reach an acceptable level of performance. 
This also means that learning the high-level features (e.g., syntactic structure in text, edge detectors for images) necessary to perform specific tasks with private data can be much more sample-costly.&lt;/p&gt;

&lt;p&gt;This blog post surveys results that leverage public self-supervised pre-training to obtain high-performing models through differentially private fine-tuning.
The pre-train-fine-tune paradigm is straightforward to execute and results in high-performing models under modest privacy budgets for many standard computer vision and natural language processing tasks. 
Moreover, existing results have shown that private fine-tuning consistently benefits from improvements in public pre-training.&lt;/p&gt;

&lt;h2 id=&quot;self-supervised-pre-training&quot;&gt;Self-Supervised Pre-Training&lt;/h2&gt;

&lt;p&gt;Self-supervised learning is a paradigm which leverages unlabeled data to learn representations that can be useful for a range of downstream tasks.
Since self-supervised learning doesn’t target specific tasks itself, 
the (pre-)training procedure doesn’t require labeled data — in many cases, mildly curated unlabeled data is sufficient for self-supervised pre-training to produce models for subsequent fine-tuning. 
So far, there have been two broadly successful instantiations of this learning paradigm in computer vision [&lt;a href=&quot;http://proceedings.mlr.press/v119/chen20j.html&quot;&gt;9&lt;/a&gt;] and natural language processing [&lt;a href=&quot;https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf&quot;&gt;7&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1810.04805&quot;&gt;8&lt;/a&gt;]. 
We recap the two approaches below.&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Contrastive pre-training for vision:&lt;/strong&gt; 
One class of self-supervised methods in computer vision (SimCLR, [&lt;a href=&quot;http://proceedings.mlr.press/v119/chen20j.html&quot;&gt;9&lt;/a&gt;]) performs pre-training through contrastive learning. 
Algorithms of this type produce embeddings for images with the goal of creating different embeddings for semantically different images and similar embeddings for similar ones. 
Concretely, the algorithm used in SimCLR forces models to produce similar embeddings for an image and its augmented siblings (e.g., image rotated by some degrees), 
and different embeddings for separate images (and their augmentations). 
The SimCLR framework with large scale models and compute led to state-of-the-art (non-private) ImageNet fine-tuning results at the time of its writing.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Masked language modeling and autoregressive language modeling for text:&lt;/strong&gt; 
Masked Language Modeling (MLM) and Auto-regressive Language Modeling (ALM) are two self-supervised pre-training approaches. 
While the former asks models to predict deliberately masked out tokens from a piece of text, the latter asks models to simply predict the next token in a sequence. 
With large amounts of unlabeled text data, large and expressive Transformer models [&lt;a href=&quot;https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html&quot;&gt;24&lt;/a&gt;], and lots of compute, both approaches produce powerful models that are good starting points for downstream fine-tuning. 
For instance, Bidirectional Encoder Representations from Transformers (BERT, [&lt;a href=&quot;https://arxiv.org/abs/1810.04805&quot;&gt;8&lt;/a&gt;]), produced state-of-the-art (non-private) results (at the time) for a large collection of language understanding tasks when fine-tuned on each.&lt;/p&gt;

&lt;h2 id=&quot;fine-tuning-self-supervised-models-with-dp-optimization&quot;&gt;Fine-Tuning Self-Supervised Models With DP-Optimization&lt;/h2&gt;
&lt;p&gt;Self-supervised pre-training is appealing in the context of differentially private machine learning. 
This is because (i) the mildly curated data needed for pre-training can usually be obtained cheaply from the public domain, and (ii) pre-trained models may contain useful domain knowledge that can reduce the sample complexity of subsequent private learning. 
A paradigm for private learning that leverages self-supervised pre-training could follow two steps:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;collect cheap and public (unlabeled) data from the task domain (e.g., vision, language, etc.) to pre-train a model with self-supervised learning, and&lt;/li&gt;
  &lt;li&gt;collect moderate amounts of task-specific private (labeled) data and fine-tune the pre-trained model under differential privacy to perform the task.&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To date, some of the best differentially private deep learning results in the literature have resulted from instantiating this paradigm [&lt;a href=&quot;https://arxiv.org/abs/2011.11660&quot;&gt;4&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/2110.05679&quot;&gt;11&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/2110.06500&quot;&gt;12&lt;/a&gt;].
Below, we review works which capitalize on self-supervised pre-training by differentially privately fine-tuning pre-trained models with an iterative gradient method like DP-SGD [&lt;a href=&quot;https://dl.acm.org/doi/abs/10.1145/2976749.2978318&quot;&gt;19&lt;/a&gt;, &lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/6736861&quot;&gt;20&lt;/a&gt;].&lt;sup id=&quot;fnref:3&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;
&lt;img src=&quot;/images/fine-tuning-paradigm.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Private fine-tuning with SimCLR features:&lt;/strong&gt; 
The authors of [&lt;a href=&quot;https://arxiv.org/abs/2011.11660&quot;&gt;4&lt;/a&gt;] fine-tuned a linear model on top of the embedding vectors produced by SimCLRv2 from the CIFAR-10 dataset. Under a privacy budget of \(\epsilon=2\), 
these models reached an average test accuracy of 92.7%. This number can be further improved to ~94% with the use of larger and wider pre-trained models in the SimCLRv2 family.&lt;sup id=&quot;fnref:4&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt; 
These test accuracies are very close to some standard non-private results attained by an off-the-shelf ResNet architecture [&lt;a href=&quot;https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html&quot;&gt;6&lt;/a&gt;].&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Privately fine-tuning BERT variants and GPT-2:&lt;/strong&gt; 
The authors of [&lt;a href=&quot;https://arxiv.org/abs/2110.05679&quot;&gt;11&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/2110.06500&quot;&gt;12&lt;/a&gt;, &lt;a href=&quot;http://proceedings.mlr.press/v139/yu21f.html&quot;&gt;16&lt;/a&gt;] showed that with appropriate hyper-parameters, fine-tuning BERT variants and GPT-2 with DP-optimization results in high-performing private models for text classification and language generation — even on datasets of modest sizes and under modest privacy budgets. 
Notably, some of these models attain a task performance close to non-private models from previous years in the literature. 
These results also exceed many non-private learning results from the pre-BERT and pre-GPT years.&lt;sup id=&quot;fnref:5&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:5&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;More interestingly, the authors showed that the larger (and thus better) the pre-trained model, the better the private fine-tuning performance gets. 
This empirical observation in private fine-tuning of large Transformers is qualitatively different from what’s implied by the usual minimax optimal rates derived for vanilla private learning with convex loss functions under approximate differential privacy [&lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/6979031&quot;&gt;14&lt;/a&gt;, &lt;a href=&quot;https://proceedings.neurips.cc/paper/2019/hash/3bd8fdb090f1f5eb66a00c84dbc5ad51-Abstract.html&quot;&gt;15&lt;/a&gt;]. 
This discrepancy between experimental results for training large models and the theory for learning with convex losses suggests there is more to be understood.&lt;sup id=&quot;fnref:6&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:6&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;Overall, for both vision and language tasks, private learning performance has consistently improved with the improvement in the quality of pre-training, 
where the latter is measured by the non-private fine-tuning performance.&lt;sup id=&quot;fnref:7&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:7&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;7&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p float=&quot;left&quot;&gt;
  &lt;img src=&quot;../images/figure1_classification.png&quot; width=&quot;48%&quot; /&gt;
  &lt;img src=&quot;../images/figure1_generation.png&quot; width=&quot;48%&quot; /&gt; 
  &lt;caption&gt;Figure 1: Privately fine-tuning better (and larger) pre-trained models lead to consistently improving performance for text classification and language generation. 
Left: text classification on MNLI [&lt;a href=&quot;https://arxiv.org/abs/1704.05426&quot;&gt;25&lt;/a&gt;]. Right: language generation on E2E [&lt;a href=&quot;https://arxiv.org/abs/1706.09254&quot;&gt;26&lt;/a&gt;].&lt;/caption&gt;
&lt;/p&gt;

&lt;h2 id=&quot;conclusion-and-outlook&quot;&gt;Conclusion and Outlook&lt;/h2&gt;

&lt;p&gt;We surveyed recent works in the literature that obtained highly performant private machine learning models leveraging self-supervised pre-training. 
Common to these results is the trend that the performance of private learning consistently improved with the quality of public pre-training. 
We therefore anticipate that the general paradigm may be useful in additional settings (e.g., federated learning) and tasks (e.g., private synthetic image generation), and lead to better private learning results.&lt;/p&gt;

&lt;p&gt;We have thus far assumed that the data for public pre-training can be cheaply obtained.
This, however, does not imply that determining whether a particular source of data is appropriate for public pre-training is an easy problem.
Using publicly available data is not necessarily risk-free in terms of privacy.
For instance, the authors of [&lt;a href=&quot;https://www.usenix.org/conference/usenixsecurity21/presentation/carlini-extracting&quot;&gt;33&lt;/a&gt;] were able to extract personally identifiable information from a GPT-2 model pre-trained on data scraped from the public internet.&lt;/p&gt;

&lt;p&gt;Self-supervised pre-training has led to progress in private deep learning, but leveraging pre-trained models alone will not address several fundamental challenges to differentially private learning.
First and foremost, the datasets of machine learning tasks may be sampled from long-tailed distributions [&lt;a href=&quot;https://proceedings.neurips.cc/paper/2020/hash/1e14bfe2714193e7af5abc64ecbd6b46-Abstract.html&quot;&gt;21&lt;/a&gt;]. 
When privately trained on such datasets, a machine learning model may fail to acquire the learning signal necessary to perform accurate predictions for examples on the tail [&lt;a href=&quot;https://dl.acm.org/doi/abs/10.1145/3442188.3445934&quot;&gt;28&lt;/a&gt;] or from underrepresented (sub)populations [&lt;a href=&quot;https://proceedings.neurips.cc/paper/2019/hash/fc0de4e0396fff257ea362983c2dda5a-Abstract.html&quot;&gt;29&lt;/a&gt;]. 
Second, many machine learning problems are in a domain where public data (even unlabeled data) may be sparse, e.g., medical imaging. 
Developing refined versions of the pre-train-fine-tune approach for problems from these domains is an interesting avenue for future work.&lt;/p&gt;

&lt;p&gt;Lastly, differential privacy as one specific definition of privacy may not capture all that’s desired for privacy in reality. 
For instance, while differentially private algorithms naturally give machine unlearning guarantees [&lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/9519428&quot;&gt;30&lt;/a&gt;, &lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/7163042&quot;&gt;32&lt;/a&gt;], tailored unlearning algorithms tend to have higher capacities of unlearning [&lt;a href=&quot;https://proceedings.neurips.cc/paper/2021/hash/9627c45df543c816a3ddf2d8ea686a99-Abstract.html&quot;&gt;31&lt;/a&gt;].
In addition, what constitutes a record in the differential privacy framework can oftentimes be unclear. 
Inappropriately defined example boundaries can create correlated records which cause differential privacy guarantees to degrade [&lt;a href=&quot;https://arxiv.org/abs/1603.01508&quot;&gt;22&lt;/a&gt;].
Moreover, differential privacy guarantees won’t directly prevent the inference of private data outside the original context [&lt;a href=&quot;https://heinonline.org/hol-cgi-bin/get_pdf.cgi?handle=hein.journals/washlr79&amp;amp;section=16&quot;&gt;23&lt;/a&gt;]. 
These are fundamental limitations of differential privacy which improvements to differentially private learning won’t touch on.&lt;/p&gt;

&lt;h2 id=&quot;acknowledgements&quot;&gt;Acknowledgements&lt;/h2&gt;

&lt;p&gt;The authors thank Nicolas Papernot and Gautam Kamath for detailed feedback and edit suggestions.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;p&gt;[1] Rahman MA, Rahman T, Laganière R, Mohammed N, Wang Y. Membership Inference Attack against Differentially Private Deep Learning Model. Trans. Data Priv.. 2018 Apr 1;11(1):61-79.&lt;/p&gt;

&lt;p&gt;[2] Carlini N, Liu C, Erlingsson Ú, Kos J, Song D. The secret sharer: Evaluating and testing unintended memorization in neural networks. In 28th USENIX Security Symposium (USENIX Security 19) 2019 (pp. 267-284).&lt;/p&gt;

&lt;p&gt;[3] Guo C, Karrer B, Chaudhuri K, van der Maaten L. Bounding Training Data Reconstruction in Private (Deep) Learning. arXiv preprint arXiv:2201.12383. 2022 Jan 28.&lt;/p&gt;

&lt;p&gt;[4] Tramer F, Boneh D. Differentially private learning needs better features (or much more data). arXiv preprint arXiv:2011.11660. 2020 Nov 23.&lt;/p&gt;

&lt;p&gt;[5] Yu D, Zhang H, Chen W, Liu TY. Do not let privacy overbill utility: Gradient embedding perturbation for private learning. arXiv preprint arXiv:2102.12677. 2021 Feb 25.&lt;/p&gt;

&lt;p&gt;[6] He K, Zhang X, Ren S, Sun J. Deep residual learning for image recognition. InProceedings of the IEEE conference on computer vision and pattern recognition 2016 (pp. 770-778).&lt;/p&gt;

&lt;p&gt;[7] Radford A, Narasimhan K, Salimans T, Sutskever I. Improving language understanding by generative pre-training.&lt;/p&gt;

&lt;p&gt;[8] Devlin J, Chang MW, Lee K, Toutanova K. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805. 2018 Oct 11.&lt;/p&gt;

&lt;p&gt;[9] Chen T, Kornblith S, Norouzi M, Hinton G. A simple framework for contrastive learning of visual representations. InInternational conference on machine learning 2020 Nov 21 (pp. 1597-1607). PMLR.&lt;/p&gt;

&lt;p&gt;[10] Li XL, Liang P. Prefix-tuning: Optimizing continuous prompts for generation. arXiv preprint arXiv:2101.00190. 2021 Jan 1.&lt;/p&gt;

&lt;p&gt;[11] Li X, Tramer F, Liang P, Hashimoto T. Large language models can be strong differentially private learners. arXiv preprint arXiv:2110.05679. 2021 Oct 12.&lt;/p&gt;

&lt;p&gt;[12] Yu D, Naik S, Backurs A, Gopi S, Inan HA, Kamath G, Kulkarni J, Lee YT, Manoel A, Wutschitz L, Yekhanin S. Differentially private fine-tuning of language models. arXiv preprint arXiv:2110.06500. 2021 Oct 13.&lt;/p&gt;

&lt;p&gt;[13] Liu Y, Ott M, Goyal N, Du J, Joshi M, Chen D, Levy O, Lewis M, Zettlemoyer L, Stoyanov V. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692. 2019 Jul 26.&lt;/p&gt;

&lt;p&gt;[14] Bassily R, Smith A, Thakurta A. Private empirical risk minimization: Efficient algorithms and tight error bounds. In2014 IEEE 55th Annual Symposium on Foundations of Computer Science 2014 Oct 18 (pp. 464-473). IEEE.&lt;/p&gt;

&lt;p&gt;[15] Bassily R, Feldman V, Talwar K, Guha Thakurta A. Private stochastic convex optimization with optimal rates. Advances in Neural Information Processing Systems. 2019;32.&lt;/p&gt;

&lt;p&gt;[16] Yu D, Zhang H, Chen W, Yin J, Liu TY. Large scale private learning via low-rank reparametrization. InInternational Conference on Machine Learning 2021 Jul 1 (pp. 12208-12218). PMLR.&lt;/p&gt;

&lt;p&gt;[17] Radford A, Wu J, Child R, Luan D, Amodei D, Sutskever I. Language models are unsupervised multitask learners. OpenAI blog. 2019 Feb 24;1(8):9.&lt;/p&gt;

&lt;p&gt;[18] Bommasani R, Hudson DA, Adeli E, Altman R, Arora S, von Arx S, Bernstein MS, Bohg J, Bosselut A, Brunskill E, Brynjolfsson E, et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258. 2021 Aug 16.&lt;/p&gt;

&lt;p&gt;[19] Abadi M, Chu A, Goodfellow I, McMahan HB, Mironov I, Talwar K, Zhang L. Deep learning with differential privacy. InProceedings of the 2016 ACM SIGSAC conference on computer and communications security 2016 Oct 24 (pp. 308-318).&lt;/p&gt;

&lt;p&gt;[20] Song S, Chaudhuri K, Sarwate AD. Stochastic gradient descent with differentially private updates. In2013 IEEE Global Conference on Signal and Information Processing 2013 Dec 3 (pp. 245-248). IEEE.&lt;/p&gt;

&lt;p&gt;[21] Feldman V, Zhang C. What neural networks memorize and why: Discovering the long tail via influence estimation. Advances in Neural Information Processing Systems. 2020;33:2881-91.&lt;/p&gt;

&lt;p&gt;[22] Ghosh A, Kleinberg R. Inferential privacy guarantees for differentially private mechanisms. arXiv preprint arXiv:1603.01508. 2016 Mar 4.&lt;/p&gt;

&lt;p&gt;[23] Nissenbaum H. Privacy as contextual integrity. Wash. L. Rev.. 2004;79:119.&lt;/p&gt;

&lt;p&gt;[24] Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser Ł, Polosukhin I. Attention is all you need. Advances in neural information processing systems. 2017;30.&lt;/p&gt;

&lt;p&gt;[25] Williams A, Nangia N, Bowman SR. A broad-coverage challenge corpus for sentence understanding through inference. arXiv preprint arXiv:1704.05426. 2017 Apr 18.&lt;/p&gt;

&lt;p&gt;[26] Novikova J, Dušek O, Rieser V. The E2E dataset: New challenges for end-to-end generation. arXiv preprint arXiv:1706.09254. 2017 Jun 28.&lt;/p&gt;

&lt;p&gt;[27] Papernot N, Chien S, Song S, Thakurta A, Erlingsson U. Making the shoe fit: Architectures, initializations, and tuning for learning with privacy.&lt;/p&gt;

&lt;p&gt;[28] Suriyakumar VM, Papernot N, Goldenberg A, Ghassemi M. Chasing your long tails: Differentially private prediction in health care settings. InProceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency 2021 Mar 3 (pp. 723-734).&lt;/p&gt;

&lt;p&gt;[29] Bagdasaryan E, Poursaeed O, Shmatikov V. Differential privacy has disparate impact on model accuracy. Advances in Neural Information Processing Systems. 2019;32.&lt;/p&gt;

&lt;p&gt;[30] Bourtoule L, Chandrasekaran V, Choquette-Choo CA, Jia H, Travers A, Zhang B, Lie D, Papernot N. Machine unlearning. In2021 IEEE Symposium on Security and Privacy (SP) 2021 May 24 (pp. 141-159). IEEE.&lt;/p&gt;

&lt;p&gt;[31] Sekhari A, Acharya J, Kamath G, Suresh AT. Remember what you want to forget: Algorithms for machine unlearning. Advances in Neural Information Processing Systems. 2021 Dec 6;34.&lt;/p&gt;

&lt;p&gt;[32] Cao Y, Yang J. Towards making systems forget with machine unlearning. In2015 IEEE Symposium on Security and Privacy 2015 May 17 (pp. 463-480). IEEE.&lt;/p&gt;

&lt;p&gt;[33] Carlini N, Tramer F, Wallace E, Jagielski M, Herbert-Voss A, Lee K, Roberts A, Brown T, Song D, Erlingsson U, Oprea A. Extracting training data from large language models. In30th USENIX Security Symposium (USENIX Security 21) 2021 (pp. 2633-2650).&lt;/p&gt;

&lt;p&gt;[34] Nasr M, Songi S, Thakurta A, Papemoti N, Carlin N. Adversary instantiation: Lower bounds for differentially private machine learning. In2021 IEEE Symposium on Security and Privacy (SP) 2021 May 24 (pp. 866-882). IEEE.&lt;/p&gt;

&lt;p&gt;[35] Jagielski M, Ullman J, Oprea A. Auditing differentially private machine learning: How private is private sgd?. Advances in Neural Information Processing Systems. 2020;33:22205-16.&lt;/p&gt;
&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Authors of [&lt;a href=&quot;https://arxiv.org/abs/2108.07258&quot;&gt;18&lt;/a&gt;] framed these self-supervised models which are trained on broad data at scale that are adaptable to a wide range of downstream tasks as “foundation models.” &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;The idea of privately fine-tuning a publicly pre-trained model certainly isn’t new. One of the first differentially private deep learning papers [&lt;a href=&quot;https://arxiv.org/abs/1607.00133&quot;&gt;19&lt;/a&gt;] considered an experiment which fine-tuned convolutional nets on CIFAR-10 which were pre-trained on CIFAR-100. Results on privately fine-tuning &lt;em&gt;self-supervised&lt;/em&gt; models are, on the other hand, more recent. Covering these results is our main focus here. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Blue and pink sphere avatars taken from [&lt;a href=&quot;https://arxiv.org/abs/2108.07258&quot;&gt;18&lt;/a&gt;]. Credit to &lt;a href=&quot;https://cs.stanford.edu/~dorarad/&quot;&gt;Drew A. Hudson&lt;/a&gt; for making these. &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Unpublished result. &lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:5&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Hyper-parameters that work well for non-private learning typically aren’t those that work best for differentially private learning [&lt;a href=&quot;https://openreview.net/pdf?id=rJg851rYwH&quot;&gt;27&lt;/a&gt;]. It’s crucial to use a large batch size, a small clipping norm, an appropriate learning rate, and a reasonably large number of training epochs to obtain the mentioned private learning results [&lt;a href=&quot;https://arxiv.org/abs/2110.05679&quot;&gt;11&lt;/a&gt;]. &lt;a href=&quot;#fnref:5&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:6&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;In practice, past works have presented mixed results on whether larger models would yield better performance. While some showed that using more filters in a convolutional network can degrade the performance of private learning after some threshold [&lt;a href=&quot;https://openreview.net/pdf?id=rJg851rYwH&quot;&gt;27&lt;/a&gt;], others showed that a larger model can outperform a smaller model from a different model family [&lt;a href=&quot;https://arxiv.org/abs/2011.11660&quot;&gt;4&lt;/a&gt;]. Note these results are conditioned on their particular hyperparameter choices. &lt;a href=&quot;#fnref:6&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:7&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Since the pre-training data for large language models are oftentimes collected through large scale web scraping (e.g., WebText), a common concern is that some training and test instances for downstream tasks may already appear in the pre-training data. Self-supervised pre-training therefore can give models an opportunity to “see” this data even before they are privately fine-tuned. Authors of [&lt;a href=&quot;https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf&quot;&gt;17&lt;/a&gt;] confirmed that there is a 1-6% overlap between the test set of many natural language processing tasks and the pre-training data they collected (WebText); these common tasks, however, don’t include those studied by authors of [&lt;a href=&quot;https://arxiv.org/abs/2110.05679&quot;&gt;11&lt;/a&gt;]. The numbers suggest a possibility that existing private fine-tuning results in the literature could be slightly inflated compared to when the pre-training data didn’t contain any instance for any downstream task for which evaluation was performed. &lt;a href=&quot;#fnref:7&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <author>
        
            <name>Xuechen Li</name>
        
            <name>Florian Tramèr</name>
        
            <name>Janardhan Kulkarni</name>
        
            <name>Tatsunori Hashimoto</name>
        
        </author>
        <pubDate>Tue, 15 Mar 2022 11:00:00 -0800</pubDate>
        <link>http://localhost:4000/dp-fine-tuning/</link>
        <guid isPermaLink="true">http://localhost:4000/dp-fine-tuning/</guid>
      </item>
    
      <item>
        <title>A simple recipe for private synthetic data generation</title>
        <description>&lt;p&gt;In the &lt;a href=&quot;https://differentialprivacy.org/synth-data-0/&quot;&gt;last blog post&lt;/a&gt;, we covered the potential pitfalls of synthetic data without formal privacy guarantees, and motivated the need for differentially private synthetic data mechanisms.  In this blog post, we will describe the &lt;strong&gt;select-measure-generate&lt;/strong&gt; paradigm, which is a simple and effective template for designing synthetic data mechanisms.  The three steps underlying the select-measure-generate paradigm are illustrated and explained below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/select-measure-reconstruct.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Select&lt;/strong&gt; a collection of queries to measure — typically low-dimensional marginals.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Measure&lt;/strong&gt; the selected queries privately using a noise-addition mechanism.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Generate&lt;/strong&gt; synthetic data that best explains the noisy measurements.&lt;sup id=&quot;fnref:0&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:0&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Mechanisms in this class differ primarily in their methodology for selecting queries and their algorithm for generating synthetic data from noisy measurements.  The focus of this blog post is the final &lt;strong&gt;Generate&lt;/strong&gt; step.  Specifically, we will explore different ways in which one can model data distributions for the purpose of generating synthetic data, outlining the qualitative pros and cons of each method. We will then introduce the &lt;strong&gt;&lt;a href=&quot;https://github.com/ryan112358/private-pgm&quot; target=&quot;_blank&quot;&gt;Marginal-Based Inference (MBI)&lt;/a&gt;&lt;/strong&gt; repository that provides methods that, given some set of noisy measurements, enables users to generate synthetic data in a generic and scalable way.&lt;/p&gt;

&lt;p&gt;Separating the Generate subroutine from the existing synthetic data generation mechanisms greatly simplifies the design space of new differentially private mechanisms.  It allows the mechanism designer to focus on &lt;em&gt;selecting the queries&lt;/em&gt; to maximize utility of the synthetic data, rather than &lt;em&gt;how to generate synthetic data&lt;/em&gt; that explain the noisy measurements well.  Both are challenging technical problems that require different techniques to solve, and MBI provides principled solutions to the latter problem, while exposing an interface that can be readily adopted by mechanism designers.&lt;/p&gt;

&lt;h1 id=&quot;the-generate-subproblem-a-unifying-view&quot;&gt;The Generate Subproblem: A Unifying View&lt;/h1&gt;

&lt;p&gt;In this section we will introduce the main optimization problem that underlies several methods for the Generate subproblem, and provide a high-level overview of how each method attempts to solve this optimization problem. Let \( y = \mathcal{M}(D) \) be the noisy measurements obtained from running a privacy mechanism on a discrete dataset \( D \).  Our goal is to post-process these noise measurements to obtain synthetic data that explains them well.  In particular, we wish to minimize over the space of all &lt;em&gt;datasets&lt;/em&gt; for one that maximizes the likelihood of the observations \( y \).&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;\[ \hat{D} \in \text{arg} \max_{D \in \mathcal{D}} \log \mathbb{P}[\mathcal{M}(D) = y] \]&lt;/p&gt;

&lt;p&gt;This is a high-dimensional discrete optimization problem, and is generally intractable to solve in practice, even in low-dimensional settings.  It is common to consider the relaxed problem that instead optimizes over the set of &lt;em&gt;probability distributions&lt;/em&gt; \( \mathcal{S} \):&lt;/p&gt;

&lt;p&gt;\[ \hat{P} \in \text{arg} \max_{P \in \mathcal{S}} \log \mathbb{P}[\mathcal{M}(P) = y] \label{eq1} \tag{1} \]&lt;/p&gt;

&lt;p&gt;More generally, we can consider any objective function that measures how well \( P \) explains \( y \).  The log-likelihood is a natural choice, although other choices are also possible and used in practice.  In the special-but-common case where the mechanism is an instance of the Gaussian mechanism, we have \( \mathcal{M}(D) = f(D) + \mathcal{N}(0, \sigma^2)^k \) and \( \log \mathbb{P}[\mathcal{M}(P) = y] \propto - || f(P) - y ||_2^2 \).  If \( f \) is a linear function of \( P \), then Problem \ref{eq1} is simply a quadratic program.  In the subsequent subsections, we will describe different approaches to solve or approximately solve Problem \ref{eq1}.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Remark 1&lt;/strong&gt;: The distribution learned from solving Problem \ref{eq1} will resemble the true data with respect to the statistics measured by \( \mathcal{M} \).  It may or may not accurately preserve other statistics — that is data dependent.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Remark 2&lt;/strong&gt;: The most common statistics to measure are &lt;strong&gt;low-dimensional marginals&lt;/strong&gt;.  A marginal for a subset of attributes counts the number of records in the dataset that match each setting of possible values. They are appealing statistics to measure because:&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;They capture low-dimensional structure common in real world data distributions.&lt;/li&gt;
    &lt;li&gt;Each cell in a marginal is a count, a statistic that is fairly robust to noise.&lt;/li&gt;
    &lt;li&gt;One individual can only contribute to a single cell of a marginal, so all cells have low sensitivity and can be measured simultaneously with low privacy cost.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;direct&quot;&gt;Direct&lt;/h3&gt;

&lt;p&gt;We can attempt to solve Problem \ref{eq1} directly by utilizing any algorithm for convex optimization over the probability simplex, such as multiplicative weights.  This method works well in low-dimensional regimes, although quickly becomes intractable for higher-dimensional domains, where it is generally intractable to even enumerate all the entries of a single distribution \( P \), let alone optimize over the space of all distributions.&lt;/p&gt;

&lt;p&gt;Until recently, variants of the direct method were the only general-purpose solutions available for this problem, and as a result, many mechanisms struggled to scale to high-dimensional domains.  Recently, several methods have been proposed that attempt to overcome the curse of dimensionality inherent in the direct approach, which scale by imposing additional assumptions on the mechanism \( \mathcal{M} \) and/or by relaxing the optimization problem.  A common theme is to restrict attention to a subset of joint distributions which have tractable representations.     The sections below describe these more scalable methods, including the different (implicit) assumptions each method makes, as well as the consequences of those assumptions.&lt;/p&gt;

&lt;h3 id=&quot;probabilistic-graphical-models-pgm&quot;&gt;Probabilistic Graphical Models (PGM)&lt;/h3&gt;

&lt;p&gt;The first method we describe is &lt;a href=&quot;https://arxiv.org/abs/1901.09136&quot; target=&quot;\_blank&quot;&gt;PGM&lt;/a&gt;, which was a key component of the first-place solution in the 2018 NIST Differential Privacy &lt;a href=&quot;https://www.nist.gov/ctl/pscr/open-innovation-prize-challenges/past-prize-challenges/2018-differential-privacy-synthetic&quot; target=&quot;\_blank&quot;&gt;Synthetic Data Competition&lt;/a&gt; and in both the first and second-place solutions in the follow-up &lt;a href=&quot;https://www.nist.gov/ctl/pscr/open-innovation-prize-challenges/current-and-upcoming-prize-challenges/2020-differential&quot; target=&quot;\_blank&quot;&gt;Temporal Map Competition&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;PGM scales by restricting attention to distributions that can be represented as a graphical model \( P_{\theta} \).  The key observation of PGM is that when \( \mathcal{M} \) only depends on \( P \) through its low-dimensional marginals, then one of the optimizers of Problem \ref{eq1} is a graphical model with parameters \( \theta \).  In this case, Problem \ref{eq1} is under-determined and typically has infinitely many solutions.  It turns out that the solution found by PGM has maximum entropy among all solutions to the problem — a very natural way to break ties among equally good solutions. Remarkably, these facts are true for any dataset — they do not require the underlying data to be generated from a graphical model with the same structure &lt;a href=&quot;https://arxiv.org/abs/2108.04978&quot; target=&quot;\_blank&quot;&gt;[MMS21]&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The parameter vector \( \theta \) is often much smaller than \( P \), and we can efficiently optimize it, bypassing the curse of dimensionality in this special case.  The size of \( \theta \) and in turn the complexity of PGM depends on the mechanism \( \mathcal{M} \), and in the worst case is the same as the Direct method.&lt;sup id=&quot;fnref:6&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:6&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;  However, in many common cases of practical interest, the complexity of PGM is exponentially better than that of Direct, in which case we can efficiently solve the optimization problem above, finding \( \theta \) and thus a tractable representation of \( \hat{P} \).  The complexity ultimately depends on the size of the junction tree derived from the mechanism \( \mathcal{M} \), and understanding this relationship requires some expertise in graphical models.  However, if we utilize this understanding to design \( \mathcal{M} \), we can avoid this worst-case behavior, as &lt;a href=&quot;https://arxiv.org/abs/2108.04978&quot; target=&quot;\_blank&quot;&gt;MST&lt;/a&gt; and &lt;a href=&quot;http://vldb.org/pvldb/vol14/p2190-cai.pdf&quot; target=&quot;\_blank&quot;&gt;PrivMRF&lt;/a&gt; do.&lt;/p&gt;

&lt;h3 id=&quot;relaxed-tabular&quot;&gt;Relaxed Tabular&lt;/h3&gt;

&lt;p&gt;An alternative approach was proposed in the recent &lt;a href=&quot;https://arxiv.org/abs/2103.06641&quot; target=&quot;\_blank&quot;&gt;RAP&lt;/a&gt; paper.  The key idea is to restrict attention to “pseudo-distributions” that can be represented in a relaxed tabular format.  The format is similar to the one-hot encoding of a discrete dataset, although the entries need not be \( 0 \) or \( 1 \), which enables gradient-based optimization to be performed on the cells in this table.  The number of rows is a tunable knob that can be set to trade off expressive capacity with computational efficiency.  With a sufficiently large knob size, the true minimizer of the original problem can be expressed in this way, but there is no guarantee that gradient-based optimization will converge to it because this representation introduces non-convexity.  Moreover, the search space of this method includes “spurious” distributions, so even the global optimum of relaxed problem would not necessarily solve the original problem.&lt;sup id=&quot;fnref:9&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:9&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;  Despite these drawbacks, this method appears to work well in practice.&lt;/p&gt;

&lt;h3 id=&quot;generative-networks&quot;&gt;Generative Networks&lt;/h3&gt;

&lt;p&gt;Among the iterative methods introduced by &lt;a href=&quot;https://arxiv.org/abs/2106.07153&quot; target=&quot;\_blank&quot;&gt;[LVW21]&lt;/a&gt; is GEM (Generative networks with the exponential mechanism), an approach inspired by generative adversarial networks. They propose representing any dataset as mixture of product distributions over attributes in the data domain. They implicitly encode such distributions using a generative neural network with a softmax layer. In concrete terms, given some Gaussian noise \( \mathbf{z} \sim \mathcal{N}(0, I) \), their &lt;strong&gt;Generate&lt;/strong&gt; step outputs \( f_\theta(\mathbf{z}) \) where \( f \) is some feedforward neural network parametrized by \( \theta \). \( f_\theta(\mathbf{z}) \) represents a collection of marginal distributions for each individual attribute in the domain, which can be used to directly answer any k-way marginal query. Alternatively, one can sample directly from \( f_\theta(\mathbf{z}) \) if the goal is generate synthetic tabular data.&lt;/p&gt;

&lt;p&gt;Note that the size of \( \mathbf{z} \) can be arbitrarily large, meaning that this generative network approach can theoretically be scaled up to capture any distribution \( P \). Moreover, &lt;a href=&quot;https://arxiv.org/abs/2106.07153&quot;&gt;[LVW21]&lt;/a&gt; show that one can achieve strong performance in practical settings even when \( \mathbf{z} \) is small, making such generative network approaches to scale in terms of both computation and memory. Howevever, as is commonly found in deep learning methods, this optimization problem is nonconvex.&lt;/p&gt;

&lt;h3 id=&quot;local-consistency&quot;&gt;Local Consistency&lt;/h3&gt;

&lt;p&gt;Finally, &lt;a href=&quot;https://arxiv.org/abs/2106.07153&quot; target=&quot;\_blank&quot;&gt;GUM&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/abs/2109.06153&quot; target=&quot;\_blank&quot;&gt;APPGM&lt;/a&gt; do not search over any space of distributions, but instead impose &lt;em&gt;local consistency&lt;/em&gt; constraints on the noisy measurements.  These methods relax Problem \ref{eq1} to optimize over the space of pseudo-marginals, rather than distributions.  The pseudo-marginals are required to be internally consistent, but there is no guarantee that there is a distribution which realizes those pseudo-marginals.  As a result, the solution found by these methods need not be feasible in Problem \ref{eq1}.  Nevertheless, we can attempt to generate synthetic data using heuristics to translate these locally consistent pseudo-marginals into synthetic tabular data.  This approach was used by team DPSyn in both NIST competitions.&lt;/p&gt;

&lt;h3 id=&quot;summary&quot;&gt;Summary&lt;/h3&gt;

&lt;p&gt;A qualitative comparison between the discussed methods is given in the table below.&lt;sup id=&quot;fnref:7&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:7&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Remark 3&lt;/strong&gt;: Among the alternatives discussed here, only Direct and PGM can be expected to solve Problem \ref{eq1}.    The alternatives fail to solve Problem \ref{eq1} in general, either from non-convexity, or from introducing spurious distributions to the search space.  This distinguishing feature of PGM comes at a cost: the complexity can be much higher than the alternatives, and in the worst-case, will not be feasible to run.  In such cases, one of the approximations must be used instead.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;Direct&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;PGM&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;Relaxed Tabular&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;Generative Networks&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;Local Consistency&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Search space includes optimum&lt;/td&gt;
      &lt;td&gt;&lt;span style=&quot;color:green&quot;&gt;Yes&lt;/span&gt;&lt;/td&gt;
      &lt;td&gt;&lt;span style=&quot;color:green&quot;&gt;Yes&lt;/span&gt;&lt;/td&gt;
      &lt;td&gt;&lt;span style=&quot;color:green&quot;&gt;Yes&lt;/span&gt;&lt;/td&gt;
      &lt;td&gt;&lt;span style=&quot;color:green&quot;&gt;Yes&lt;/span&gt;&lt;/td&gt;
      &lt;td&gt;&lt;span style=&quot;color:green&quot;&gt;Yes&lt;/span&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Search space excludes spurious distributions&lt;/td&gt;
      &lt;td&gt;&lt;span style=&quot;color:green&quot;&gt;Yes&lt;/span&gt;&lt;/td&gt;
      &lt;td&gt;&lt;span style=&quot;color:green&quot;&gt;Yes&lt;/span&gt;&lt;/td&gt;
      &lt;td&gt;&lt;span style=&quot;color:red&quot;&gt;No&lt;/span&gt;&lt;/td&gt;
      &lt;td&gt;&lt;span style=&quot;color:green&quot;&gt;Yes&lt;/span&gt;&lt;/td&gt;
      &lt;td&gt;&lt;span style=&quot;color:red&quot;&gt;No&lt;/span&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Convexity preserving&lt;/td&gt;
      &lt;td&gt;&lt;span style=&quot;color:green&quot;&gt;Yes&lt;/span&gt;&lt;/td&gt;
      &lt;td&gt;&lt;span style=&quot;color:green&quot;&gt;Yes&lt;/span&gt;&lt;/td&gt;
      &lt;td&gt;&lt;span style=&quot;color:red&quot;&gt;No&lt;/span&gt;&lt;/td&gt;
      &lt;td&gt;&lt;span style=&quot;color:red&quot;&gt;No&lt;/span&gt;&lt;/td&gt;
      &lt;td&gt;&lt;span style=&quot;color:green&quot;&gt;Yes&lt;/span&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Solves Problem \ref{eq1}&lt;/td&gt;
      &lt;td&gt;&lt;span style=&quot;color:green&quot;&gt;Yes&lt;/span&gt;&lt;/td&gt;
      &lt;td&gt;&lt;span style=&quot;color:green&quot;&gt;Yes&lt;/span&gt;&lt;/td&gt;
      &lt;td&gt;&lt;span style=&quot;color:red&quot;&gt;No&lt;/span&gt;&lt;/td&gt;
      &lt;td&gt;&lt;span style=&quot;color:red&quot;&gt;No&lt;/span&gt;&lt;/td&gt;
      &lt;td&gt;&lt;span style=&quot;color:red&quot;&gt;No&lt;/span&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Factors influencing scalability&lt;/td&gt;
      &lt;td&gt;&lt;span style=&quot;color:red&quot;&gt;Size of Entire Domain&lt;/span&gt;&lt;/td&gt;
      &lt;td&gt;&lt;span style=&quot;color:orange&quot;&gt;Size of Junction Tree&lt;/span&gt;&lt;/td&gt;
      &lt;td&gt;&lt;span style=&quot;color:green&quot;&gt;Size of Largest Marginal&lt;/span&gt;&lt;/td&gt;
      &lt;td&gt;&lt;span style=&quot;color:green&quot;&gt;Size of Largest Marginal&lt;/span&gt;&lt;/td&gt;
      &lt;td&gt;&lt;span style=&quot;color:green&quot;&gt;Size of Largest Marginal&lt;/span&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h1 id=&quot;generating-synthetic-data-with-mbi&quot;&gt;Generating Synthetic Data with MBI&lt;/h1&gt;

&lt;p&gt;Now that we have introduced the techniques underlying the Generate step, we will show how to utilize the implementations in the MBI repository to develop end-to-end mechanisms for differentially private synthetic data.&lt;/p&gt;

&lt;h2 id=&quot;preparing-noisy-measurements&quot;&gt;Preparing Noisy Measurements&lt;/h2&gt;

&lt;p&gt;The input to any method for Generate is a collection of noisy measurements.  We show below how to prepare these measurements in a format compatible with the methods for Generate implemented in the MBI repository.  The measurements are represented as a list, where each element of the list is a noisy marginal (represented as a numpy array), along with relevant metadata including the attributes in the marginal and the amount of noise used to answer it.  In the code snippet below, the selected marginals are hard-coded, but in general this list can be modified to tailor the synthetic data towards a different set of marginals.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scipy&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sparse&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mbi&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dataset&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;load&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;adult.csv&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;adult-domain.json&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# SELECT the marginals we&apos;d like to measure
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;marginals&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;marital-status&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;sex&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
             &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;education-num&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;race&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
             &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;sex&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;hours-per-week&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
             &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;workclass&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,),&lt;/span&gt;
             &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;marital-status&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;occupation&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;income&amp;gt;50K&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# MEASURE the marginals and log the noisy answers
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sigma&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt; 
&lt;span class=&quot;n&quot;&gt;measurements&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;M&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;marginals&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;project&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;M&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;datavector&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scale&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sigma&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;I&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sparse&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;eye&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;measurements&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;I&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sigma&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;M&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The above code snippet is a 5-fold composition of Gaussian mechanisms with \( \sigma = 50 \), and hence the entire mechanism is \( \frac{5}{2 \sigma^2} = \frac{1}{1000} \)-zCDP.&lt;/p&gt;

&lt;h2 id=&quot;generating-synthetic-data-from-measurements&quot;&gt;Generating Synthetic Data from Measurements&lt;/h2&gt;

&lt;p&gt;Given measurements represented in the format above, we can readily generate synthetic data using one of several methods.  For example, the code snippet below generates synthetic data that approximately matches the noisy measurements:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mbi&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;FactoredInference&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# PGM
&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mbi&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MixtureInference&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# Relaxed Tabular + Softmax
&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mbi&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LocalInference&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# Local Consistency
&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mbi&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PublicInference&lt;/span&gt;   &lt;span class=&quot;c1&quot;&gt;# Not Discussed
&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# GENERATE synthetic data using PGM 
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;engine&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;FactoredInference&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;domain&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;iters&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2500&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;engine&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;estimate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;measurements&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;synth&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;synthetic_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;To generate synthetic data, we have to simply instantiate one of the inference engines imported.  In the code snippet above, we use the FactoredInference engine, which corresponds to the PGM method.  The other inference engines share the same interface, and can be used instead if desired.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Remark 4&lt;/strong&gt;: By utilizing the inference engines implemented in MBI, end-to-end synthetic data mechanisms can be written with remarkably little code.  This simple example required less than 25 lines of code, and &lt;a href=&quot;https://github.com/ryan112358/private-pgm/tree/master/mechanisms&quot; target=&quot;\_blank&quot;&gt;more complex mechanisms&lt;/a&gt; can usually be written in a single file with less than 200 lines of code.  As a result, future research can focus on the measurement selection subproblem, and new ideas can more rapidly be evaluated and iterated on.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;We evaluated the quality of the synthetic data generated by measuring the error of the measured marginals.  Interestingly, the synthetic data has lower error than the noisy marginals, with reductions in error up to 30% for the larger marginals, and around 3% for the smaller ones.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/smr1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Remark 5:&lt;/strong&gt; It is not surprising that the synthetic data enjoys lower error than the noisy marginals.  Problem \ref{eq1} can be seen as a &lt;em&gt;projection problem&lt;/em&gt;, and there is substantial theoretical &lt;a href=&quot;https://arxiv.org/abs/1212.0297&quot; target=&quot;\_blank&quot;&gt;[NTZ12]&lt;/a&gt; and empirical [&lt;a href=&quot;https://dl.acm.org/doi/abs/10.1145/2783258.2783366&quot; target=&quot;\_blank&quot;&gt;LWK15&lt;/a&gt;, &lt;a href=&quot;https://systems.cs.columbia.edu/private-systems-class/papers/Abowd2019Census.pdf&quot; target=&quot;\_blank&quot;&gt;AAGK+19&lt;/a&gt;] evidence that solving this problem reduces error.  Intuitively, the benefit arises due to the inconsistencies in the noisy observations that are resolved through the optimization procedure.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;We can also use the synthetic data to estimate marginals we didn’t measure with the Gaussian mechanism.  These estimates may or may not be accurate, it depends on the data and the marginal being estimated.  For example, the error on the (sex, income&amp;gt;50K) marginal is around 0.02, while the error on the (education-num, occupation) marginal is about 0.5.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/smr2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Remark 6:&lt;/strong&gt; The fact that the synthetic data is not accurate for some marginals is not a limitation of the method used for Generate, but rather an artifact of what marginals were selected.  Thus, it is clear that selecting the right marginals to measure plays a crucial role in the quality of the synthetic data. This is an important open problem that will be the topic of a future blog post.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;coming-up-next&quot;&gt;Coming up Next&lt;/h1&gt;

&lt;p&gt;In this blog post, we focused on the &lt;strong&gt;Generate&lt;/strong&gt; step of the select-measure-generate paradigm.  For the next blog post in this series, we will focus on state-of-the-art approaches to the &lt;strong&gt;Select&lt;/strong&gt; sub-problem.  If you have any comments, questions, or remarks, please feel free to share them in the comments section below.  If you would like to try generating synthetic data with MBI, check out this &lt;a href=&quot;https://colab.research.google.com/drive/1c8gT5m_GWfQoa_mx8eXh4sPD48Y0z3ML?usp=sharing&quot;&gt;jupyter notebook&lt;/a&gt; on Google Colab!&lt;/p&gt;

&lt;hr /&gt;
&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:0&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;The Generate step is a post-processing of already privatized noisy marginals, and therefore the privacy analysis only needs to reason about the first two steps. &lt;a href=&quot;#fnref:0&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Here we assume that \( \mathcal{M} \) is a mechanism with a discrete output space.  In practice, this is always the case because any mechanism implemented on a finite computer must have a discrete output space.  For continuous output spaces, interpret the objective function as a log density rather than a log probability. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:6&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;For example, this worst-case behavior is realized if &lt;strong&gt;all&lt;/strong&gt; 2-way marginals are measured.  While this can be seen as a limitation of PGM, &lt;a href=&quot;http://people.seas.harvard.edu/~salil/research/synthetic-Feb2010.pdf&quot;&gt;it is known&lt;/a&gt; that generating synthetic data that preserves all 2-way marginals is computationally hard in the worst-case. &lt;a href=&quot;#fnref:6&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:9&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;This idea was refined into &lt;a href=&quot;https://arxiv.org/abs/2106.07153&quot; target=&quot;\_blank&quot;&gt;RAP&lt;sup&gt;softmax&lt;/sup&gt;&lt;/a&gt; in follow-up-work, which overcomes the latter issue, but does not resolve the non-convexity issue. &lt;a href=&quot;#fnref:9&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:7&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;These approximations were all developed concurrently, and systematic empirical comparisons between them (and PGM) have not been done to date.  Some experimental comparisons can be found in &lt;a href=&quot;https://arxiv.org/abs/2106.07153&quot; target=&quot;\_blank&quot;&gt;[LVW21]&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/abs/2109.06153&quot;&gt;[MPSM21]&lt;/a&gt;. &lt;a href=&quot;#fnref:7&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <author>
        
            <name>Ryan McKenna</name>
        
            <name>Terrance Liu</name>
        
        </author>
        <pubDate>Thu, 27 Jan 2022 09:00:00 -0800</pubDate>
        <link>http://localhost:4000/synth-data-1/</link>
        <guid isPermaLink="true">http://localhost:4000/synth-data-1/</guid>
      </item>
    
      <item>
        <title>What is Synthetic Data?</title>
        <description>&lt;p&gt;The concept of synthetic data seems to be having “a moment” in the privacy world as a promising approach to sharing data while protecting privacy.  Strictly speaking, any time you make up data, you have produced a synthetic dataset, but more specifically&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;a &lt;strong&gt;synthetic dataset&lt;/strong&gt; is a stand-in for some original dataset that has the same format, and accurately reflects the statistical properties of the original dataset, but contains only “fake” records.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Intuitively, a synthetic dataset can be used as if it were the real data—we can stare at it, compute summary statistics from it, train models from it, and do everything we normally do with data—but because the records do not correspond to “real” people, we don’t have to worry about protecting privacy.&lt;/p&gt;

&lt;p&gt;To be sure, synthetic data is a very appealing concept, but&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;simply making data “synthetic” does not guarantee privacy in any meaningful sense of the word,&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;and we need to be careful about what it actually means to generate &lt;strong&gt;private synthetic data.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In this post I will briefly describe &lt;strong&gt;differentially private synthetic data&lt;/strong&gt;, why we need it, and what we know about producing it.  The post will serve as a warmup to some more technical posts that will follow later, describing general approaches and state-of-the-art algorithms for producing synthetic data.&lt;/p&gt;

&lt;h3 id=&quot;what-is-synthetic-data&quot;&gt;What is synthetic data?&lt;/h3&gt;

&lt;p&gt;Given a dataset \(X\), a synthetic dataset \(Y\) is a new dataset that has the same &lt;em&gt;structure&lt;/em&gt; as \(X\), but whose elements are “fake.”  For example, if \(X\) is a highly stylized version of the data collected in the US decennial census, then each element of \(X\) would be a tuple of the form (age, census_block, sex, race, ethnicity) corresponding to the data of a real person who filled out a census card.  A synthetic dataset should also contain such tuples, but they no longer need to correspond to real people.&lt;/p&gt;

&lt;p&gt;An example of a synthetic dataset for the census would be a dataset consisting of 300,000,000 individuals, all living at one location in rural Mississippi, and all of whom are white, non-hispanic females aged 81.  And therein lies the challenge.  Of course we want a synthetic dataset not to merely have the same &lt;em&gt;structure&lt;/em&gt; as the original data, but also to preserve the &lt;em&gt;statistical properties&lt;/em&gt; of the original data.  A synthetic dataset for the census should put roughly the right number of individuals of the right types in the right places to reflect the actual population of the US.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Thus, to generate synthetic data, we have to know something about the original data, which creates opportunities to leak sensitive information.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;synthetic-data-from-generative-modeling&quot;&gt;Synthetic data from generative modeling&lt;/h4&gt;

&lt;p&gt;Ensuring that a synthetic dataset really only reflects the statistical properties of the real data, and doesn’t encode sensitive information about the individuals in the real dataset, is quite difficult.  I could go on forever about strawman proposals that would generate records that appear to be “fake” but really just encode the original dataset in obvious ways.  Just so you believe I could do it, suppose for every real record, we get a fake record by adding 10 years to that individual’s age.  But what people typically mean when they talk about generating synthetic data, and what the synthetic data products being sold by companies typically do, is train a generative model for the dataset \(X\) and obtain the synthetic dataset \(Y\) by sampling from that model.  The hope is that samples from the generative model look like the original dataset in aggregate, but the actual people in the sample have nothing to do with the real people in the original dataset.  For example, a generative model from the US population will lead to more people living in California than in Montana, but ideally wouldn’t simply spit out the data of the actual residents of California.&lt;/p&gt;

&lt;h4 id=&quot;generative-models-dont-magically-protect-privacy&quot;&gt;Generative models don’t magically protect privacy&lt;/h4&gt;

&lt;p&gt;Simply training a generative model on \(X\) doesn’t actually mean we’ve hidden anything about \(X\) or the people in it.  For an extreme example, suppose the records in \(X\) are \(x_1,\dots,x_n\) and our generative model is arbitrarily expressive and determines that the best model for the data is just the uniform distribution on the set \(x_1,\dots,x_n\).  Then if our synthetic dataset consists of \(n\) iid samples from the generative model, our synthetic dataset will simply contain one or more copies of the real data for a large fraction (about 70%) of the people in the original dataset.&lt;/p&gt;

&lt;h4 id=&quot;reconstruction-attacks-say-hello&quot;&gt;Reconstruction attacks say hello&lt;/h4&gt;

&lt;p&gt;My example was admittedly a strawman, and when we train a generative model we typically expect it to produce new examples that aren’t in the original training dataset.  Maybe by training generative models in the right way, we can ensure that the generative model only captures statistics about the dataset, without revealing information about the individuals in the dataset?  Let’s not forget that&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;a synthetic dataset is just one of many possible ways to release statistical information about a dataset, and we know from the theory of reconstruction attacks that &lt;strong&gt;any&lt;/strong&gt; mechanism that accurately preserves too many statistics inevitably destroys privacy.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Specifically, there is a rich and fairly comprehensive theory of &lt;strong&gt;reconstruction attacks&lt;/strong&gt;, whose &lt;a href=&quot;/reconstruction-theory/&quot;&gt;theory&lt;/a&gt; and &lt;a href=&quot;/diffix-attack/&quot;&gt;practice&lt;/a&gt; we detailed in earlier posts.  This theory says that any synthetic dataset that preserves answers to every statistic of the form “How many elements of the dataset satisfies property P?” with any meaningful notion of accuracy, is spectacularly non-private, in the sense that an attacker can reconstruct nearly all of the private information in the original dataset.  Viewed through this lens, synthetic data becomes something of a red herring.  The important question is which subset of statistics we want to preserve and how to preserve them without compromising privacy.&lt;/p&gt;

&lt;h3 id=&quot;differentially-private-synthetic-data&quot;&gt;Differentially private synthetic data&lt;/h3&gt;

&lt;p&gt;Making a dataset “synthetic” may not be a magic bullet, but it’s still a useful goal.  Fortunately, we already have a good working definition of what it means for an algorithm to protect the privacy of the individuals in the dataset—&lt;strong&gt;differential privacy&lt;/strong&gt;.  So perhaps we can design a differentially private algorithm that takes the original dataset \(X\) and outputs a synthetic dataset \(Y\) that preserves many of the properties of \(X\) accurately (but still within the limits imposed by reconstruction attacks)?  In fact, the answer turns out to be a resounding yes!&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;There is a differentially private algorithm that takes a dataset \(X\) and outputs a synthetic dataset \(Y\) that preserves an &lt;strong&gt;exponential&lt;/strong&gt; number of statistical properties of \(X\) &lt;a href=&quot;https://arxiv.org/abs/1109.2229&quot;&gt;[BLR08]&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This statement is intentionally quite informal, but intutively this result says that we can hope to do amazing things, like generate a synthetic dataset that satisfies the strong guarantee of differential privacy while still approximately preserving impressively complex statistics of the dataset, such as the marginal distribution of every set of three attributes, or the prediction error of every linear classifier.&lt;/p&gt;

&lt;p&gt;So what’s the problem?  Well, primarily it’s computational complexity. This algorithm, and the many beautiful algorithms it inspired, all have exponential worst-case running time.  Specifically, if each element of the dataset contains \(d\) bits, then the worst-case running time is at least as large as \(2^d\).  Unfortunately, this turns out to be an inherent bottleneck for any algorithm that generates synthetic data.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Any private algorithm that takes a dataset \(X\) and outputs a synthetic dataset \(Y\) that preserves even just the correlations between each pair of features must have worst-case running time that grows exponentially in the number of features, under widely believed complexity assumptions. &lt;a href=&quot;https://eccc.weizmann.ac.il/report/2010/017/&quot;&gt;[UV11]&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;While there are inherent limits of the accuracy of all differentially private algorithms (see &lt;a href=&quot;https://privacytools.seas.harvard.edu/publications/exposed-survey-attacks-private-data&quot;&gt;[DSSU17]&lt;/a&gt;), and there are significant barriers to making differentially private algorithms practical,&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;computational complexity is the main barrier that is specific to differentially private algorithms for generating synthetic data.&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;where-do-we-go-from-here&quot;&gt;Where do we go from here?&lt;/h4&gt;

&lt;p&gt;Despite the computational bottlenecks, there has stll been a lot of amazing progress on differentially private synthetic data.  And, despite my criticism of generative modeling in isolation as a means of generating synthetic data, most of these approaches are indeed based on &lt;em&gt;differentially private generative models&lt;/em&gt; such as sparse graphical models or GANs, and leverage the fact that these types of models can typically be fit efficiently to realistic data, despite their worst-case hardness.  In the next few posts, we will try to describe the landscape of the most promising approaches that we currently have available.&lt;/p&gt;

&lt;hr /&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;See &lt;a href=&quot;https://arxiv.org/abs/2110.13239&quot;&gt;[AACGKLSSTZ21]&lt;/a&gt; for an interesting example of a &lt;em&gt;statistical&lt;/em&gt; limitation that is specific to differentially private algorithms that generate synthetic data. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <author>
        
            <name>Jonathan Ullman</name>
        
        </author>
        <pubDate>Mon, 10 Jan 2022 15:00:00 -0800</pubDate>
        <link>http://localhost:4000/synth-data-0/</link>
        <guid isPermaLink="true">http://localhost:4000/synth-data-0/</guid>
      </item>
    
  </channel>
</rss>
