<!DOCTYPE html>
<html>
  <head>
    <title>Differential Privacy - DifferentialPrivacy.org</title>

        <meta charset="utf-8" />
    <meta content='text/html; charset=utf-8' http-equiv='Content-Type'>
    <meta http-equiv='X-UA-Compatible' content='IE=edge'>
    <meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1.0'>
    <meta name="twitter:card" content="summary" />
    <meta name="twitter:site" content="@DiffPriv" />
    <meta name="twitter:image" content="https://differentialprivacy.org/images/logodp.png" />
    <meta name="twitter:image:alt" content="DP.org logo" />

    
    <meta name="description" content="Website for the differential privacy research community">
    <meta property="og:description" content="Website for the differential privacy research community" />
    <meta name="twitter:description" content="Website for the differential privacy research community"/>
    
    <meta name="author" content="Differential Privacy" />

    
    <meta property="og:title" content="DifferentialPrivacy.org" />
    <meta property="twitter:title" content="DifferentialPrivacy.org" />
    


    <!--[if lt IE 9]>
      <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="alternate" type="application/rss+xml" title="Differential Privacy - Website for the differential privacy research community" href="/feed.xml" />
	<link href="https://fonts.googleapis.com/css2?family=Open+Sans:wght@300;400;700&display=swap" rel="stylesheet">

    <!-- Created with Jekyll Now - http://github.com/barryclark/jekyll-now -->
    <link rel="icon" type="image/png" sizes="96x96" href="/favicon-96x96.png">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  </head>

  <body>
    <div class="wrapper-masthead">
      <div class="container">
        <header class="masthead clearfix">
          <!-- no image for now <a href="/" class="site-avatar"><img src="favicon-96x96.png" /></a> -->

          <div class="site-info">
            <h1 class="site-name"><a href="/"><b><font color="#3d85c6">Differential</font><font color="#b45f06">Privacy</font><font color="#cccccc">.org</font></b></a></h1>
            <!-- <p class="site-description">Website for the differential privacy research community</p> -->
          </div>

          <nav>
            <a href="/">Home</a>
            <a href="/about">About</a>
			<a href="/categories">Posts</a>
			<a href="/resources">Resources</a>
          </nav>
        </header>
      </div>
    </div>

    <div id="main" role="main" class="container">
      <div class="posts">
  
    <article class="post">

      <h1><a href="/open-problems-how-generic-can-composition-be/">Open problem(s) - How generic can composition results be?</a></h1>

      <div class="entry">
        <p>The composition theorem is a cornerstone of differential privacy literature. 
In its most basic formulation, it states that if two mechanisms \(\mathcal{M}_1\) and \(\mathcal{M}_2\) are respectively \(\varepsilon_1\)-DP and \(\varepsilon_2\)-DP, then the mechanism \(\mathcal{M}\) defined by \(\mathcal{M}(D)=\left(\mathcal{M}_1(D),\mathcal{M}_2(D)\right)\) is \((\varepsilon_1+\varepsilon_2)\)-DP.
A large body of work focused on proving extensions of this composition theorem.
These extensions are of two kinds.</p>

      </div>

      <a href="/open-problems-how-generic-can-composition-be/" class="read-more">Read More</a>
    </article>
  
    <article class="post">

      <h1><a href="/inverse-sensitivity/">Beyond Global Sensitivity via Inverse Sensitivity</a></h1>

      <div class="entry">
        <p>The most well-known and widely-used method for achieving differential privacy is to compute the true function value \(f(x)\) and then add Laplace or Gaussian noise scaled to the <em>global sensitivity</em> of \(f\). 
This may be overly conservative. In this post we’ll show how we can do better.</p>

      </div>

      <a href="/inverse-sensitivity/" class="read-more">Read More</a>
    </article>
  
    <article class="post">

      <h1><a href="/colt23-bsp/">Covariance-Aware Private Mean Estimation, Efficiently</a></h1>

      <div class="entry">
        <p>Last week, the Mark Fulk award for best student paper at <a href="https://learningtheory.org/colt2023/">COLT 2023</a> was awarded to the following two papers on private mean estimation:</p>
<ul>
  <li><a href="https://arxiv.org/abs/2301.07078">A Fast Algorithm for Adaptive Private Mean Estimation</a>, by <a href="https://web.stanford.edu/~jduchi/">John Duchi</a>, <a href="https://dblp.org/pid/252/5821.html">Saminul Haque</a>, and <a href="https://web.stanford.edu/~rohithk/">Rohith Kuditipudi</a> <strong>[<a href="https://arxiv.org/abs/2301.07078">DHK23</a>]</strong>;</li>
  <li><a href="https://arxiv.org/abs/2301.12250">Fast, Sample-Efficient, Affine-Invariant Private Mean and Covariance Estimation for Subgaussian Distributions</a> by <a href="https://cs-people.bu.edu/grbrown/">Gavin Brown</a>, <a href="https://www.samuelbhopkins.com/">Samuel B. Hopkins</a>, and <a href="https://cs-people.bu.edu/ads22/">Adam Smith</a> <strong>[<a href="https://arxiv.org/abs/2301.12250">BHS23</a>]</strong>.</li>
</ul>

      </div>

      <a href="/colt23-bsp/" class="read-more">Read More</a>
    </article>
  
    <article class="post">

      <h1><a href="/tpdp2023/">Call for Papers - TPDP 2023 - Submission deadline July 7</a></h1>

      <div class="entry">
        <p>The <a href="https://tpdp.journalprivacyconfidentiality.org/2023/">9th Workshop on the Theory and Practice of Differential Privacy (TPDP 2023)</a> will take place in Boston September 27-28, 2023.
This is the first year the workshop is a standalone event. However, the <a href="https://opendp.org/event/opendp-community-meeting-2023">OpenDP community meeting</a> is the following day (also in Boston). It is also moving from a one-day event to two days.</p>

      </div>

      <a href="/tpdp2023/" class="read-more">Read More</a>
    </article>
  
    <article class="post">

      <h1><a href="/open-problem-better-privacy-guarantees-for-larger-groups/">Open problem - Better privacy guarantees for larger groups</a></h1>

      <div class="entry">
        <p>Consider a simple query counting the number of people in various mutually exclusive groups.
In the differential privacy literature, it is typical to assume that each of these groups should be subject to the same privacy loss: the noise added to each count has the same magnitude, and everyone gets the same privacy guarantees.
However, in settings where these groups have vastly different population sizes, larger populations may be willing to accept more error in exchange for stronger privacy protections.
In particular, in many use cases, <em>relative</em> error (the noisy count is within 5% of the true value) matters more than absolute error (the noisy count is at a distance of at most 100 of the true value).
This leads to a natural question: can we use this fact to develop a mechanism that improves the privacy guarantees of individuals in larger groups, subject to a constraint on relative error?</p>

      </div>

      <a href="/open-problem-better-privacy-guarantees-for-larger-groups/" class="read-more">Read More</a>
    </article>
  
    <article class="post">

      <h1><a href="/composition-basics/">Composition Basics</a></h1>

      <div class="entry">
        <p>Our data is subject to many different uses. Many entities will have access to our data and those entities will perform many different analyses that involve our data. The greatest risk to privacy is that an attacker will combine multiple pieces of information from the same or different sources and that the combination of these will reveal sensitive details about us.
Thus we cannot study privacy leakage in a vacuum; it is important that we can reason about the accumulated privacy leakage over multiple independent analyses, which is known as <em>composition</em>. We have <a href="/privacy-composition/">previously discussed</a> why composition is so important for differential privacy.</p>

<p>This is the first in a series of posts on <em>composition</em> in which we will explain in more detail how compositoin analyses work.</p>

<p>Composition is quantitative. The differential privacy guarantee of the overall system will depend on the number of analyses and the privacy parameters that they each satisfy. The exact relationship between these quantities can be complex. There are various composition theorems that give bounds on the overall parameters in terms of the parameters of the parts of the system.</p>

<p>The simplest composition theorem is what is known as basic composition, which applies to pure \(\varepsilon\)-DP (although it can be extended to approximate \((\varepsilon,\delta)\)-DP):</p>

<blockquote>
  <p><strong>Theorem</strong> (Basic Composition)
Let \(M_1, M_2, \cdots, M_k : \mathcal{X}^n \to \mathcal{Y}\) be randomized algorithms. Suppose \(M_j\) is \(\varepsilon_j\)-DP for each \(j \in [k]\).
Define \(M : \mathcal{X}^n \to \mathcal{Y}^k\) by \(M(x)=(M_1(x),M_2(x),\cdots,M_k(x))\), where each algorithm is run independently. Then \(M\) is \(\varepsilon\)-DP for \(\varepsilon = \sum_{j=1}^k \varepsilon_j\).</p>
</blockquote>

      </div>

      <a href="/composition-basics/" class="read-more">Read More</a>
    </article>
  
    <article class="post">

      <h1><a href="/privacy-doona/">Privacy Doona&#58; Why We Should Hide Among The Clones</a></h1>

      <div class="entry">
        <p>In this blog post, we will discuss a recent(ish) result of Feldman, McMillan, and Talwar <a href="https://arxiv.org/abs/2012.12803" title="Vitaly Feldman, Audra McMillan, Kunal Talwar. Hiding Among the Clones: A Simple and Nearly Optimal Analysis of Privacy Amplification by Shuffling. FOCS 2021"><strong>[FMT21]</strong></a>, which provides an improved and simple analysis of the so-called “amplification by shuffling” formally connecting local privacy (LDP) and shuffle privacy.<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup> Now, I’ll assume the reader is familiar with both LDP and Shuffle DP: if not, a quick-and-dirty refresher (with less quick, and less dirty references) can be found <a href="\trustmodels">here</a>, and of course there is also Albert Cheu’s excellent survey on Shuffle DP <a href="https://arxiv.org/abs/2107.11839" title="Albert Cheu. Differential Privacy in the Shuffle Model: A Survey of Separations. arXiv 2021"><strong>[Cheu21]</strong></a>.</p>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>The title of this post is a reference to the title of <a href="https://arxiv.org/abs/2012.12803" title="Vitaly Feldman, Audra McMillan, Kunal Talwar. Hiding Among the Clones: A Simple and Nearly Optimal Analysis of Privacy Amplification by Shuffling. FOCS 2021"><strong>[FMT21]</strong></a>, “Hiding Among The Clones,” and to the notion of <em>privacy blanket</em> introduced by Balle, Bell, Gascón, and Nissim <a href="https://link.springer.com/chapter/10.1007/978-3-030-26951-7_22" title="Borja Balle, James Bell, Adrià Gascón, Kobbi Nissim. The Privacy Blanket of the Shuffle Model. CRYPTO 2019"><strong>[BBGN19]</strong></a>. Intuitively, the “amplification by shuffling” paradigm can be seen as anonymizing the messages from local randomizers, whose message distribution can be mathematically decomposed as a mixture of “noise distribution not depending on the user’s input” and “distribution actually depending on their input.” As a result, each user randomly sends a message from the first or second distribution of the mixture.  But the shuffling then hides the informative messages (drawn from the second part of the mixture) among the non-informative (noise) ones: so the noise messages end up providing a “privacy blanket” in which sensitive information is safely and soundly wrapped. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

      </div>

      <a href="/privacy-doona/" class="read-more">Read More</a>
    </article>
  
    <article class="post">

      <h1><a href="/dp-fine-tuning/">Differentially private deep learning can be effective with self-supervised models</a></h1>

      <div class="entry">
        <p>Differential Privacy (DP) is a formal definition of privacy which guarantees that the outcome of a statistical procedure does not vary much regardless of whether an individual input is included or removed from the training dataset. 
This guarantee is desirable when we are tasked to train machine learning models on private datasets that should not memorize individual inputs. 
Past works have shown that differentially private models can be resilient to strong membership inference [<a href="https://proceedings.mlr.press/v37/kairouz15.html">1</a>, <a href="https://ieeexplore.ieee.org/abstract/document/9519424">34</a>, <a href="https://proceedings.neurips.cc/paper/2020/hash/fc4ddc15f9f4b4b06ef7844d6bb53abf-Abstract.html">35</a>] and data reconstruction attacks [<a href="https://www.usenix.org/conference/usenixsecurity19/presentation/carlini">2</a>, <a href="https://arxiv.org/abs/2201.12383">3</a>] when the privacy parameter is set to be sufficiently small. 
See a <a href="https://differentialprivacy.org/how-to-deploy-ml-with-dp/">prior post</a> for more background on differentially private machine learning.</p>

      </div>

      <a href="/dp-fine-tuning/" class="read-more">Read More</a>
    </article>
  
    <article class="post">

      <h1><a href="/synth-data-1/">A simple recipe for private synthetic data generation</a></h1>

      <div class="entry">
        <p>In the <a href="https://differentialprivacy.org/synth-data-0/">last blog post</a>, we covered the potential pitfalls of synthetic data without formal privacy guarantees, and motivated the need for differentially private synthetic data mechanisms.  In this blog post, we will describe the <strong>select-measure-generate</strong> paradigm, which is a simple and effective template for designing synthetic data mechanisms.  The three steps underlying the select-measure-generate paradigm are illustrated and explained below.</p>

      </div>

      <a href="/synth-data-1/" class="read-more">Read More</a>
    </article>
  
    <article class="post">

      <h1><a href="/synth-data-0/">What is Synthetic Data?</a></h1>

      <div class="entry">
        <p>The concept of synthetic data seems to be having “a moment” in the privacy world as a promising approach to sharing data while protecting privacy.  Strictly speaking, any time you make up data, you have produced a synthetic dataset, but more specifically</p>

      </div>

      <a href="/synth-data-0/" class="read-more">Read More</a>
    </article>
  
    <article class="post">

      <h1><a href="/neurips2021/">Conference Digest - NeurIPS 2021</a></h1>

      <div class="entry">
        <p>The accepted papers for <a href="https://neurips.cc/Conferences/2020">NeurIPS 2021</a> were recently announced, and there’s a huge amount of differential privacy content. 
We found one relevant workshop and 48 papers.
This is up from 31 papers last year, an over 50% increase!
It looks like there’s huge growth in interest on differentially private machine learning.
Impressively, at the time of this writing, all but five papers are already posted on arXiv!
For the full list of accepted papers, see <a href="https://neurips.cc/Conferences/2021/AcceptedPapersInitial">here</a>.
Please let us know if we missed relevant papers on differential privacy!</p>

      </div>

      <a href="/neurips2021/" class="read-more">Read More</a>
    </article>
  
    <article class="post">

      <h1><a href="/how-to-deploy-ml-with-dp/">How to deploy machine learning with differential privacy?</a></h1>

      <div class="entry">
        <p>In many applications of machine learning, such as machine learning for medical diagnosis, we would like to have machine learning algorithms that do not memorize sensitive information about the training set, such as the specific medical histories of individual patients. Differential privacy is a notion that allows quantifying the degree of privacy protection provided by an algorithm on the underlying (sensitive) data set it operates on. Through the lens of differential privacy, we can design machine learning algorithms that responsibly train models on private data.</p>

      </div>

      <a href="/how-to-deploy-ml-with-dp/" class="read-more">Read More</a>
    </article>
  
    <article class="post">

      <h1><a href="/one-shot-top-k/">One-shot DP Top-k mechanisms</a></h1>

      <div class="entry">
        <p>In the last <a href="https://differentialprivacy.org/exponential-mechanism-bounded-range/"><em>blog post</em></a>, we showed that the exponential mechanism enjoys improved composition bounds over general pure DP mechanisms due to a property called <strong>bounded range</strong>.  For this post, we will present another useful, and somewhat surprising, property of the exponential mechanism in its application of top-\(k\) selection.</p>

      </div>

      <a href="/one-shot-top-k/" class="read-more">Read More</a>
    </article>
  
    <article class="post">

      <h1><a href="/exponential-mechanism-bounded-range/">A Better Privacy Analysis of the Exponential Mechanism</a></h1>

      <div class="entry">
        <p>A basic and frequent task in data analysis is <em>selection</em> – given a set of options \(\mathcal{Y}\), output the (approximately) best one, where “best” is defined by some loss function \(\ell : \mathcal{Y} \times \mathcal{X}^n \to \mathbb{R}\) and a dataset \(x \in \mathcal{X}^n\). That is, we want to output some \(y \in \mathcal{Y}\) that approximately minimizes \(\ell(y,x)\). Naturally, we are interested in <em>private selection</em> – i.e., the output should be differentially private in terms of the dataset \(x\).
This post discusses algorithms for private selection – in particular, we give an improved privacy analysis of the popular exponential mechanism.</p>

      </div>

      <a href="/exponential-mechanism-bounded-range/" class="read-more">Read More</a>
    </article>
  
    <article class="post">

      <h1><a href="/open-problem-optimal-query-release/">Open Problem - Optimal Query Release for Pure Differential Privacy</a></h1>

      <div class="entry">
        <p>Releasing large sets of statistical queries is a centerpiece of the theory of differential privacy.  Here, we are given a <em>dataset</em> \(x = (x_1,\dots,x_n) \in [T]^n\), and a set of <em>statistical queries</em> \(f_1,\dots,f_k\), where each query is defined by some bounded function \(f_j : [T] \to [-1,1]\), and (abusing notation) is defined as
\[
f_j(x) = \frac{1}{n} \sum_{i=1}^{n} f_j(x_i).
\]
We use \(f(x) = (f_1(x),\dots,f_k(x))\) to denote the vector consisting of the true answers to all these queries.
Our goal is to design an \((\varepsilon, \delta)\)-differentially private algorithm \(M\) that takes a dataset \(x\in [T]^n\) and outputs a random vector \(M(x)\in \mathbb{R}^k\) such that \(\| M(x) - f(x) \|\) is small in expectation for some norm \(\|\cdot\|\). Usually algorithms for this problem also give high probability bounds on the error, but we focus on expected error for simplicity.</p>

      </div>

      <a href="/open-problem-optimal-query-release/" class="read-more">Read More</a>
    </article>
  
    <article class="post">

      <h1><a href="/icml2021/">Conference Digest - ICML 2021</a></h1>

      <div class="entry">
        <p><a href="https://icml.cc/Conferences/2021">ICML 2021</a>, one of the biggest conferences in machine learning, naturally has a ton of interesting sounding papers on the topic of differential privacy.
We went through this year’s <a href="https://icml.cc/Conferences/2021/AcceptedPapersInitial">accepted papers</a> and aggregated all the relevant papers we could find.
In addition, this year features three workshops on the topic of privacy, as well as a tutorial.
As always, please inform us if we overlooked any papers on differential privacy.</p>

      </div>

      <a href="/icml2021/" class="read-more">Read More</a>
    </article>
  
    <article class="post">

      <h1><a href="/inference-is-not-a-privacy-violation/">Statistical Inference is Not a Privacy Violation</a></h1>

      <div class="entry">
        <p>On April 28, 2021, the US Census Bureau <a href="https://www.census.gov/programs-surveys/decennial-census/decade/2020/planning-management/process/disclosure-avoidance/2020-das-updates.html">released</a> a new demonstration of its differentially private Disclosure Avoidance System (DAS) for the 2020 US Census. The public were given a month to submit feedback before the system is finalized.
This demonstration data and the feedback has generated a lot of discussion, including media coverage on <a href="https://www.npr.org/2021/05/19/993247101/for-the-u-s-census-keeping-your-data-anonymous-and-useful-is-a-tricky-balance">National Public Radio</a>, in <a href="https://www.washingtonpost.com/local/social-issues/2020-census-differential-privacy-ipums/2021/06/01/6c94b46e-c30d-11eb-93f5-ee9558eecf4b_story.html">the Washington Post</a>, and via <a href="https://apnews.com/article/business-census-2020-technology-e701e313e841674be6396321343b7e49">the Associated Press</a>. The DAS is also the subject of an <a href="https://www.courtlistener.com/docket/59728874/state-v-united-states-department-of-commerce/">ongoing lawsuit</a>.</p>

      </div>

      <a href="/inference-is-not-a-privacy-violation/" class="read-more">Read More</a>
    </article>
  
    <article class="post">

      <h1><a href="/tpdp21-cfp/">Call for Papers - Workshop on the Theory and Practice of Differential Privacy (TPDP 2021)</a></h1>

      <div class="entry">
        <p>Work on differential privacy spans a number of different research communities, including theoretical computer science, machine learning, statistics, security, law, databases, cryptography, programming languages, social sciences, and more.
Each of these communities may choose to publish their work in their own community’s venues, which could result in small groups of differential privacy researchers becoming isolated.
To alleviate these issues, we have the Workshop on the <a href="https://tpdp.journalprivacyconfidentiality.org/">Theory and Practice of Differential Privacy</a> (TPDP), which is intended to bring these subcommunities together under one roof (well, a virtual one at least for 2020 and 2021).</p>

      </div>

      <a href="/tpdp21-cfp/" class="read-more">Read More</a>
    </article>
  
    <article class="post">

      <h1><a href="/alt-highlights/">ALT Highlights - An Equivalence between Private Learning and Online Learning (ALT '21 Tutorial)</a></h1>

      <div class="entry">
        <p>Welcome to ALT Highlights, a series of blog posts spotlighting various happenings at the recent conference <a href="http://algorithmiclearningtheory.org/alt2021/">ALT 2021</a>, including plenary talks, tutorials, trends in learning theory, and more! 
To reach a broad audience, the series will be disseminated as guest posts on different blogs in machine learning and theoretical computer science. 
Given the topic of this post, we felt <a href="https://differentialprivacy.org/">DifferentialPrivacy.org</a> was a great fit.
This initiative is organized by the <a href="https://www.let-all.com/">Learning Theory Alliance</a>, and overseen by <a href="http://www.gautamkamath.com/">Gautam Kamath</a>. 
All posts in ALT Highlights are indexed on the official <a href="https://www.let-all.com/blog/2021/04/20/alt-highlights-2021/">Learning Theory Alliance blog</a>.</p>

      </div>

      <a href="/alt-highlights/" class="read-more">Read More</a>
    </article>
  
    <article class="post">

      <h1><a href="/flavoursofdelta/">What is δ, and what δifference does it make?</a></h1>

      <div class="entry">
        <p>There are many variants or flavours of differential privacy (DP) some weaker than others: often, a given variant comes with own guarantees and “conversion theorems” to the others. As an example, “pure” DP has a single parameter \(\varepsilon\), and corresponds to a very stringent notion of DP:</p>

      </div>

      <a href="/flavoursofdelta/" class="read-more">Read More</a>
    </article>
  
    <article class="post">

      <h1><a href="/tpdp2020/">Conference Digest - TPDP 2020</a></h1>

      <div class="entry">
        <p><a href="https://tpdp.journalprivacyconfidentiality.org/2020/">TPDP 2020</a> is a workshop focused on differential privacy. As such, it’s a great place to learn about recent developments in the DP research community.
It will be held on 13 November and is co-located with <a href="https://www.sigsac.org/ccs/CCS2020/">CCS</a>, but, of course, it’s virtual this year. <a href="https://www.sigsac.org/ccs/CCS2020/registration.html">Registration is only US$35 if you register by Friday, 30 October.</a> Check out the 8 excellent talks and 71 posters below – wow, the workshop has grown!</p>

      </div>

      <a href="/tpdp2020/" class="read-more">Read More</a>
    </article>
  
    <article class="post">

      <h1><a href="/diffix-attack/">Reconstruction Attacks in Practice</a></h1>

      <div class="entry">
        <p>This is the second of two posts describing the theory and practice of reconstruction attacks.  To read the first post, which covers the theoretical basis of such attacks, <a href="https://differentialprivacy.org/reconstruction-theory/">[click here]</a>.</p>

      </div>

      <a href="/diffix-attack/" class="read-more">Read More</a>
    </article>
  
    <article class="post">

      <h1><a href="/reconstruction-theory/">The Theory of Reconstruction Attacks</a></h1>

      <div class="entry">
        <p>We often see people asking whether or not differential privacy might be overkill.  Why do we need strong privacy protections like differential privacy when we’re only releasing approximate, aggregate statistical information about a dataset?  Is it really possible to extract information about specific users from releasing these statistics?  The answer turns out to be a resounding yes!  The textbook by Dwork and Roth <a href="https://www.cis.upenn.edu/~aaroth/privacybook.html">[DR14]</a> calls this phenomenon the Fundamental Law of Information Recovery:</p>

<blockquote>
  <p>Giving overly accurate answers to too many questions will inevitably destroy privacy.</p>
</blockquote>

      </div>

      <a href="/reconstruction-theory/" class="read-more">Read More</a>
    </article>
  
    <article class="post">

      <h1><a href="/neurips2020/">Conference Digest - NeurIPS 2020</a></h1>

      <div class="entry">
        <p><a href="https://neurips.cc/Conferences/2020">NeurIPS 2020</a> is the biggest conference on machine learning, with tons of content on differential privacy in many different forms.
We were able to find two workshops, a competition, and 31 papers. 
This was just going off the preliminary <a href="https://nips.cc/Conferences/2020/AcceptedPapersInitial">accepted papers list</a>, so it’s possible that we might have missed some papers on differential privacy – please let us know!
We will update this post later, once all the conference material (papers and videos) are publicly available.</p>

      </div>

      <a href="/neurips2020/" class="read-more">Read More</a>
    </article>
  
    <article class="post">

      <h1><a href="/open-problem-avoid-union/">Open Problem - Avoiding the Union Bound for Multiple Queries</a></h1>

      <div class="entry">
        <p><strong>Background:</strong> Perhaps the best-studied problem in differential privacy is answering multiple counting queries.
The standard approach is to add independent, appropriately-calibrated (Laplace or Gaussian) noise to each query result and apply a composition theorem.
To bound the maximum error over the query answers, one takes a union bound over the independent noise samples.
However, this is <em>not</em> optimal.
The problem is to identify the optimal method (up to constant factors).</p>

      </div>

      <a href="/open-problem-avoid-union/" class="read-more">Read More</a>
    </article>
  
    <article class="post">

      <h1><a href="/private-pac/">Differentially Private PAC Learning</a></h1>

      <div class="entry">
        <p>The study of differentially private PAC learning runs all the way from
its introduction in 2008 <a href="https://arxiv.org/abs/0803.0924" title="Shiva Prasad Kasiviswanathan, Homin K. Lee, Kobbi Nissim, Sofya Raskhodnikova, and Adam Smith. What Can We Learn Privately? FOCS 2008"><strong>[KLNRS08]</strong></a> to a best paper award at the
Symposium on Foundations of Computer Science (FOCS) this year <a href="https://arxiv.org/abs/2003.00563" title="Mark Bun, Roi Livni, and Shay Moran. An equivalence between private classification and online prediction. FOCS 2020"><strong>[BLM20]</strong></a>.
In this post, we’ll recap the history of this line of work, aiming for
enough detail for a rough understanding of the results and methods.</p>

      </div>

      <a href="/private-pac/" class="read-more">Read More</a>
    </article>
  
    <article class="post">

      <h1><a href="/icml2020/">Conference Digest - ICML 2020</a></h1>

      <div class="entry">
        <p><a href="https://icml.cc/virtual/2020">ICML 2020</a> is one of the premiere venues in machine learning, and generally features a lot of great work in differentially private machine learning.
This year is no exception: the relevant papers are listed below to the best of our ability, including links to the full versions of papers, as well as the conference pages (which contain slides and 15 minute videos for each paper).
As always, please inform us if we overlooked any papers on differential privacy.</p>

      </div>

      <a href="/icml2020/" class="read-more">Read More</a>
    </article>
  
    <article class="post">

      <h1><a href="/colt2020/">Conference Digest - COLT 2020</a></h1>

      <div class="entry">
        <p><a href="https://www.learningtheory.org/colt2020/">COLT 2020</a> was held online in July, and featured nine papers on differential privacy, as well as a keynote talk by Salil Vadhan.
While differential privacy has always had a home in the COLT community, it seems like this year was truly exceptional in terms of the number of results.
We link all the content below, including pointers to the papers, videos on Youtube, and the page on the conference website. 
Please let us know if we missed any papers on differential privacy, either in the comments below or by email.</p>

      </div>

      <a href="/colt2020/" class="read-more">Read More</a>
    </article>
  
    <article class="post">

      <h1><a href="/privacy-composition/">Why Privacy Needs Composition</a></h1>

      <div class="entry">
        <p>We’re back!  In our last <a href="\average-case-dp">post</a> we discussed some of the subtle pitfalls of formulating the assumptions underlying average-case relaxations of differential privacy.  This time we’re going to look at the composition property of differential privacy—that is, the fact that running two independent differentially private algorithms on your data and combining their outputs is still differentially private. This is a key property of differential privacy and is actually closely related to the worst-case nature of differential privacy.</p>

      </div>

      <a href="/privacy-composition/" class="read-more">Read More</a>
    </article>
  
    <article class="post">

      <h1><a href="/open-problem-all-pairs/">Open Problem - Private All-Pairs Distances</a></h1>

      <div class="entry">
        <p><strong>Background:</strong> Suppose we are interested in computing the distance between two vertices in a graph. Under edge or node differential privacy, this problem is not promising because the removal of a single edge can make distances change from 1 to \(n − 1\) or can even disconnect the graph. However, a different setting that makes sense to consider is that of a weighted graph \((G, w)\) whose topology \(G = (V, E)\) is publicly known but edge weight function \(w : E \to \mathbb{R}^+\) must be kept private. (For instance, consider transit times on a road network. The topology of the road network may be publicly available as a map, but the edge weights corresponding to transit times may be based on private GPS locations of individual cars.)</p>

      </div>

      <a href="/open-problem-all-pairs/" class="read-more">Read More</a>
    </article>
  
    <article class="post">

      <h1><a href="/average-case-dp/">The Pitfalls of Average-Case Differential Privacy</a></h1>

      <div class="entry">
        <p>Differential privacy protects against extremely strong adversaries—even ones who know the entire dataset except for one bit of information about one individual.  Since its inception, people have considered ways to relax the definition to assume a more realistic adversary.  A natural way to do so is to incorporate some distributional assumptions. That is, rather than considering a worst-case dataset, assume the dataset is drawn from some distribution and provide some form of “average-case” or “Bayesian” privacy guarantee with respect to this distribution. This is especially tempting as it is common for statistical analysis to work under distributional assumptions.</p>

      </div>

      <a href="/average-case-dp/" class="read-more">Read More</a>
    </article>
  
    <article class="post">

      <h1><a href="/stoc2020/">Conference Digest - STOC 2020</a></h1>

      <div class="entry">
        <p><a href="http://acm-stoc.org/stoc2020/">STOC 2020</a> was recently held online, as one of the first major theory conferences during the COVID-19 era.
It featured four papers on differential privacy, which we list and link below.
Each one is accompanied by a video from the conference, as well as a longer video if available.
Please let us know if we missed any papers on differential privacy, either in the comments below or by email.</p>

      </div>

      <a href="/stoc2020/" class="read-more">Read More</a>
    </article>
  
    <article class="post">

      <h1><a href="/trustmodels/">Trust Models, and Notions of Privacy</a></h1>

      <div class="entry">
        <p>There exist various notions of differential privacy which, while sharing a common core, differ in some key specific aspects. Broadly speaking, vary among a few main axes, such as the type of guarantee they provide, the specific similarity between data they consider, and the trust model they aim to address. This last point will be the focus of this post: <em>which notion of privacy is best suited to the specific scenario at hand?</em></p>

      </div>

      <a href="/trustmodels/" class="read-more">Read More</a>
    </article>
  
    <article class="post">

      <h1><a href="/welcome/">Welcome to DifferentialPrivacy.org!</a></h1>

      <div class="entry">
        <p>Hello, welcome to this new website! Our goal is to serve as a hub for the differential privacy research community and to promote the work in this area. Please read on to learn more!</p>

      </div>

      <a href="/welcome/" class="read-more">Read More</a>
    </article>
  
</div>

    </div>

    <!--
    <div class="wrapper-footer">
      <div class="container">
        <footer class="footer">
          



<a href="https://github.com/differentialprivacy/differentialprivacy"><i class="svg-icon github"></i></a>




<a href="https://www.twitter.com/DiffPriv"><i class="svg-icon twitter"></i></a>



        </footer>
      </div>
    </div>
    -->

    
	<!-- Google Analytics -->
	<script>
		(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
		(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
		m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
		})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

		ga('create', 'UA-11154678-4', 'auto');
		ga('send', 'pageview', {
		  'page': '/',
		  'title': 'DifferentialPrivacy.org'
		});
	</script>
	<!-- End Google Analytics -->


    
    <!--  -->
  </body>
</html>
